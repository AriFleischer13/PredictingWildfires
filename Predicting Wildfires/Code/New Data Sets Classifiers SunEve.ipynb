{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in the 3 Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 586362 entries, 0 to 586361\n",
      "Data columns (total 41 columns):\n",
      " #   Column                        Non-Null Count   Dtype  \n",
      "---  ------                        --------------   -----  \n",
      " 0   index_3day                    586362 non-null  int64  \n",
      " 1   latitude                      586362 non-null  float64\n",
      " 2   longitude                     586362 non-null  float64\n",
      " 3   ('time', 'first')             586362 non-null  object \n",
      " 4   ('time', 'last')              586362 non-null  object \n",
      " 5   ('DOY', 'first')              586362 non-null  int64  \n",
      " 6   ('DOY', 'last')               586362 non-null  int64  \n",
      " 7   ('Year', 'first')             586362 non-null  int64  \n",
      " 8   ('Year', 'last')              586362 non-null  int64  \n",
      " 9   ('cwat', 'amin')              586362 non-null  float64\n",
      " 10  ('cwat', 'amax')              586362 non-null  float64\n",
      " 11  ('cwat', 'mean')              586362 non-null  float64\n",
      " 12  ('cwat', 'var')               586362 non-null  float64\n",
      " 13  ('r', 'amin')                 586362 non-null  float64\n",
      " 14  ('r', 'amax')                 586362 non-null  float64\n",
      " 15  ('r', 'mean')                 586362 non-null  float64\n",
      " 16  ('r', 'var')                  586362 non-null  float64\n",
      " 17  ('tozne', 'amin')             586362 non-null  float64\n",
      " 18  ('tozne', 'amax')             586362 non-null  float64\n",
      " 19  ('tozne', 'mean')             586362 non-null  float64\n",
      " 20  ('tozne', 'var')              586362 non-null  float64\n",
      " 21  ('gh', 'amin')                586362 non-null  float64\n",
      " 22  ('gh', 'amax')                586362 non-null  float64\n",
      " 23  ('gh', 'mean')                586362 non-null  float64\n",
      " 24  ('gh', 'var')                 586362 non-null  float64\n",
      " 25  ('pwat', 'amin')              586362 non-null  float64\n",
      " 26  ('pwat', 'amax')              586362 non-null  float64\n",
      " 27  ('pwat', 'mean')              586362 non-null  float64\n",
      " 28  ('pwat', 'var')               586362 non-null  float64\n",
      " 29  ('paramId_0', 'amin')         586362 non-null  float64\n",
      " 30  ('paramId_0', 'amax')         586362 non-null  float64\n",
      " 31  ('paramId_0', 'mean')         586362 non-null  float64\n",
      " 32  ('paramId_0', 'var')          586362 non-null  float64\n",
      " 33  ('pres', 'amin')              106334 non-null  float64\n",
      " 34  ('pres', 'amax')              106334 non-null  float64\n",
      " 35  ('pres', 'mean')              106334 non-null  float64\n",
      " 36  ('pres', 'var')               51233 non-null   float64\n",
      " 37  ('pres', 'count')             586362 non-null  int64  \n",
      " 38  ('macro_season', '<lambda>')  586362 non-null  int64  \n",
      " 39  ('month', '<lambda>')         586362 non-null  int64  \n",
      " 40  fire                          586362 non-null  int64  \n",
      "dtypes: float64(30), int64(9), object(2)\n",
      "memory usage: 183.4+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_3day</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>('time', 'first')</th>\n",
       "      <th>('time', 'last')</th>\n",
       "      <th>('DOY', 'first')</th>\n",
       "      <th>('DOY', 'last')</th>\n",
       "      <th>('Year', 'first')</th>\n",
       "      <th>('Year', 'last')</th>\n",
       "      <th>('cwat', 'amin')</th>\n",
       "      <th>...</th>\n",
       "      <th>('paramId_0', 'mean')</th>\n",
       "      <th>('paramId_0', 'var')</th>\n",
       "      <th>('pres', 'amin')</th>\n",
       "      <th>('pres', 'amax')</th>\n",
       "      <th>('pres', 'mean')</th>\n",
       "      <th>('pres', 'var')</th>\n",
       "      <th>('pres', 'count')</th>\n",
       "      <th>('macro_season', '&lt;lambda&gt;')</th>\n",
       "      <th>('month', '&lt;lambda&gt;')</th>\n",
       "      <th>fire</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>-125.0</td>\n",
       "      <td>2001-01-01 00:00:00</td>\n",
       "      <td>2001-01-03 18:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2001</td>\n",
       "      <td>2001</td>\n",
       "      <td>0.03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>32.0</td>\n",
       "      <td>-125.0</td>\n",
       "      <td>2001-01-04 00:00:00</td>\n",
       "      <td>2001-01-06 18:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>2001</td>\n",
       "      <td>2001</td>\n",
       "      <td>0.02</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>32.0</td>\n",
       "      <td>-125.0</td>\n",
       "      <td>2001-01-07 00:00:00</td>\n",
       "      <td>2001-01-09 18:00:00</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>2001</td>\n",
       "      <td>2001</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>32.0</td>\n",
       "      <td>-125.0</td>\n",
       "      <td>2001-01-10 00:00:00</td>\n",
       "      <td>2001-01-12 18:00:00</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>2001</td>\n",
       "      <td>2001</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>15.583333</td>\n",
       "      <td>600.810606</td>\n",
       "      <td>34430.0</td>\n",
       "      <td>62960.0</td>\n",
       "      <td>55440.0</td>\n",
       "      <td>196700200.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>32.0</td>\n",
       "      <td>-125.0</td>\n",
       "      <td>2001-01-13 00:00:00</td>\n",
       "      <td>2001-01-15 18:00:00</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>2001</td>\n",
       "      <td>2001</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index_3day  latitude  longitude    ('time', 'first')     ('time', 'last')  \\\n",
       "0           0      32.0     -125.0  2001-01-01 00:00:00  2001-01-03 18:00:00   \n",
       "1           1      32.0     -125.0  2001-01-04 00:00:00  2001-01-06 18:00:00   \n",
       "2           2      32.0     -125.0  2001-01-07 00:00:00  2001-01-09 18:00:00   \n",
       "3           3      32.0     -125.0  2001-01-10 00:00:00  2001-01-12 18:00:00   \n",
       "4           4      32.0     -125.0  2001-01-13 00:00:00  2001-01-15 18:00:00   \n",
       "\n",
       "   ('DOY', 'first')  ('DOY', 'last')  ('Year', 'first')  ('Year', 'last')  \\\n",
       "0                 1                3               2001              2001   \n",
       "1                 4                6               2001              2001   \n",
       "2                 7                9               2001              2001   \n",
       "3                10               12               2001              2001   \n",
       "4                13               15               2001              2001   \n",
       "\n",
       "   ('cwat', 'amin')  ...  ('paramId_0', 'mean')  ('paramId_0', 'var')  \\\n",
       "0              0.03  ...               0.000000              0.000000   \n",
       "1              0.02  ...               0.000000              0.000000   \n",
       "2              0.00  ...               0.000000              0.000000   \n",
       "3              0.00  ...              15.583333            600.810606   \n",
       "4              0.00  ...               0.000000              0.000000   \n",
       "\n",
       "   ('pres', 'amin')  ('pres', 'amax')  ('pres', 'mean')  ('pres', 'var')  \\\n",
       "0               NaN               NaN               NaN              NaN   \n",
       "1               NaN               NaN               NaN              NaN   \n",
       "2               NaN               NaN               NaN              NaN   \n",
       "3           34430.0           62960.0           55440.0      196700200.0   \n",
       "4               NaN               NaN               NaN              NaN   \n",
       "\n",
       "   ('pres', 'count')  ('macro_season', '<lambda>')  ('month', '<lambda>')  \\\n",
       "0                  0                             0                      1   \n",
       "1                  0                             0                      1   \n",
       "2                  0                             0                      1   \n",
       "3                  4                             0                      1   \n",
       "4                  0                             0                      1   \n",
       "\n",
       "   fire  \n",
       "0     0  \n",
       "1     0  \n",
       "2     0  \n",
       "3     0  \n",
       "4     0  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threeday_df = pd.read_csv(\"bestgeo_sametime_3day.csv\")\n",
    "threeday_df.info()\n",
    "threeday_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 352107 entries, 0 to 352106\n",
      "Data columns (total 41 columns):\n",
      " #   Column                        Non-Null Count   Dtype  \n",
      "---  ------                        --------------   -----  \n",
      " 0   index_5day                    352107 non-null  int64  \n",
      " 1   latitude                      352107 non-null  float64\n",
      " 2   longitude                     352107 non-null  float64\n",
      " 3   ('time', 'first')             352107 non-null  object \n",
      " 4   ('time', 'last')              352107 non-null  object \n",
      " 5   ('DOY', 'first')              352107 non-null  int64  \n",
      " 6   ('DOY', 'last')               352107 non-null  int64  \n",
      " 7   ('Year', 'first')             352107 non-null  int64  \n",
      " 8   ('Year', 'last')              352107 non-null  int64  \n",
      " 9   ('cwat', 'amin')              352107 non-null  float64\n",
      " 10  ('cwat', 'amax')              352107 non-null  float64\n",
      " 11  ('cwat', 'mean')              352107 non-null  float64\n",
      " 12  ('cwat', 'var')               352107 non-null  float64\n",
      " 13  ('r', 'amin')                 352107 non-null  float64\n",
      " 14  ('r', 'amax')                 352107 non-null  float64\n",
      " 15  ('r', 'mean')                 352107 non-null  float64\n",
      " 16  ('r', 'var')                  352107 non-null  float64\n",
      " 17  ('tozne', 'amin')             352107 non-null  float64\n",
      " 18  ('tozne', 'amax')             352107 non-null  float64\n",
      " 19  ('tozne', 'mean')             352107 non-null  float64\n",
      " 20  ('tozne', 'var')              352107 non-null  float64\n",
      " 21  ('gh', 'amin')                352107 non-null  float64\n",
      " 22  ('gh', 'amax')                352107 non-null  float64\n",
      " 23  ('gh', 'mean')                352107 non-null  float64\n",
      " 24  ('gh', 'var')                 352107 non-null  float64\n",
      " 25  ('pwat', 'amin')              352107 non-null  float64\n",
      " 26  ('pwat', 'amax')              352107 non-null  float64\n",
      " 27  ('pwat', 'mean')              352107 non-null  float64\n",
      " 28  ('pwat', 'var')               352107 non-null  float64\n",
      " 29  ('paramId_0', 'amin')         352107 non-null  float64\n",
      " 30  ('paramId_0', 'amax')         352107 non-null  float64\n",
      " 31  ('paramId_0', 'mean')         352107 non-null  float64\n",
      " 32  ('paramId_0', 'var')          352107 non-null  float64\n",
      " 33  ('pres', 'amin')              90895 non-null   float64\n",
      " 34  ('pres', 'amax')              90895 non-null   float64\n",
      " 35  ('pres', 'mean')              90895 non-null   float64\n",
      " 36  ('pres', 'var')               49916 non-null   float64\n",
      " 37  ('pres', 'count')             352107 non-null  int64  \n",
      " 38  ('macro_season', '<lambda>')  352107 non-null  int64  \n",
      " 39  ('month', '<lambda>')         352107 non-null  int64  \n",
      " 40  fire                          352107 non-null  int64  \n",
      "dtypes: float64(30), int64(9), object(2)\n",
      "memory usage: 110.1+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_5day</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>('time', 'first')</th>\n",
       "      <th>('time', 'last')</th>\n",
       "      <th>('DOY', 'first')</th>\n",
       "      <th>('DOY', 'last')</th>\n",
       "      <th>('Year', 'first')</th>\n",
       "      <th>('Year', 'last')</th>\n",
       "      <th>('cwat', 'amin')</th>\n",
       "      <th>...</th>\n",
       "      <th>('paramId_0', 'mean')</th>\n",
       "      <th>('paramId_0', 'var')</th>\n",
       "      <th>('pres', 'amin')</th>\n",
       "      <th>('pres', 'amax')</th>\n",
       "      <th>('pres', 'mean')</th>\n",
       "      <th>('pres', 'var')</th>\n",
       "      <th>('pres', 'count')</th>\n",
       "      <th>('macro_season', '&lt;lambda&gt;')</th>\n",
       "      <th>('month', '&lt;lambda&gt;')</th>\n",
       "      <th>fire</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>-125.0</td>\n",
       "      <td>2001-01-01 00:00:00</td>\n",
       "      <td>2001-01-05 18:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>2001</td>\n",
       "      <td>0.03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>32.0</td>\n",
       "      <td>-125.0</td>\n",
       "      <td>2001-01-06 00:00:00</td>\n",
       "      <td>2001-01-10 18:00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>2001</td>\n",
       "      <td>2001</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>2.95</td>\n",
       "      <td>174.050000</td>\n",
       "      <td>34430.0</td>\n",
       "      <td>34430.0</td>\n",
       "      <td>34430.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>32.0</td>\n",
       "      <td>-125.0</td>\n",
       "      <td>2001-01-11 00:00:00</td>\n",
       "      <td>2001-01-15 18:00:00</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>2001</td>\n",
       "      <td>2001</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>6.40</td>\n",
       "      <td>274.884211</td>\n",
       "      <td>61430.0</td>\n",
       "      <td>62960.0</td>\n",
       "      <td>62443.333333</td>\n",
       "      <td>770233.333333</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>32.0</td>\n",
       "      <td>-125.0</td>\n",
       "      <td>2001-01-16 00:00:00</td>\n",
       "      <td>2001-01-20 18:00:00</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "      <td>2001</td>\n",
       "      <td>2001</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>32.0</td>\n",
       "      <td>-125.0</td>\n",
       "      <td>2001-01-21 00:00:00</td>\n",
       "      <td>2001-01-25 18:00:00</td>\n",
       "      <td>21</td>\n",
       "      <td>25</td>\n",
       "      <td>2001</td>\n",
       "      <td>2001</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.70</td>\n",
       "      <td>9.800000</td>\n",
       "      <td>73310.0</td>\n",
       "      <td>73310.0</td>\n",
       "      <td>73310.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index_5day  latitude  longitude    ('time', 'first')     ('time', 'last')  \\\n",
       "0           0      32.0     -125.0  2001-01-01 00:00:00  2001-01-05 18:00:00   \n",
       "1           1      32.0     -125.0  2001-01-06 00:00:00  2001-01-10 18:00:00   \n",
       "2           2      32.0     -125.0  2001-01-11 00:00:00  2001-01-15 18:00:00   \n",
       "3           3      32.0     -125.0  2001-01-16 00:00:00  2001-01-20 18:00:00   \n",
       "4           4      32.0     -125.0  2001-01-21 00:00:00  2001-01-25 18:00:00   \n",
       "\n",
       "   ('DOY', 'first')  ('DOY', 'last')  ('Year', 'first')  ('Year', 'last')  \\\n",
       "0                 1                5               2001              2001   \n",
       "1                 6               10               2001              2001   \n",
       "2                11               15               2001              2001   \n",
       "3                16               20               2001              2001   \n",
       "4                21               25               2001              2001   \n",
       "\n",
       "   ('cwat', 'amin')  ...  ('paramId_0', 'mean')  ('paramId_0', 'var')  \\\n",
       "0              0.03  ...                   0.00              0.000000   \n",
       "1              0.00  ...                   2.95            174.050000   \n",
       "2              0.00  ...                   6.40            274.884211   \n",
       "3              0.00  ...                   0.00              0.000000   \n",
       "4              0.00  ...                   0.70              9.800000   \n",
       "\n",
       "   ('pres', 'amin')  ('pres', 'amax')  ('pres', 'mean')  ('pres', 'var')  \\\n",
       "0               NaN               NaN               NaN              NaN   \n",
       "1           34430.0           34430.0      34430.000000              NaN   \n",
       "2           61430.0           62960.0      62443.333333    770233.333333   \n",
       "3               NaN               NaN               NaN              NaN   \n",
       "4           73310.0           73310.0      73310.000000              NaN   \n",
       "\n",
       "   ('pres', 'count')  ('macro_season', '<lambda>')  ('month', '<lambda>')  \\\n",
       "0                  0                             0                      1   \n",
       "1                  1                             0                      1   \n",
       "2                  3                             0                      1   \n",
       "3                  0                             0                      1   \n",
       "4                  1                             0                      1   \n",
       "\n",
       "   fire  \n",
       "0     0  \n",
       "1     0  \n",
       "2     0  \n",
       "3     0  \n",
       "4     0  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fiveday_df = pd.read_csv(\"bestgeo_sametime_5day.csv\")\n",
    "fiveday_df.info()\n",
    "fiveday_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 176295 entries, 0 to 176294\n",
      "Data columns (total 41 columns):\n",
      " #   Column                        Non-Null Count   Dtype  \n",
      "---  ------                        --------------   -----  \n",
      " 0   index_10day                   176295 non-null  int64  \n",
      " 1   latitude                      176295 non-null  float64\n",
      " 2   longitude                     176295 non-null  float64\n",
      " 3   ('time', 'first')             176295 non-null  object \n",
      " 4   ('time', 'last')              176295 non-null  object \n",
      " 5   ('DOY', 'first')              176295 non-null  int64  \n",
      " 6   ('DOY', 'last')               176295 non-null  int64  \n",
      " 7   ('Year', 'first')             176295 non-null  int64  \n",
      " 8   ('Year', 'last')              176295 non-null  int64  \n",
      " 9   ('cwat', 'amin')              176295 non-null  float64\n",
      " 10  ('cwat', 'amax')              176295 non-null  float64\n",
      " 11  ('cwat', 'mean')              176295 non-null  float64\n",
      " 12  ('cwat', 'var')               176295 non-null  float64\n",
      " 13  ('r', 'amin')                 176295 non-null  float64\n",
      " 14  ('r', 'amax')                 176295 non-null  float64\n",
      " 15  ('r', 'mean')                 176295 non-null  float64\n",
      " 16  ('r', 'var')                  176295 non-null  float64\n",
      " 17  ('tozne', 'amin')             176295 non-null  float64\n",
      " 18  ('tozne', 'amax')             176295 non-null  float64\n",
      " 19  ('tozne', 'mean')             176295 non-null  float64\n",
      " 20  ('tozne', 'var')              176295 non-null  float64\n",
      " 21  ('gh', 'amin')                176295 non-null  float64\n",
      " 22  ('gh', 'amax')                176295 non-null  float64\n",
      " 23  ('gh', 'mean')                176295 non-null  float64\n",
      " 24  ('gh', 'var')                 176295 non-null  float64\n",
      " 25  ('pwat', 'amin')              176295 non-null  float64\n",
      " 26  ('pwat', 'amax')              176295 non-null  float64\n",
      " 27  ('pwat', 'mean')              176295 non-null  float64\n",
      " 28  ('pwat', 'var')               176295 non-null  float64\n",
      " 29  ('paramId_0', 'amin')         176295 non-null  float64\n",
      " 30  ('paramId_0', 'amax')         176295 non-null  float64\n",
      " 31  ('paramId_0', 'mean')         176295 non-null  float64\n",
      " 32  ('paramId_0', 'var')          176295 non-null  float64\n",
      " 33  ('pres', 'amin')              70527 non-null   float64\n",
      " 34  ('pres', 'amax')              70527 non-null   float64\n",
      " 35  ('pres', 'mean')              70527 non-null   float64\n",
      " 36  ('pres', 'var')               44410 non-null   float64\n",
      " 37  ('pres', 'count')             176295 non-null  int64  \n",
      " 38  ('macro_season', '<lambda>')  176295 non-null  int64  \n",
      " 39  ('month', '<lambda>')         176295 non-null  int64  \n",
      " 40  fire                          176295 non-null  int64  \n",
      "dtypes: float64(30), int64(9), object(2)\n",
      "memory usage: 55.1+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_10day</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>('time', 'first')</th>\n",
       "      <th>('time', 'last')</th>\n",
       "      <th>('DOY', 'first')</th>\n",
       "      <th>('DOY', 'last')</th>\n",
       "      <th>('Year', 'first')</th>\n",
       "      <th>('Year', 'last')</th>\n",
       "      <th>('cwat', 'amin')</th>\n",
       "      <th>...</th>\n",
       "      <th>('paramId_0', 'mean')</th>\n",
       "      <th>('paramId_0', 'var')</th>\n",
       "      <th>('pres', 'amin')</th>\n",
       "      <th>('pres', 'amax')</th>\n",
       "      <th>('pres', 'mean')</th>\n",
       "      <th>('pres', 'var')</th>\n",
       "      <th>('pres', 'count')</th>\n",
       "      <th>('macro_season', '&lt;lambda&gt;')</th>\n",
       "      <th>('month', '&lt;lambda&gt;')</th>\n",
       "      <th>fire</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>-125.0</td>\n",
       "      <td>2001-01-01 00:00:00</td>\n",
       "      <td>2001-01-10 18:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>2001</td>\n",
       "      <td>2001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.475</td>\n",
       "      <td>87.025000</td>\n",
       "      <td>34430.0</td>\n",
       "      <td>34430.0</td>\n",
       "      <td>34430.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>32.0</td>\n",
       "      <td>-125.0</td>\n",
       "      <td>2001-01-11 00:00:00</td>\n",
       "      <td>2001-01-20 18:00:00</td>\n",
       "      <td>11</td>\n",
       "      <td>20</td>\n",
       "      <td>2001</td>\n",
       "      <td>2001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.200</td>\n",
       "      <td>144.420513</td>\n",
       "      <td>61430.0</td>\n",
       "      <td>62960.0</td>\n",
       "      <td>62443.333333</td>\n",
       "      <td>7.702333e+05</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>32.0</td>\n",
       "      <td>-125.0</td>\n",
       "      <td>2001-01-21 00:00:00</td>\n",
       "      <td>2001-01-30 18:00:00</td>\n",
       "      <td>21</td>\n",
       "      <td>30</td>\n",
       "      <td>2001</td>\n",
       "      <td>2001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.400</td>\n",
       "      <td>86.605128</td>\n",
       "      <td>65030.0</td>\n",
       "      <td>75960.0</td>\n",
       "      <td>71433.333333</td>\n",
       "      <td>3.250763e+07</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>32.0</td>\n",
       "      <td>-125.0</td>\n",
       "      <td>2001-01-31 00:00:00</td>\n",
       "      <td>2001-02-09 18:00:00</td>\n",
       "      <td>31</td>\n",
       "      <td>40</td>\n",
       "      <td>2001</td>\n",
       "      <td>2001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>32.0</td>\n",
       "      <td>-125.0</td>\n",
       "      <td>2001-02-10 00:00:00</td>\n",
       "      <td>2001-02-19 18:00:00</td>\n",
       "      <td>41</td>\n",
       "      <td>50</td>\n",
       "      <td>2001</td>\n",
       "      <td>2001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.550</td>\n",
       "      <td>408.920513</td>\n",
       "      <td>41470.0</td>\n",
       "      <td>76180.0</td>\n",
       "      <td>57815.000000</td>\n",
       "      <td>2.113109e+08</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index_10day  latitude  longitude    ('time', 'first')     ('time', 'last')  \\\n",
       "0            0      32.0     -125.0  2001-01-01 00:00:00  2001-01-10 18:00:00   \n",
       "1            1      32.0     -125.0  2001-01-11 00:00:00  2001-01-20 18:00:00   \n",
       "2            2      32.0     -125.0  2001-01-21 00:00:00  2001-01-30 18:00:00   \n",
       "3            3      32.0     -125.0  2001-01-31 00:00:00  2001-02-09 18:00:00   \n",
       "4            4      32.0     -125.0  2001-02-10 00:00:00  2001-02-19 18:00:00   \n",
       "\n",
       "   ('DOY', 'first')  ('DOY', 'last')  ('Year', 'first')  ('Year', 'last')  \\\n",
       "0                 1               10               2001              2001   \n",
       "1                11               20               2001              2001   \n",
       "2                21               30               2001              2001   \n",
       "3                31               40               2001              2001   \n",
       "4                41               50               2001              2001   \n",
       "\n",
       "   ('cwat', 'amin')  ...  ('paramId_0', 'mean')  ('paramId_0', 'var')  \\\n",
       "0               0.0  ...                  1.475             87.025000   \n",
       "1               0.0  ...                  3.200            144.420513   \n",
       "2               0.0  ...                  2.400             86.605128   \n",
       "3               0.0  ...                  0.000              0.000000   \n",
       "4               0.0  ...                 10.550            408.920513   \n",
       "\n",
       "   ('pres', 'amin')  ('pres', 'amax')  ('pres', 'mean')  ('pres', 'var')  \\\n",
       "0           34430.0           34430.0      34430.000000              NaN   \n",
       "1           61430.0           62960.0      62443.333333     7.702333e+05   \n",
       "2           65030.0           75960.0      71433.333333     3.250763e+07   \n",
       "3               NaN               NaN               NaN              NaN   \n",
       "4           41470.0           76180.0      57815.000000     2.113109e+08   \n",
       "\n",
       "   ('pres', 'count')  ('macro_season', '<lambda>')  ('month', '<lambda>')  \\\n",
       "0                  1                             0                      1   \n",
       "1                  3                             0                      1   \n",
       "2                  3                             0                      1   \n",
       "3                  0                             0                      2   \n",
       "4                 10                             0                      2   \n",
       "\n",
       "   fire  \n",
       "0     0  \n",
       "1     0  \n",
       "2     0  \n",
       "3     0  \n",
       "4     0  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tenday_df = pd.read_csv(\"bestgeo_sametime_10day.csv\")\n",
    "tenday_df.info()\n",
    "tenday_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Training and Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Feature and Target Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-Day Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 586362 entries, 0 to 586361\n",
      "Data columns (total 31 columns):\n",
      " #   Column                        Non-Null Count   Dtype  \n",
      "---  ------                        --------------   -----  \n",
      " 0   ('cwat', 'amin')              586362 non-null  float64\n",
      " 1   ('cwat', 'amax')              586362 non-null  float64\n",
      " 2   ('cwat', 'mean')              586362 non-null  float64\n",
      " 3   ('cwat', 'var')               586362 non-null  float64\n",
      " 4   ('r', 'amin')                 586362 non-null  float64\n",
      " 5   ('r', 'amax')                 586362 non-null  float64\n",
      " 6   ('r', 'mean')                 586362 non-null  float64\n",
      " 7   ('r', 'var')                  586362 non-null  float64\n",
      " 8   ('tozne', 'amin')             586362 non-null  float64\n",
      " 9   ('tozne', 'amax')             586362 non-null  float64\n",
      " 10  ('tozne', 'mean')             586362 non-null  float64\n",
      " 11  ('tozne', 'var')              586362 non-null  float64\n",
      " 12  ('gh', 'amin')                586362 non-null  float64\n",
      " 13  ('gh', 'amax')                586362 non-null  float64\n",
      " 14  ('gh', 'mean')                586362 non-null  float64\n",
      " 15  ('gh', 'var')                 586362 non-null  float64\n",
      " 16  ('pwat', 'amin')              586362 non-null  float64\n",
      " 17  ('pwat', 'amax')              586362 non-null  float64\n",
      " 18  ('pwat', 'mean')              586362 non-null  float64\n",
      " 19  ('pwat', 'var')               586362 non-null  float64\n",
      " 20  ('paramId_0', 'amin')         586362 non-null  float64\n",
      " 21  ('paramId_0', 'amax')         586362 non-null  float64\n",
      " 22  ('paramId_0', 'mean')         586362 non-null  float64\n",
      " 23  ('paramId_0', 'var')          586362 non-null  float64\n",
      " 24  ('pres', 'amin')              106334 non-null  float64\n",
      " 25  ('pres', 'amax')              106334 non-null  float64\n",
      " 26  ('pres', 'mean')              106334 non-null  float64\n",
      " 27  ('pres', 'var')               51233 non-null   float64\n",
      " 28  ('pres', 'count')             586362 non-null  int64  \n",
      " 29  ('macro_season', '<lambda>')  586362 non-null  int64  \n",
      " 30  ('month', '<lambda>')         586362 non-null  int64  \n",
      "dtypes: float64(28), int64(3)\n",
      "memory usage: 138.7 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 586362 entries, 0 to 586361\n",
      "Data columns (total 27 columns):\n",
      " #   Column                        Non-Null Count   Dtype  \n",
      "---  ------                        --------------   -----  \n",
      " 0   ('cwat', 'amin')              586362 non-null  float64\n",
      " 1   ('cwat', 'amax')              586362 non-null  float64\n",
      " 2   ('cwat', 'mean')              586362 non-null  float64\n",
      " 3   ('cwat', 'var')               586362 non-null  float64\n",
      " 4   ('r', 'amin')                 586362 non-null  float64\n",
      " 5   ('r', 'amax')                 586362 non-null  float64\n",
      " 6   ('r', 'mean')                 586362 non-null  float64\n",
      " 7   ('r', 'var')                  586362 non-null  float64\n",
      " 8   ('tozne', 'amin')             586362 non-null  float64\n",
      " 9   ('tozne', 'amax')             586362 non-null  float64\n",
      " 10  ('tozne', 'mean')             586362 non-null  float64\n",
      " 11  ('tozne', 'var')              586362 non-null  float64\n",
      " 12  ('gh', 'amin')                586362 non-null  float64\n",
      " 13  ('gh', 'amax')                586362 non-null  float64\n",
      " 14  ('gh', 'mean')                586362 non-null  float64\n",
      " 15  ('gh', 'var')                 586362 non-null  float64\n",
      " 16  ('pwat', 'amin')              586362 non-null  float64\n",
      " 17  ('pwat', 'amax')              586362 non-null  float64\n",
      " 18  ('pwat', 'mean')              586362 non-null  float64\n",
      " 19  ('pwat', 'var')               586362 non-null  float64\n",
      " 20  ('paramId_0', 'amin')         586362 non-null  float64\n",
      " 21  ('paramId_0', 'amax')         586362 non-null  float64\n",
      " 22  ('paramId_0', 'mean')         586362 non-null  float64\n",
      " 23  ('paramId_0', 'var')          586362 non-null  float64\n",
      " 24  ('pres', 'count')             586362 non-null  int64  \n",
      " 25  ('macro_season', '<lambda>')  586362 non-null  int64  \n",
      " 26  ('month', '<lambda>')         586362 non-null  int64  \n",
      "dtypes: float64(24), int64(3)\n",
      "memory usage: 120.8 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    539319\n",
       "1     47043\n",
       "Name: fire, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threeday_features_full = threeday_df.iloc[:,9:40]\n",
    "threeday_features_no_null = threeday_df.iloc[:,np.r_[9:33, 37:40]]\n",
    "threeday_features_full.info()\n",
    "threeday_features_no_null.info()\n",
    "threeday_target = threeday_df[\"fire\"]\n",
    "threeday_target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABa4AAAV+CAYAAACeXakkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAEAAElEQVR4nOz9fbikVX3n+78/AkEiIA9qpwUmbQacX1QSDB1kjmeSHUmAiBM0RwXHSJNwQuLoUU+YExpPzsBIcNozQY0YnSHC0BpAGNSBw4OkRXccZ3gQjBERHVB6pIWA0oDdJjI0+f7+uFdB9e791L2fqna9X9dV165ada+7vnftvfZdte61vitVhSRJkiRJkiRJg+JZSx2AJEmSJEmSJEn97LiWJEmSJEmSJA0UO64lSZIkSZIkSQPFjmtJkiRJkiRJ0kCx41qSJEmSJEmSNFDsuJYkSZIkSZIkDRQ7rgdYkn+b5F1LHccgSbI1yc/sQr1PJzl+IWKSJmP7XRhJViS5O8meSx2Llj/b8cLxvKxBYBufXpLfSPLJpY5D2hXD2L6T/Psk/88u1LOtalkZxvY7X/yMvCM7rgdUkucDpwD/oT0eSzK+SK+9Kkkl2b2v7NQkl8yy/jlJzlmI2Kpq76r6zizjqL6H64DzFiImaSLb7/zqj7+qHgK+AJy+pEFp2bMdzz/PyxoktvEp931JklMBquoa4GVJfm4hXktaKLvSvieco6bbblWSjXONcTJV9ftVde4s47Ctalka1vY7F35Gnp4d14PrVOD6qvr7pQ5kOaiq24B9k6xe6lg0Ek7F9ruQLgV+b6mD0LJ3KrbjBeN5WQPgVGzj20my2yTFl+PFYg2fU5ll++6/gDTkbKtaLk5l9Nrv0/yMvCM7rgfXrwN/NdWTSV6aZEOSzUkeSvLuJM9O8vdJnte2+aMk25Ls2x7/cZIPtvsnJPnrJD9Mcv+EURtfbD8fa6k5/umuHkSS/ZNcm+T7SR5t9w/ue368xfXf2mv9f0kOTHJpi+3LSVb1bV9JDm33L0nyZ0muS7Ilya1J/vE04YwDJ+zqsUg7wfY7efv90xbvD5PckeSf9T13fZLz+x5fkeTiKUK7FfiZJD+9q8cmzcJyacdjSTYl+cMkDyd5MMlrk7w6yX9v8b+7b/tnJVmb5NtJHklyZZID+p7/T0n+NsnjSb6Y5KV9z3le1jBZLm387iSv6Xu8e5IfJPmF9nimNvvRdg7+EfArk7zEOLZTDZ+Z2ncleVuSe4B75vJCfefMLUm+keR1fc+dmuS/JvlAkseSfCfJ/9LK72/n5TV921+S5I/b/d75+4y+8/dvTxPKOLZVLQ/D2n6nPO8nOanV731e+PV2bn7+FKGNY3t+RlV5G8Ab8H3gF6d4bh/gQeAM4Nnt8Svac18E/rd2/y+BbwO/3vfc69r9MeBwuosXPwc8BLy2PbcKKGD3eTiOA4H/DfjJFud/Av5z3/PjwL3APwaeC3wD+O/ArwK7Ax8H/mPf9gUc2u5fAmwGjmrbXgp8cppY/gD49FL/br0t/5vtd8r2+1ttn7u34/9b4NntuZ8CHgZeBbwZ+A6wzzSxfQ34jaX+XXtbvrdl1I7HgG3Avwb2AH63HdtlLe6XAj8GfqZt/y7gFuBgYE+6aZqX9+3vd1q9PYEPAl/te+4SPC97G5LbMmrj/xq4tO/xCcA3+x7P1GYfB17Z4nz2JPs/oMW671L/zrx5m+1tuvbdni9gQ/v73muOr/UG4IWtDZ0E/AhY2Z47tZ2DfxvYDfhj4LvAn7U2eSywBdi7bX8J8Mftfu/8/Z52/n418HfA/lPEYVv1tixuQ9x+pzzvt+cvbW38QOAB4DXTxOVn5L6bI64H1350jWAyrwH+tqrOr6ofV9WWqrq1PfdXwC+nmzLxc8CH2uNnA78I/BeAqhqvqjur6h+q6mt0U4t+eb4PoqoeqapPVdXfVdUWulw9E1/nP1bVt6vqceAG4NtV9bmq2kbXUfbyaV7i01V1W9v2UuCIabbdQve+SgttP2y/O7TfqvqLts9tVXU+3Qn/n7Tn/hb4fWA98KfAKe01p2J71kLbj2XQjpsngfOq6kngk8DzgD9tcd8F3NVihS4Nz/9dVZuq6gngHOD17Xioqotbvd5zP5/kuX2v5XlZw2I/lkcbvwz4jSQ/2R7/i1ZGi2OmNnt1Vf3XFuePJ9l/7z3ab/5DlxbMfkzdvnv+bVVtrjmmC6qq/1RVD7Q2dAXdCNCj+ja5r6r+Y1U9BVwBHAK8p6qeqKq/BP4ncOgUu3+ybftkVV0PbKV9dp6EbVXLxX4MYfudxXn/bXSDtMaB/6+qrp0mND8j97HjenA9Sjc6YjKH0I3umMxf0V3p+QXgTrorUb8MHA3cW1U/AEjyiiRfSJcC4HG6DqPnzV/4nSQ/meQ/JPkfSX5INxJlv2yfQ++hvvt/P8njvad5ib/tu/93M2y7D/DYrAKX5sb2+8zjp9tkm+p4d5uu/BjdKO3+uK+lu5r9rar60gzh2Z610JZFO24eaR+4oWuXMHVb/WngM21K5GPA3cBTwIokuyVZ16ZU/hDY2Or0x+15WcNiWbTxqrqXrp3+89Z5/Ru0jutZttn7Z3iJ3nv02LwFLS286dp3z0x/+7OS5JQkX+07b76M7dvYxPMt1S023l821bnykXYhuGe686ptVcvFULbfmc77VfUY3cCulwHnMz0/I/ex43pwfQ148RTP3U83NX8y/43uKuzrgL+qqm8A/4hu2mB/nqDLgGuAQ6rqucC/B9Kem9WKrLN0RovnFVW1L/BLrTxTV1kwPwv8zRK8rkaP7XeCdPmszwTeSDfFcT+66cn9+zqP7sv3yiRvmmZfu9Nd2bY9ayEtl3a8s+6nS3uwX9/t2VX1PbqRnCfSpQN6Ll26A9j1c7rnZS2l5dTGLwfeRNc+v9E6s2F2bXamWH4W2FhVP5y3aKWFN1377plzO0y33sqfA28HDmyfb7/O0n3Xta1qORjW9jvdeZ8kR9Cl77qcbrbWdPyM3MeO68F1PVNPJ7wW+Kkk70qyZ5J9krwCoKr+DriDbhpC78Pzf6Ob+tv/YXofYHNV/TjJUXQfbHu+D/wD8DNTBZdkY5JTZ3Ec+9BdhXos3eJOZ8+izkL5ZbpUBtJCs/1Ovq9tLb7dk/xrYN++mH6JLn/YKe12QZKDptjXUXQfzP/HHOKRZrJc2vHO+vfAee3DPEmen+TEvpifAB6hy33/3jm+ludlLaXl1MY/SZdr8630pQlhftqs7VTDaLr2PaMk5yQZn8Wmz6HrQPt+q/fbdKMpl4JtVcvFsLbfKc/7LZ3YXwDvpvvOe1CSfznNvmzPfey4HlwfB16dZK+JT7S8r78G/HO6Kbn3sP0q4H9Ft4DDbX2P9+GZFcwB/iXwniRb6BZ1ubJv/39HN/Lxv7YpE0f3v36Sn6BLKH/LLI7jg8BewA/a9p+dRZ15l+QXgR9V1W0zbizNne13RzfSnXz/O/A/6BaDu7/FtC/de/b2qvpeSxNyEfAfk0x2xfvNdJ1r0kJaLu14Z/0p3WiRv2yx3QK8oj33cbr2+z26xVh3+fU9L2sALJs2XlUPAjcD/wtdDs7+Y5xrm30T3SKt0jCZsn3P0iHAf51pozbj4ny69vcQ3cJsM9ZbILZVLRfD2n6nPO8D/xbYVFUfbWtO/Bbwx0kOm7gTPyPvKFVLORtV00nyXuDhqvrgUsfSL8n/Crytqqacyj9oknwKuKgtaiEtONvvwkjyAroOgpdPsYiUNG9sxwvH87IGgW18xjj+OfCWqnrjUsYh7Yq5tO8kXwWOqapH5juuhWBb1XIzSu13Ij8j78iOa0mSJEmSJEnSQDFViCRJkiRJkiRpoNhxLUmSJEmSJEkaKHZcS5IkSZIkSZIGih3XkiRJkiRJkqSBsvtSBzDfnve859WqVaum3eZHP/oRz3nOcxYnoCUyCscIo3Gckx3jHXfc8YOqev4ShbTghqkdG8eOBiWWQY/Ddjw4v6PFNIrHDMv3uEe9HQ/b79V4F9Ywxdsfq+14MH5vxjF4cQxCDLONYzm349l8ph40g/K3M1vGu/BminnaNlxVy+p25JFH1ky+8IUvzLjNsBuFY6wajeOc7BiB22sA2ttC3YapHRvHjgYllkGPw3Y8OL+jxTSKx1y1fI971NvxsP1ejXdhDVO8/bHajr8w7fOLxTi2NwhxDEIMVbOLYzm349l8ph40g/K3M1vGu/Bminm6NmyqEEmSJEmSJEnSQLHjWpIkSZIkSZI0UOy4liRJkiRJkiQNFDuuJUmSJEmSJEkDxY5rSZIkSZIkSdJAseNakiRJkiRJkjRQdl/qAJbCnd97nFPXXgfAxnUnLHE0knaF7VgafrZjjZokG4EtwFPAtqpaneQA4ApgFbAReGNVPdq2Pws4rW3/jqq6sZUfCVwC7AVcD7yzqirJnsDHgSOBR4CTqmpjq7MG+KMWyh9X1fq5Hk9/GwbbsaSds6rv/8clxz9nCSORNN9W+flA88QR15IkSdLi+ZWqOqKqVrfHa4Gbquow4Kb2mCQvAU4GXgocD3wkyW6tzkeB04HD2u34Vn4a8GhVHQp8AHhf29cBwNnAK4CjgLOT7L+gRylJkiTNkR3XkiRJ0tI5EeiNfl4PvLav/JNV9URV3QfcCxyVZCWwb1XdXFVFN8L6tZPs6yrgmCQBjgM2VNXmNpp7A890dkuSJEkDaSRThUiSJElLoIC/TFLAf6iqC4EVVfUgQFU9mOQFbduDgFv66m5qZU+2+xPLe3Xub/valuRx4MD+8knqSNKi6E8dIEnSbNhxLUnSEEjybOCLwJ505++rqursYc6PK42gV1bVA61zekOSb06zbSYpq2nKd7XO9i+anE6XhoQVK1YwPj4+ZYAr9oIzDt/29OPpth0EW7duHfgY+xnvwhmmWEeF615Iw2fixSjbrhaCHdfSCEhyCF1n1E8B/wBcWFV/muQc4HeB77dN311V17c6dnhJg+UJ4FVVtTXJHsCXktwA/CZdftx1SdbS5cc9c0J+3BcCn0vy4qp6imfy495C146PB26gLz9ukpPp8uOe1JcfdzVdZ9cdSa7pdZBLmp2qeqD9fDjJZ+jyTT+UZGUbbb0SeLhtvgk4pK/6wcADrfzgScr762xKsjvwXGBzKx+bUGd8ihgvBC4EWL16dY2NjU22GQAXXHo159/5zNeJjW+eettBMD4+znTHM2iMd+EMU6ySJI0yc1xLo2EbcEZV/SxwNPC21qkF8IG2SNQRfZ3WLgglDZjqbG0P92i3wvy40lBI8pwk+/TuA8cCXweuAda0zdYAV7f71wAnJ9kzyYvozrm3tbQiW5Ic3drnKRPq9Pb1euDzrZ3fCBybZP92Dj62lUmSJEkDyxHX0ghoX3J7+TO3JLmb6XNbPt3hBdyXpNfhtZHW4QWQpNfhdUOrc06rfxXw4YkdXq1Or8Pr8vk8RmkUtAtIdwCHAn9WVbcmMT+uNBxWAJ/pTo3sDlxWVZ9N8mXgyiSnAd8F3gBQVXcluRL4Bt0F6Le1GRMAb+WZ2U83tBvARcAn2nl7M91FaKpqc5JzgS+37d7TOy9LkiRJg8qOa2nEJFkFvBy4FXgl8PYkpwC3043KfhQ7vKSB1DqtjkiyH10H2Mum2XzR8+PuTG5c2D4/7qjkGh3VvKqjetz9quo7wM9PUv4IcMwUdc4Dzpuk/HZgh/ZfVT+mdXxP8tzFwMU7F7UkSZK0dOy4lkZIkr2BTwHvqqofJvkocC5dB9S5wPnA72CH16wNSmfMoMQBgxPLco6jqh5LMk43e2Fg8uPuTG5c2D4/7qDnxp0vo5pXdVSPW5IkSdKus+NaGhFtMbdPAZdW1acBquqhvuf/HLi2PbTDa5YGpTNmUOKAwYllucWR5PnAk63Tei/gV+lyyfdy2q5jx/y4lyV5P93ijL38uE8l2ZLkaLqZF6cAF/TVWQPcTF9+3CQ3Au/ty09/LHDWnA9KkiRJkqQpuDijNAJarumLgLur6v195Sv7Nnsd3SJR4IJQ0iBaCXwhydfo8tRuqKpr6Tqsfy3JPcCvtcdU1V1ALz/uZ9kxP+7H6BZs/Dbb58c9sOXH/QNgbdvXZrpZGV9uN/PjSpIkSZIWlCOupdHwSuAtwJ1JvtrK3g28KckRdKk7NgK/By4IJQ2iqvoaXX76ieXmx5UkScvGqrXXbfd447oTligSSdJSs+NaGgFV9SUmzzV9/TR17PCSJEmSJEnSkrDjWpIkSZIkSdKC659V4YwKzcQc15IkSZIkSZKkgWLHtSRJkiRJkiRpoNhxLUmSJEmSJEkaKLPquE6yMcmdSb6a5PZWdkCSDUnuaT/379v+rCT3JvlWkuP6yo9s+7k3yYeSpJXvmeSKVn5rklV9dda017gnyZp5O3JJkiRJkiRJ0kDamRHXv1JVR1TV6vZ4LXBTVR0G3NQek+QlwMnAS4HjgY8k2a3V+ShwOnBYux3fyk8DHq2qQ4EPAO9r+zoAOBt4BXAUcHZ/B7kkSZIkSZI0H5LsluSvk1zbHi/KoE1Jk5tLqpATgfXt/nrgtX3ln6yqJ6rqPuBe4KgkK4F9q+rmqirg4xPq9PZ1FXBMa9jHARuqanNVPQps4JnObkmSJEmSJGm+vBO4u+/xgg/alDS13We5XQF/maSA/1BVFwIrqupBgKp6MMkL2rYHAbf01d3Uyp5s9yeW9+rc3/a1LcnjwIH95ZPUkSRJkiRJA2rV2uuWOgRp1pIcDJwAnAf8QSs+ERhr99cD48CZ9A3aBO5L0hu0uZE2aLPtszdo84ZW55y2r6uADydJG9wpaRKz7bh+ZVU90DqnNyT55jTbZpKymqZ8V+s884LJ6XRXs1ixYgXj4+PThAcr9oIzDt8GMOO2w2rr1q3L9tj6jcJxjsIxSpIkScOujba8HfheVb2mpb68AlgFbATe2GYSk+QsutGXTwHvqKobW/mRwCXAXsD1wDurqpLsSTdr+UjgEeCkqtq4aAcnjYYPAn8I7NNXthiDNn8wv4chLR+z6riuqgfaz4eTfIYu3/RDSVa2hrsSeLhtvgk4pK/6wcADrfzgScr762xKsjvwXGBzKx+bUGd8kvguBC4EWL16dY2NjU3cZDsXXHo159/ZHfrGN0+/7bAaHx9npvdhORiF4xyFY5QkSZKWgV6KgX3b416KgXVJ1rbHZ05IMfBC4HNJXlxVT/FMioFb6Dquj6cbqfl0ioEkJ9OlGDhp8Q5NWt6SvAZ4uKruSDI2myqTlO3qoM2JsezU4Myl0hsQ2tOLc+vWrZxx+FM7lE9WbxCObdgGCw5bvDC3mGfsuE7yHOBZVbWl3T8WeA9wDbAGWNd+Xt2qXANcluT9dCfhw4DbquqpJFuSHA3cCpwCXNBXZw1wM/B64PPtqvKNwHv7kt8fC5y1S0cqSZIkSdICMMXAwulPN7Jx3QlLGImWuVcCv5Hk1cCzgX2T/AWLM2hzOzs7OHOpnDohFVBvYOj4+Djnf+lHO5RPVm8QBpMO22DBYYsX5hbzbBZnXAF8KcnfALcB11XVZ+k6rH8tyT3Ar7XHVNVdwJXAN4DPAm9rV44B3gp8jG7Bxm/TnYABLgIObCfsP6Alu6+qzcC5wJfb7T2tTJIkSZKkQfFBuhQD/9BXtl2KAaA/xcBkazkdxCxTDAC9FAOS5kFVnVVVB1fVKroZEZ+vqt/imYGWsOOgzZOT7JnkRTwzaPNBYEuSo5OEbtBmf53evp4etLnQxyYNsxlHXFfVd4Cfn6T8EeCYKeqcR3eleWL57cDLJin/MfCGKfZ1MXDxTHFKkiRJkrTYBinFQItn1mkGFnrK+cRUAlPpX4dqOgs9PX5QpuAPQhyDEMOAxLEOuDLJacB3aX1XVXVXkt6gzW3sOGjzErpc9Tew/aDNT7RBm5vpOsglTWO2izNKkiRJkqQdDUyKAdi5NAMLPeV8YiqBqZxx+Lan16GazkKnFRiUKfiDEMcgxLBUcVTVOG19tcUatClpcrNJFSJJkiRJkiZhigFJkhaGI64lSZIkSZp/phiQJGkO7LiWJEmSJGkemGJAknbdqr70QhvXnbCEkWhQmCpEkiRJkiRJkjRQ7LiWJGkIJDkkyReS3J3kriTvbOXnJPlekq+226v76pyV5N4k30pyXF/5kUnubM99qOXRpOXavKKV35pkVV+dNUnuabc1SJIkSZK0gEwVIknScNgGnFFVX0myD3BHkg3tuQ9U1Z/0b5zkJXT5L18KvBD4XJIXtxyaHwVOB24BrgeOp8uheRrwaFUdmuRk4H3ASUkOAM4GVgPVXvuaqnp0gY9ZkiRJkjSiHHEtSdIQqKoHq+or7f4W4G7goGmqnAh8sqqeqKr7gHuBo5KsBPatqpurqoCPA6/tq7O+3b8KOKaNxj4O2FBVm1tn9Qa6zm5JOynJbkn+Osm17fEBSTa02Qwbkuzft62zJiRJkjSyHHEtSdKQaZ1RLwduBV4JvD3JKcDtdKOyH6Xr1L6lr9qmVvZkuz+xnPbzfoCq2pbkceDA/vJJ6vTHdTrdSG5WrFjB+Pj4tMexYi844/BtADNuu1xs3bp1ZI6136ge9xTeSXfhad/2eC1wU1WtS7K2PT7TWROSJEkadXZcS5I0RJLsDXwKeFdV/TDJR4Fz6TqjzgXOB34HyCTVa5pydrHOMwVVFwIXAqxevbrGxsamPZYLLr2a8+/sPopsfPP02y4X4+PjzPS+LEejetwTJTkYOAE4D/iDVnwiMNburwfGgTPpmzUB3JekN2tiI23WRNtnb9bEDa3OOW1fVwEfnjhrotXpzZq4fGGOVJIkSZo7O64lSRoSSfag67S+tKo+DVBVD/U9/+fAte3hJuCQvuoHAw+08oMnKe+vsynJ7sBzgc2tfGxCnfH5OCZpxHwQ+ENgn76yFVX1IHQpgZK8oJUv+qwJ2LmZE/2zJmDwZ04M28h/4104wxSrJEmjzI5raQQkOYQuj+1PAf8AXFhVf9qmDl8BrAI2Am/sTRtOchbdlOOngHdU1Y2t/EjgEmAvuunJ76yqSrJne40jgUeAk6pqY6uzBvijFs4fV1Uvh66kWWqjJi8C7q6q9/eVr+x1egGvA77e7l8DXJbk/XRpBg4Dbquqp5JsSXI0XaqRU4AL+uqsAW4GXg98vrXvG4H39uXePRY4a6GOVVqOkrwGeLiq7kgyNpsqk5Qt6KwJ2LmZE/2zJmDwZ04M28h/4104wxSrJEmjzMUZpdGwjS7v7c8CRwNva7kze3k1DwNuao+ZkFfzeOAjSXZr++rl1Tys3XoLtD2dVxP4AF1eTfryar4COAo4u3/hKUmz9krgLcCrkny13V4N/L9tkbavAb8C/J8AVXUXcCXwDeCzwNtablyAtwIfo1uw8dt0KQag6xg/sKUk+APa/4SWXuBc4Mvt9p5eygFJs/ZK4Ddaqo9P0rXlvwAeaoum0n4+3Lafy6wJJpk1Mdm+JEmSpIHliGtpBLTRmL1pyFuS3E03Rdi8mtKQqKovMfmoyeunqXMeXS7dieW3Ay+bpPzHwBum2NfFwMWzjVfS9qrqLNpMhTbi+l9V1W8l+Xd0Mx3WtZ9XtyrOmpCkCVatve7p+xvXnbCEkUiSFoMd19KISbIKeDndl92ByqspSdIIWgdcmeQ04Lu0i0dVdVeS3qyJbew4a+ISurRdN7D9rIlPtAvOm+lmT1FVm5P0Zk2AsyYkSZI0BOy4lkZIkr3pFnZ7V1X9sBsQPfmmk5QtaF7NnVkMCrZfEGopF9cZlMV9BiUOGJxYjEPSoKqqcdoCp1X1CHDMFNs5a0KSJC0pZzpoKdlxLY2IJHvQdVpfWlWfbsUP9RZ2m8e8mpsmyas5NqHO+MT4dmYxKNh+QailXAxqUBb3GZQ4YHBiMQ5JkiRJkoaXizNKI6Dlmr4IuLuq3t/3VC8XJuyYV/PkJHsmeRHP5NV8ENiS5Oi2z1Mm1Ont6+m8msCNwLFJ9m+5NY9tZZIkSZIkSdKkHHEtjYZXAm8B7kzy1Vb2bsyrKUmSJEmSpAFkx7U0AqrqS0yeaxrMqylJkiRJkqQBY6oQSZIkSZIkSdJAmXXHdZLdkvx1kmvb4wOSbEhyT/u5f9+2ZyW5N8m3khzXV35kkjvbcx9qOXJpeXSvaOW3JlnVV2dNe417kqxBkiRJkiRJkrSs7UyqkHcCdwP7tsdrgZuqal2Ste3xmUleQpfb9qXAC4HPJXlxy4/7UeB04BbgeuB4uvy4pwGPVtWhSU4G3geclOQA4GxgNVDAHUmuqapH53TUkiRJkiRpXq1ae91ShyBJWkZmNeI6ycHACcDH+opPBNa3++uB1/aVf7Kqnqiq+4B7gaOSrAT2raqbq6qAj0+o09vXVcAxbTT2ccCGqtrcOqs30HV2S5IkSZIkSZKWqdmOuP4g8IfAPn1lK6rqQYCqejDJC1r5QXQjqns2tbIn2/2J5b0697d9bUvyOHBgf/kkdSRJkiRJkiQtYxNnc2xcd8ISRaLFNmPHdZLXAA9X1R1Jxmaxz0xSVtOU72qd/hhPp0tBwooVKxgfH582wBV7wRmHbwOYcdthtXXr1mV7bP1G4ThH4RglSZIkSZKkfrMZcf1K4DeSvBp4NrBvkr8AHkqyso22Xgk83LbfBBzSV/9g4IFWfvAk5f11NiXZHXgusLmVj02oMz4xwKq6ELgQYPXq1TU2NjZxk+1ccOnVnH9nd+gb3zz9tsNqfHycmd6H5WAUjnMUjlGSJEmSJEnqN2OO66o6q6oOrqpVdIsufr6qfgu4BljTNlsDXN3uXwOcnGTPJC8CDgNua2lFtiQ5uuWvPmVCnd6+Xt9eo4AbgWOT7J9kf+DYViZJkiRJkiRJWqZmm+N6MuuAK5OcBnwXeANAVd2V5ErgG8A24G1V9VSr81bgEmAv4IZ2A7gI+ESSe+lGWp/c9rU5ybnAl9t276mqzXOIWZIkSZIkDTlz3krS8rdTHddVNU5L1VFVjwDHTLHdecB5k5TfDrxskvIf0zq+J3nuYuDinYlTkiRJkiRJkjS8ZkwVIkmSJEmSJC1XSZ6d5LYkf5PkriT/ppUfkGRDknvaz/376pyV5N4k30pyXF/5kUnubM99qKXLpaXUvaKV35pk1aIfqDRk7LiWJEmSJEnSKHsCeFVV/TxwBHB8kqOBtcBNVXUYcFN7TJKX0KW5fSlwPPCRJLu1fX0UOJ1uzbfD2vMApwGPVtWhwAeA9y3CcUlDzY5rSZKGQJJDknwhyd1tFMg7W/mijAJJsqa9xj1J1iBJkp7maE1puFVna3u4R7sVcCKwvpWvB17b7p8IfLKqnqiq+4B7gaOSrAT2raqbq6qAj0+o09vXVcAxvfYtaXJzWZxRkiQtnm3AGVX1lST7AHck2QCcSjcKZF2StXSjQM6cMArkhcDnkry4LZjcGwVyC3A93SiQG+gbBZLkZLpRICclOQA4G1hN9wH+jiTXVNWji3b0kiQNtt5oza1J9gC+lOQG4DdZ4PP04h6mtHy1EdN3AIcCf1ZVtyZZUVUPAlTVg0le0DY/iK6N9mxqZU+2+xPLe3Xub/valuRx4EDgBxPiOJ3ufwArVqxgfHx83o5xV5xx+Lan7/fH0l/e/9zWrVs54/CnJq0z3f525bXmw9atW5f8Pd4ZwxYvzC1mO64lSRoC7QNz70PzliR30334PREYa5utp1tE+Uz6RoEA9yXpjQLZSBsFApCkNwrkhlbnnLavq4APt1EgxwEbqmpzq7OB7kv05Qt2wJIkDZE2snKq0ZpjrXxBztPttSXNUbtwdESS/YDPJHnZNJtPNlK6pimfrs7EOC4ELgRYvXp1jY2NTRPGwjt17XVP39/45rFJy/ufGx8f5/wv/WjSOtPtb1deaz6Mj4+z1O/xzhi2eGFuMZsqRJKkIdOmBr8cuBXYbhQI0D8K5P6+ar3RHgcxy1EgQG8UyFT7kiRJTZLdknwVeJjugu9inaclzaOqeozuItPxwEMt/Qft58Nts03AIX3VDgYeaOUHT1K+XZ0kuwPPBTYvxDFIy4UjriVJGiJJ9gY+Bbyrqn44TVq8+RwFMqvRITs7rXHFXs9M+xu26W67ahin9s2HUT1uSaNlUEZr7sz5eL7/P0+czj9b/Z8JdtV8HMegnK8GIY5BiGEx40jyfODJqnosyV7Ar9Kl47kGWAOsaz+vblWuAS5L8n66dD+HAbdV1VNJtrSFHW8FTgEu6KuzBrgZeD3weWdMSNOz41qSpCHRcmZ+Cri0qj7dih9KsrLl3JuvUSCbJowC2cQz05x7dcYnxrez0xovuPRqzr+z+ygyn9P9BtkwTu2bD6N63JJGU+v4GqdvtOYCn6cnvv6sz8fz/f954nT+2Trj8G1PfybYVfPxWWJQzleDEMcgxLDIcawE1rc8188Crqyqa5PcDFyZ5DTgu8AbAKrqriRXAt+gW4vmbe3iFcBbgUuAvejS/NzQyi8CPtFSA22my3MvaRqmCpEkaQi0XNMXAXdX1fv7nuqN3IAdR4GcnGTPJC/imVEgDwJbkhzd9nnKhDq9ffWPArkRODbJ/kn2B45tZZIkiW60ZhtpTd9ozW+yOOdpSXNUVV+rqpdX1c9V1cuq6j2t/JGqOqaqDms/N/fVOa+q/nFV/ZOquqGv/Pa2j39cVW/vtdOq+nFVvaGqDq2qo6rqO4t/pNJwccS1JEnD4ZXAW4A7W/5MgHfTTVtc0FEgVbU5ybnAl9t27+n/0C5JkhytKUnSfLPjWpKkIVBVX2Ly3JYAx0xR5zzgvEnKbwd2yLtZVT+mfaGe5LmLgYtnG68kSaOkqr5Gt3DyxPJHWITztCRJy5GpQiRJkqQFluTZSW5L8jdJ7kryb1r5AUk2JLmn/dy/r85ZSe5N8q0kx/WVH5nkzvbch1o6AVrKgSta+a1JVvXVWdNe454ka5AkSZIGnB3XkiRJ0sJ7AnhVVf08cARwfJKjgbXATVV1GHBTe0ySl9ClAXgp3QJvH2kpCAA+CpxOlxP3sPY8wGnAo1V1KPAB4H1tXwcAZwOvAI4Czu7vIJckSZIGkalCJEmSpAXWFmba2h7u0W4FnAiMtfL1wDhwZiv/ZFU9AdzXctoelWQjsG9V3QyQ5OPAa+ly4J4InNP2dRXw4TYa+zhgQy83fZINdJ3dly/IwUqSJC2SVWuv2+7xxnUnLFEkWgh2XEsjIMnFwGuAh6vqZa3sHOB3ge+3zd5dVde3586iG7X1FPCOqrqxlR/JMwvFXA+8s6oqyZ7Ax4EjgUeAk6pqY6uzBvij9hp/XFXrF/RgJUkaUG3E9B3AocCfVdWtSVZU1YMAVfVgkhe0zQ8CbumrvqmVPdnuTyzv1bm/7WtbkseBA/vLJ6kzMcbT6UZzs2LFCsbHx6c8nhV7wRmHb3v68XTbDoKtW7cOfIz9jHfhDFOskiSNMjuupdFwCfBhus7lfh+oqj/pL5gwNfmFwOeSvLitct6bmnwLXcf18XQjvJ6empzkZLqpySf1TU1eTTeq7I4k11TVowtzmJIkDa52Lj0iyX7AZ5LssPhan8kWY61pyne1zsQYLwQuBFi9enWNjY1NGeAFl17N+Xc+83Vi45un3nYQjI+PM93xDBrjXTjDFKskSaPMHNfSCKiqLwKbZ7n501OTq+o+oDc1eSVtanKb7tybmtyr0xtJfRVwzMSpya2zujc1WZKkkVVVj9GlBDkeeKidY2k/H26bbQIO6at2MPBAKz94kvLt6iTZHXgu3fl/qn1J0rKxau11T98kScuDI66l0fb2JKcAtwNntM7lgZ+aDNtPT17KqZ6DMtV0UOKAwYnFOCQNkiTPB56sqseS7AX8Kt0MpWuANcC69vPqVuUa4LIk76ebAXUYcFtVPZVkS1vY8VbgFOCCvjprgJuB1wOfbym9bgTe27cg47HAWQt7xJIkSdLc2HEtja6PAufSTRU+Fzgf+B2GYGoybD89eSmnJg/KVNNBiQMGJxbjkDRgVgLrW57rZwFXVtW1SW4GrkxyGvBd4A0AVXVXkiuBbwDbgLe1VCMAb+WZNSduaDeAi4BPtIUcN9Ol/qKqNic5F/hy2+49vYUaJUmSpEFlx7U0oqrqod79JH8OXNsezmVq8qZJpiaPTagzPl/HIEnSsKiqrwEvn6T8EeCYKeqcB5w3SfntwA75savqx7SO70meuxi4eOeiliRJkpaOOa6lEdXLp9m8Dvh6u38NcHKSPZO8iGemJj8IbElydMtffQrbT2de0+4/PTUZuBE4Nsn+bXrysa1MkiRJkiRJmpIjrqURkORyupHPz0uyCTgbGEtyBF3qjo3A74FTkyVJkiRJkrT0Zuy4TvJs4IvAnm37q6rq7CQHAFcAq+g6vd7YFnYjyVnAacBTwDuq6sZWfiTPdHpdD7yzLRizJ/Bx4EjgEeCkqtrY6qwB/qiF88dVtX7ORy2NmKp60yTFF02zvVOTJUmSJEmStGRmkyrkCeBVVfXzwBHA8W0V87XATVV1GHBTe0ySl9CNtnwpcDzwkbYIDXSLwZ1Ol3rgsPY8dJ3cj1bVocAH6FZYp3WOnw28AjgKOLtvNXRJkiRJkiRJ0jI0Y8d1dba2h3u0WwEnAr3Rz+uB17b7JwKfrKonquo+4F7gqJZPd9+qurnlvv34hDq9fV0FHNNy6B4HbKiqzW009wae6eyWJEmSJEmSJC1Ds8px3UZM3wEcCvxZVd2aZEVbrI2qejDJC9rmBwG39FXf1MqebPcnlvfq3N/2tS3J48CB/eWT1OmP73S6kdysWLGC8fHxaY9nxV5wxuHbAGbcdlht3bp12R5bv1E4zlE4RkmSJEmSJKnfrDqu28JsRyTZD/hMkh1y3PbJZLuYpnxX6/THdyFwIcDq1atrbGxsmvDggkuv5vw7u0Pf+Obptx1W4+PjzPQ+LAejcJyjcIySJEmSJElSv1l1XPdU1WNJxunSdTyUZGUbbb0SeLhttgk4pK/awcADrfzgScr762xKsjvwXGBzKx+bUGd8Z2KWJEmSJEkLY9Xa65Y6BEnSMjVjjuskz28jrUmyF/CrwDeBa4A1bbM1wNXt/jXAyUn2TPIiukUYb2tpRbYkObrlrz5lQp3evl4PfL7lwb4RODbJ/m1RxmNbmSRJkiRJkiRpmZrNiOuVwPqW5/pZwJVVdW2Sm4Erk5wGfBd4A0BV3ZXkSuAbwDbgbS3VCMBbgUuAvYAb2g3gIuATSe6lG2l9ctvX5iTnAl9u272nqjbP5YAlSZIkSZIkSYNtxo7rqvoa8PJJyh8BjpmiznnAeZOU3w7skB+7qn5M6/ie5LmLgYtnilOSpOUsycXAa4CHq+plrewc4HeB77fN3l1V17fnzgJOA54C3lFVN7byI3nmIvL1wDurqpLsCXwcOBJ4BDipqja2OmuAP2qv8cdVtX5BD1aSJGkOJqYv2bjuhCWKRBodvXZ3xuHb2MnMxNKU/EuSJGk4XAJ8mK5zud8HqupP+guSvIRu9tJLgRcCn0vy4jYD6qPA6cAtdB3Xx9PNgDoNeLSqDk1yMvA+4KQkBwBnA6vpFki+I8k1VfXowhymJEmSpMXUf7Fnvi/0mAdfczFjjmtJkrT0quqLdOm0ZuNE4JNV9URV3QfcCxzVFlPet6pubmtJfBx4bV+d3kjqq4Bj2poUxwEbqmpz66zeQNfZLUmSJEkDa9Xa656+aTg54lqSpOH29iSnALcDZ7TO5YPoRlT3bGplT7b7E8tpP+8HqKptSR4HDuwvn6TOdpKcTjeamxUrVjA+Pj5t4Cv26k0lZMZtl4utW7eOzLH2G9XjliRJkrTr7LiWJGl4fRQ4ly6Fx7nA+cDvAJlk25qmnF2ss31h1YXAhQCrV6+usbGxaUKHCy69mvPv7D6KbHzz9NsuF+Pj48z0vixHo3rckiRJknadqUIkSRpSVfVQVT1VVf8A/DlwVHtqE3BI36YHAw+08oMnKd+uTpLdgefSpSaZal+SJEmSJC0YO64lSRpSLWd1z+uAr7f71wAnJ9kzyYuAw4DbqupBYEuSo1v+6lOAq/vqrGn3Xw98vuXBvhE4Nsn+SfYHjm1lkiRJkiQtGDuuJUkaAkkuB24G/kmSTUlOA/7fJHcm+RrwK8D/CVBVdwFXAt8APgu8raqeart6K/AxugUbvw3c0MovAg5Mci/wB8Datq/NdGlIvtxu72llkiRJ0rKQ5JAkX0hyd5K7kryzlR+QZEOSe9rP/fvqnJXk3iTfSnJcX/mR7TP6vUk+1AaM0AaVXNHKb02yatEPVBoy5riWJGkIVNWbJim+aJrtzwPOm6T8duBlk5T/GHjDFPu6GLh41sFKkiRJw2Ub3ULnX0myD3BHkg3AqcBNVbUuyVq6wR1nJnkJcDLwUuCFwOeSvLgNFvko3YLltwDXA8fTDRY5DXi0qg5NcjLwPuCkRT1Kacg44lqSJEmSpDlwtKY03Krqwar6Sru/BbgbOAg4EVjfNlsPvLbdPxH4ZFU9UVX30c1mPKql8tu3qm5uafc+PqFOb19XAcf02vewWLX2uqdv0mJwxLWkodd/0ty47oQljESSJEkjytGa0jLRLgq9HLgVWNHWiaGqHkzygrbZQXRttGdTK3uy3Z9Y3qtzf9vXtiSPAwcCP1iYIxl8doBrJnZcS5IkSZI0B61jq9e5tSVJ/2jNsbbZemAcOJO+0ZrAfW2NiaOSbKSN1gRI0huteUOrc07b11XAh5OkjerUNBzootlKsjfwKeBdVfXDaQZET/ZETVM+XZ2JMZxOd/GKFStWMD4+PkPUc3fG4duevj/x9fqfm40Ve+18ncn0xzFxf7vy3FTv49atWxflPZ4vwxYvzC1mO64lSZIkSZonSz1ac2c6veajA2Q+Oqjmq6NrNhb6/ZgPgxDHIMSw2HEk2YOu0/rSqvp0K34oycrWflcCD7fyTcAhfdUPBh5o5QdPUt5fZ1OS3YHnAjssel5VFwIXAqxevbrGxsbm4eimd2r/xZ03j0353Gyccfg2zr9z7t2N/XFMjGFXnpt4XD3j4+Msxns8X4YtXphbzHZcS5IkSZI0DwZhtObOdHrNRwfIznZqTWa+OrpmY6rOKxicDqFBiGMQYljMOFqu6YuAu6vq/X1PXQOsAda1n1f3lV+W5P106X4OA26rqqeSbElyNN3Fq1OACybs62bg9cDnnTEhTc+Oa0mSJEmS5mhQRmtK2iWvBN4C3Jnkq63s3XQd1lcmOQ34LvAGgKq6K8mVwDfocty/reWoB3grcAmwF12anxta+UXAJ1pqoM10ee4lTeNZSx2AJEmStNwlOSTJF5LcneSuJO9s5Qck2ZDknvZz/746ZyW5N8m3khzXV35kkjvbcx9qo8RIsmeSK1r5rS1dQa/OmvYa9yRZs4iHLo2EWYzWhB1Ha57c2u2LeGa05oPAliRHt32eMqFOb1+O1pTmUVV9qapSVT9XVUe02/VV9UhVHVNVh7Wfm/vqnFdV/7iq/klV3dBXfntVvaw99/ZeO62qH1fVG6rq0Ko6qqq+sxTHKg0TO64lSZKkhbcNOKOqfhY4GnhbkpcAa4Gbquow4Kb2mPbcycBLgeOBjyTZre3ro3T5aw9rt+Nb+WnAo1V1KPAB4H1tXwcAZwOvAI4Czu7vIJc0L3qjNV+V5Kvt9mq60Zq/luQe4NfaY6rqLqA3WvOz7Dha82PAvcC32X605oFttOYf0P5fSJK0XNlxLY2AJBcneTjJ1/vKHOElSdIiqaoHq+or7f4W4G66hdZOBNa3zdYDr233TwQ+WVVPVNV9dB1YR7VUA/tW1c1tBNfHJ9Tp7esq4Jh2rj4O2FBVm6vqUWADz3R2S5oHjtaUJGn+meNaGg2XAB+m+3Lb0xvhtS7J2vb4zAkjvF4IfC7Ji9sIkN4Ir1uA6+m+9N5A3wivJCfTjfA6qW+E12q6hWPuSHJN+9IsSdJIahd4X063aNOKlhqAlgP3BW2zg+jOtz2bWtmT7f7E8l6d+9u+tiV5HDiwv3ySOhNjO53uXM+KFSsYHx+f8jhW7NUtqNYz3baDYOvWrQMfYz/jXTjDFKskSaPMjmtpBFTVF/tHQTcnAmPt/npgHDiTvhFewH1tKuJRSTbSRngBJOmN8Lqh1Tmn7esq4MMTR3i1Or0RXpfP9zFKkjQMkuxNt3jbu6rqh23y0qSbTlJW05Tvap3tC6suBC4EWL16dY2NjU0VHxdcejXn3/nM14mNb55620EwPj7OdMczaIx34QxTrJKkhbFq7XVAdxF+bGlD0TTsuJZG19CO8IIdR3n1LPbomUEZsTMoccDgxGIckgZNkj3oOq0vrapPt+KHkqxs5+KVwMOtfBNwSF/1g4EHWvnBk5T319mUZHfgucDmVj42oc74PB2WJEnSvOt1LGu02XEtaaKBH+EFO47y6lns0V6DMmJnUOKAwYnFOCQNkjYT6SLg7qp6f99T1wBr6BZsWwNc3Vd+WZL306XuOgy4raqeSrIlydF0qUZOAS6YsK+bgdcDn6+qSnIj8N6+9SyOBc5aoEOVJEmS5sWMizMmOSTJF5LcneSuJO9s5S7sJg23h9rILuZxhBeTjPCabF+SJI2aVwJvAV6V5Kvt9mq6DutfS3IP8GvtMVV1F3Al8A3gs8Db2noTAG8FPka3YOO36dJ2QdcxfmBL8/UHdOtX0FJ2nQt8ud3e079AnCSNklVrr3v6JkkabLMZcb0NOKOqvpJkH7rF1TYAp+LCbtIwc4SXJEmLpKq+xOQzkQCOmaLOecB5k5TfDrxskvIfA2+YYl8XAxfPNl5JkiRpqc044rqqHqyqr7T7W4C76XLUnki3oBvt52vb/acXdquq++hGghzVRnTuW1U3V1UBH59Qp7evq4BjJi7s1jqrewu7SdoJSS6n61T+J0k2JTkNR3hJkiRJkiRpQO1UjuuWwuPldKMtB2Zht7ks6rZcF8walcXARuE45+MYq+pNUzzlCC9JkiRJkiQNnFl3XCfZm24V9HdV1Q9beupJN52kbEEXdpvLom6LvZDbYhmVxcBG4ThH4RglSZIkSZKkfjOmCgFIsgddp/WlVfXpVuzCbpIkLZIkFyd5OMnX+8pcKFmSJEmStCzN2HHdvtBeBNxdVe/ve6q3GBvsuLDbye0L8It4ZmG3B4EtSY5u+zxlQp3evp5e2A24ETg2yf7ty/ixrUySpFFzCTuu87CWbqHkw4Cb2mMmLJR8PPCRJLu1Or2Fkg9rt94+n14oGfgA3ULJ9C2U/ArgKODs/g5ySZIkSZIWwmxShbwSeAtwZ5KvtrJ30y3kdmVb5O27tPy2VXVXkt7CbtvYcWG3S4C96BZ161/Y7RNtYbfNdF+2qarNSXoLu4ELu0mSRlRVfbF/FHRzIjDW7q8HxoEz6VsoGbivnV+PSrKRtlAyQJLeQsk3tDrntH1dBXx44kLJrU5voeTL5/sYJUmSJGkmq9Zet9QhaJHM2HFdVV9i8lzT4MJukiQtpYFZKFmSJEmSpPk068UZJUnS0Fj0hZIBkpxOl4aEFStWMD4+Pm2QK/aCMw7fBjDjtsvF1q1bR+ZY+43qcUuSJA0bRzNrkNhxLUnS8Hooyco22nq+FkreNMlCyWMT6oxPFkxVXQhcCLB69eoaGxubbLOnXXDp1Zx/Z/dRZOObp992uRgfH2em92U5GtXjliQNtokddJcc/5wlikSSNBk7riVJGl69xY3XseNCyZcleT/wQp5ZKPmpJFuSHA3cSrdQ8gUT9nUzfQslJ7kReG/fgozHAmct/KFJkiRJ0uKaeEFr47oTligSgR3XkiQNhSSX0418fl6STcDZuFCyJEmSJGmZsuNakqQhUFVvmuIpF0qWJEmLyhy4kqTF8KylDkCSJEmSJEmSpH52XEuSJEmSJEmSBoqpQiRJ0pLrn3LsAiiSJEmSdoWpjJYXR1xLkiRJkiRJkgaKHdeSJEmSJGnk3fm9x1m19jpHbErSgDBViCRJkiRJkiRNYErDpeWIa0mSJEmSJEnSQLHjWpIkSZIkSZI0UOy4liRJkiRJkiQNFDuuJUmSJEmSNNKSXJzk4SRf7ys7IMmGJPe0n/v3PXdWknuTfCvJcX3lRya5sz33oSRp5XsmuaKV35pk1aIeoDSE7LiWJEmSJGkO7PCSloVLgOMnlK0Fbqqqw4Cb2mOSvAQ4GXhpq/ORJLu1Oh8FTgcOa7fePk8DHq2qQ4EPAO9bsCORlgk7riVJkiRJmptLsMNrWVm19rrtblr+quqLwOYJxScC69v99cBr+8o/WVVPVNV9wL3AUUlWAvtW1c1VVcDHJ9Tp7esq4JjexSlJk9t9qQOQJEmSJGmYVdUXJxkFfSIw1u6vB8aBM+nr8ALuS9Lr8NpI6/ACSNLr8Lqh1Tmn7esq4MNJ0jrGJC2cFVX1IEBVPZjkBa38IOCWvu02tbIn2/2J5b0697d9bUvyOHAg8IP+F0xyOt0FLFasWMH4+Ph8Hs+kzjh827zta8Ve87u/+XLBpVdv9/iMw7ufK/Ziu/d4utgX43cxk61btw5EHDtjLjHbcS1JkiQtgiQXA68BHq6ql7WyA4ArgFXARuCNVfVoe+4sulGWTwHvqKobW/mRdKM79wKuB95ZVZVkT7qRXUcCjwAnVdXGVmcN8EctlD+uqt6IL0kLZ9E7vGDnOr12tTNhvjulBqWja7o4FrOjaBA6pgYhhkGKYxKTjZSuacqnq7N9QdWFwIUAq1evrrGxsV0McfZOncdZBWccvo3z7xye7sYzDt/GG/ve4+nei41vHpvyucUyPj7OYvxNzKe5xDw8f0mSFkQb2bGF7kvxtqpavVhfoiVJGjGXAB+mOy/29FIJrEuytj0+c0IqgRcCn0vy4qp6imdSCdxCd849nm5E5tOpBJKcTJdK4KR2Xj8bWE33BfmOJNf0zu2SFt2CdXjBznV67Wpnwnx2csHgdHRNF8didlgNQsfUIMQwIHE8lGRlu/i0Eni4lW8CDunb7mDggVZ+8CTl/XU2JdkdeC47piZZMP0pbzauO2GxXlaaE3NcSwL4lao6oqpWt8dDm4/PXHSSpEG1hLkzjwM2VNXm1lm9gR1z8Uqafw+1Nss8dnixFB1e0gi7BljT7q8Bru4rP7ktnPoiuu/At7VZFluSHN3OwadMqNPb1+uBz5vuZ7jZ97DwZryk6ZRGaSSZj0+SpMWxGKkEni6fpM52dibFwMQp9QM6lftpAzzdfFLGu3AWMdZeJ9U6duzwuizJ++lmVPQ6vJ5KsiXJ0cCtdB1eF0zY183Y4SUtiCSX030Pfl6STXSzldYBVyY5Dfgu8AaAqroryZXAN4BtwNvarCiAt/JM/9cN7QZwEfCJ9j16M92gMEnTmM1cnEtwSqO0nBXwl0kK+A9tauHAL0Ax2zx4C/2lZFC+pA1KHDA4sRiHpCE3n6kEFiTFwAWXXr3dlPpByPs4nQGYbr5TjHfhLESsdngtf6ZZWP6q6k1TPHXMFNufB5w3SfntwMsmKf8x7f+ApNmZseN6qVZHpm9KY6vTm9J4+c4fpqRpvLKqHmid0xuSfHOabQdmAYqJX5anstBfogflS9qgxAGDE4txSBoSi5E7cxPPfHbv1Rmf38OQRpsdXpIkzb9dzXG93WhMoH805mTTEA9ilqMxgZ2e0ihp11XVA+3nw8BngKMwH580VJJsTHJnkq8mub2VHZBkQ5J72s/9+7Y/K8m9Sb6V5Li+8iPbfu5N8qF2IZmWu++KVn7rJBe055W56jViFiN35o3AsUn2b/8Ljm1lkiRJ0sCa72V7l2RK41xSDCzX6dujMjV9FI5zIY8xyXOAZ1XVlnb/WOA9mI9PGka/UlX9KXgWPK3XYh2YtFwsVSqBqtqc5Fzgy2279/RmNUqSpNHj4JDt+X4Mrl3tuB6oKY1zSTEw6Ln4dtWoTE0fheNc4GNcAXymDarcHbisqj6b5MuYj08adi6yKg2YpUwlUFUXAxfPOlhJkiRpie1qx/WCj8ZMciPw3r6pzccCZ+1ivJImUVXfAX5+kvJHMB+fNExcZHVIjMJMocmM6nFLkpYnF2qUpMUxY8e1UxolSRp4LrI6JEZhptBkRvW4JUmSNHomph7xAteum/HbolMaJUkabP2LrCbZbpHVBU7rJUmSJEnSgnjWUgcgSZJ2XZLnJNmnd58utdbXeSYVF+yY1uvkJHsmeRHPpPV6ENiS5Oh0ie9PmVCnty8XWZUkSZIkLbhdzXEtSZIGg4usSpIkSZKWHTuuJUkaYi6yKkmStHTMZStJC8dUIZIkSZIkSZKkgeKIa0nLWv8ICEc/SJIkSTtv4qhiSZIWgx3XkiRJkiRJkjQNL+ItPjuuJUnSQHPmhCRJGhZ+bpE0HfPi7xxzXEuSJEmSJEmSBood15IkSZIkSZKkgWLHtSRJkiRJkiRpoJjjWtLIMJeUJEmSpMVivmtpdLhw48JwxLUkSZIkSZIkaaA44lqSJEmSJEmSlpCzNHZkx7UkSRoafpiTJEnDyLSFkrTz7LiWNLLsAJMkSZIkSQvJ/Ne7zo5rSZIkSXPmBWFJmj3/Z0rSzOy4liT84CgNI6fcSpIkSdLyZce1JEmSJEnSEpl4Mf6S45+zRJFouTFFxeCb7e+ot90Zh29jbAHjGTR2XEvSBNOdOBzRKQ0uZ05IkqTl4M7vPc6p7XONn2kkjTI7riVJkiRJ0nYcqSlJWmp2XEvSTpj4Af6Mw7c5GkIaQOa/lpaWMyAkaX74/1Q7wwtOy4e/y85QdFwnOR74U2A34GNVtW6JQ5K0k0ahHfuhUsvZsLdh26c0/O1Yku141HlhfnmwHWs+Lff/CwPfcZ1kN+DPgF8DNgFfTnJNVX1jaSOTNFuj2I7Nk63lZLm1YTuxNYqWsh0v9y9U0mJZbudjzZ2faYaP7VjzYZRGYw98xzVwFHBvVX0HIMkngROBeWnUfpCWFsWCtuNhM9uTjP+PNECWbRu2PWqEDEw7tqNF2mUD0441eBw4MzRsx1o0y6HPcxg6rg8C7u97vAl4xUK92FT/7Cf+cv3ALe2URW3Hy8XOXEU117YW2Mi34fkY1WDb1BIbyHa8q23L9qQRtaDteJRG8I2aXfnd9n+/AP/vzqOBPB9r+Vhu/8uHoeM6k5TVdhskpwOnt4dbk3xrhn0+D/jBTgXxvl17bgnt9DEOqVE4zsmO8aeXIpA5GIh2vBDeMYBxDMD/pIF4Txj8OIapHc/YhmF42/FiaW1zpI65z3I97lFvx0v2e93Fc92w/R0a78Lpj9V2PAC/t0H8TD3qcUyMYQm/Y8zmvVhW7XgXPlMPlEH4+90ZoxTvALfjKdvwMHRcbwIO6Xt8MPBA/wZVdSFw4Wx3mOT2qlo9P+ENplE4RhiN41wmx7hs27Fx7GhQYjGOeTVjG4bhbceLaRSPGUb3uAfMvLfjYfu9Gu/CGqZ4hynWCZZtOzaOwYtjEGIYpDjm0bx/Nx40w/Y7M96FN5eYnzXfwSyALwOHJXlRkp8ATgauWeKYJO0c27E03GzD0vCzHUvDz3YsDT/bsbQTBn7EdVVtS/J24EZgN+DiqrpricOStBNsx9Jwsw1Lw892LA0/27E0/GzH0s4Z+I5rgKq6Hrh+Hnc5tFMudsIoHCOMxnEui2Ncxu3YOHY0KLEYxzxagDYMy+S92UmjeMwwusc9UJbxuXi2jHdhDVO8wxTrdpZxOzaO7Q1CHIMQAwxOHPNmgT5XD5Jh+50Z78Lb5ZhTtcNaDpIkSZIkSZIkLZlhyHEtSZIkSZIkSRohy7rjOsnxSb6V5N4kayd5Pkk+1J7/WpJfWIo452IWx/jmdmxfS/Lfkvz8UsQ5FzMdY992v5jkqSSvX8z45sNsjjHJWJKvJrkryV8tdoyDYrZ/D4sQx8VJHk7y9aWKocVxSJIvJLm7/W28c4nieHaS25L8TYvj3yxFHH3x7Jbkr5Ncu8RxbExyZ2u7ty9lLEtlFM7Fk5nFcY8lebz9bXw1yb9eijjn20z/G5fr73u5G7Z2PEztb9jazCziHaT3dsbPSIP2/i62pfpcPdnfUZIDkmxIck/7uf8CxzDp38cSxDHpZ+jFjqO95nafn5cihva6O3x+XqpYNL1BaUc7a1D+1mcryX5JrkryzfZe/9NBjjnJ/9n+Hr6e5PL2f27X462qZXmjS3L/beBngJ8A/gZ4yYRtXg3cAAQ4Grh1qeNegGP8X4D92/1fX47H2Lfd5+nyRL1+qeNegN/jfsA3gH/UHr9gqeMe1PdqEWP5JeAXgK8v8XuyEviFdn8f4L8vxXvS/o/u3e7vAdwKHL2E78sfAJcB1y7x72cj8LyljGGJj3/Zn4vncNxjS/33uUDHPu3/xuX4+17ut2Frx8PW/oatzcwi3kF6b2f8jDRo7+8ivz9L9rl6sr8j4P8F1rb7a4H3LcXfxxLEMeln6MWOo73Odp+flyKG9lobmfD5eali8Tbj72og2tEuxD0Qf+s7Ee964H9v93+Crn9oIGMGDgLuA/Zqj68ETp1LvMt5xPVRwL1V9Z2q+p/AJ4ETJ2xzIvDx6twC7Jdk5WIHOgczHmNV/beqerQ9vAU4eJFjnKvZ/B4B/g/gU8DDixncPJnNMf4L4NNV9V2AqhrG45wPs/17WHBV9UVg81K89oQ4Hqyqr7T7W4C76U4Wix1HVdXW9nCPdluSRRSSHAycAHxsKV5f2xmFc/FkBuZ/1WKbxf/G5fj7Xu6GrR0PVfsbtjYzKJ9/ZmOWn5EG6v1dZEvWVqb4OzqRrnOG9vO1CxzDVH8fix3HVJ+hFzWOKT4/L2oMMxikWNQMSjvaGUPwt76dJPvSXey7CKCq/mdVPcYAxwzsDuyVZHfgJ4EHmEO8y7nj+iDg/r7Hm9jxg8psthlkOxv/aXQjCobJjMeY5CDgdcC/X8S45tNsfo8vBvZPMp7kjiSnLFp0g2XY2+yCSrIKeDndSI2leP3dknyV7gLShqpakjiADwJ/CPzDEr1+vwL+srXb05c6mCUwCufiycz2mP5pmxp8Q5KXLk5oS245/r6Xu2Frx8ut/Q3SeztbA/feTvMZaRjf3/kyaMe+oqoehK4zDHjBYr3whL+PRY9jis/Qix3HB9nx8/NS/U4m+/y8ZH8fmp2lbkc74YMMzt/6bPwM8H3gP7b0Jh9L8hwGNOaq+h7wJ8B3gQeBx6vqL5lDvMu54zqTlE0c/TebbQbZrONP8it0HddnLmhE8282x/hB4Myqemrhw1kQsznG3YEj6a4MHgf8P0levNCBDaBhb7MLJsnedLMO3lVVP1yKGKrqqao6gm5mx1FJXrbYMSR5DfBwVd2x2K89hVdW1S/QpWp6W5JfWuqAFtkonIsnM5tj+grw01X188AFwH9e6KAGxHL8fS93w9aOl1v7G6T3djYG7r2d4TPSsL2/82mUj/1pfob287PmbhDa0WwM4N/6bOxOl1rpo1X1cuBHdKk2BlLLXX0i8CLghcBzkvzWXPa5nDuuNwGH9D0+mG54+s5uM8hmFX+Sn6ObBnFiVT2ySLHNl9kc42rgk0k2Aq8HPpLktYsS3fyY7d/qZ6vqR1X1A+CLwNAttDkPhr3NLogke9B9ULi0qj691PG0qUvjwPFL8PKvBH6j/T/4JPCqJH+xBHEAUFUPtJ8PA5+hm5Y7SkbhXDyZGY+pqn7YmxpcVdcDeyR53uKFuGSW4+97uRu2drzc2t8gvbczGrT3dhafkYbq/Z1ng3bsD/XStLSfC54acYq/j0WPo2fCZ+jFjGOqz89L8l5M8fl5yX4vmt6gtaMZDNTf+ixtAjb1zWa+iq4je1Bj/lXgvqr6flU9CXyabu29XY53OXdcfxk4LMmLkvwEcDJwzYRtrgFOSedouiHsDy52oHMw4zEm+Ud0fyhvqar/vgQxztWMx1hVL6qqVVW1iq4R/8uq+s+LHumum83f6tXAP0uye5KfBF5Blz9q1MzmvRopSUKX7+ruqnr/Esbx/CT7tft70Z2wvrnYcVTVWVV1cPt/cDLw+aqa0xXeXZXkOUn26d0HjgW+vhSxLKFROBdPZjbn559q7ZckR9F9Jhu2i8u7Yjn+vpe7YWvHy639DdJ7O6NBem9n+RlpqN7feTZon6uvAda0+2vovv8smGn+PhY7jqk+Qy9aHNN8fl7U9wKm/fy86LFoZoPSjmZrkP7WZ6uq/ha4P8k/aUXHAN9gcGP+LnB0kp9sfx/H0PVd7XK8u897iAOiqrYleTtwI92KyRdX1V1Jfr89/++B6+lWkr4X+Dvgt5cq3l0xy2P818CBdKOQAbZV1eqlinlnzfIYh9psjrGq7k7yWeBrdLmYPlZVo9YBNuV7tRSxJLkcGAOel2QTcHZVXbQEobwSeAtwZ7rceADvbqOcFtNKYH2S3ei+pF5ZVdcucgyDZgXwmfa/d3fgsqr67NKGtLhG4Vw8mVke9+uBtybZBvw9cHJVDf0U7cn+N9ItNLVsf9/L3bC142Frf8PWZmYR78C8t0zxGQn4RzCY7+9iWsrP1VP8Ha0DrkxyGl3HxxsWOIyp/j4WO45JP0MnuXmR45jMYr8XMMXn5yRfXoJYNLNBaUdzNejx/h/Ape0i43fozlXPYgBjrqpbk1xFlzpsG/DXwIXA3uxivFkG35EkSZIkSZIkScvIck4VIkmSJEmSJEkaQnZcS5IkSZIkSZIGih3XkiRJkiRJkqSBYse1JEmSJEmSJGmg2HEtSZIkSZIkSRoodlxLkiRJkiRJkgaKHdeSJEmSJEmSpIFix7UkSZIkSZIkaaDYcS1JkiRJkiRJGih2XEuSJEmSJEmSBood15IkSZIkSZKkgWLHtSRJkiRJkiRpoNhxLUmSJEmSJEkaKHZcS5IkSZIkSZIGih3XkiRJkiRJkqSBYse1JEmSJEmSJGmg2HEtSZIkSZIkSRoodlxLkiRJkiRJkgaKHdeSJEmSJEmSpIFix7UkSZIkSZIkaaDYcS1JkiRJkiRJGih2XEuSJEmSJEmSBood15IkSZIkSZKkgWLH9TKQ5N8meddSx7EUkvxGkk8udRzSQhr2Np5kzyTfTPKCpY5Fmq1hb3eLIcmbk/zlLtRbkeTuJHsuRFyS7XfxJHlHknVLHYeWH9vx4vG8rKVgG995ST6d5PiljmOx2XE95JI8HzgF+A/t8ViS8Rnq1Cz3vSrJxrnGON+SXJLkVICqugZ4WZKfW9qopIUxrG28P86qegK4GDhzIV5Lmm8ztbskleTQJQqvF8N4krFZbrsxyar5jqGqLq2qY2cZw6lJLmn1HgK+AJw+3zFJtt+FNyH+C4Hf8uK05pPteOF5XtZSso3vVBz93+3XAectxOsMMjuuh9+pwPVV9fczbZhk94UPZ0lcjidZLV+nMmRtfIo4LgPWOJJDQ+JUZtnutMsuBX5vqYPQsnQqtt9FU1U/Bm6g64CQ5sup2I4Xm+dlLaZTsY1Pa7Lv1FV1G7BvktVLENKSseN6+P068FdTPdmuVL0tyT3APXN5oXYV6f9K8rUkP0pyUZtWdEOSLUk+l2T/vu2PTvLfkjyW5G/6r1Yl+e02HWlLku8k+b2+58aSbEpyRpKHkzyY5LenCW0cOGEuxyYNsEVp40nWJrlqQtmfJvlQuz+bNntmkr8F/uPE/VfVJuBR4OhdjVFaRFO2uyRfbHf/JsnWJCe18t9Ncm+SzUmuSfLCVv6Hbbve7cneCKc2kuPcJP+1ta2/TPK8vtea8jy6q5KckOSvk/wwyf1Jzul7blX7n/Lb7blHk/x+kl9s5/7Hkny4b/tTk3yp73G17e9pdf8sSaYI5VbgZ5L89FyPSZrA9ju79vuPk3w+ySNJfpDk0iT79T23OckvtMcvbNtMdQzj+Flc82s5t+NLknwk3Xfore21fyrJB1u7/WaSl/dt/8Ikn0ry/ST3JXlH33NHJbm5xfdgkg8n+Ym+5z0va1Atyzbe9ve3SXbrK3tdkq+1+7NpszN9tx9n1M65VeVtiG/A94FfnOb5AjYABwB7zfG1NgK3ACuAg4CHga8ALwf2BD4PnN22PQh4BHg13QWSX2uPn9+ePwH4x0CAXwb+DviF9twYsA14D7BH28ffAftPEdcB7Tj3Xerfhzdv831brDYO/HRrZ/u2x7sBDwJHt8ezabPva/8LJo0DuAZ4x1K/p968zXSbZbs7tO/xq4AfAL/Q2sAFwBcnqXcI8ADw6vZ4HPg28GJgr/Z4XXtu2vPoHI5tDDi87fPngIeA17bnVrVj+/fAs4FjgR8D/xl4Qd+5/5fb9qcCX5rwvlwL7Af8o/Y+Hj9NLF8DfmOpf9/eltfN9jvr9ntoi2tP4PnAF4EP9r3W7wJ3Az8J3Aj8yTRx/QKweal/996Wz22Zt+NLWqxHtrb6eeA+ulkLuwF/DHyhbfss4A7gXwM/AfwM8B3guPb8kXSDQnZv/wPuBt414X3yvOxt4G7LvI1/G/i1vsf/CVjb7s+mzU773R74A+DTS/07XMybI66H337Alhm2+bdVtbnmZxrGBVX1UFV9D/gvwK1V9dfV5bD9DF0nNsBv0U39uL6q/qGqNgC30/1ToKquq6pvV+evgL8E/lnf6zwJvKeqnqyq64GtwD+ZIqbe8e83D8cnDZr9WIQ2XlX/g+5C1Gtb0auAv6uqW9rzM7XZf6C7cPXENHFswXaq4bAfM7e7fm8GLq6qr7Tz4VnAP01frrske9F1IP1pO6/1/Meq+u+t3VwJHNHKpz2P7qqqGq+qO9s+v0aXbuuXJ2x2blX9uKr+EvgRcHlVPdx37n85U1tXVY9V1Xfp8mUeMc22/k/QQtgP2++M7beq7q2qDe28/X3g/f37qqo/pxvtdSuwEvi/pwltC/DcuRybNMF+LNN23Hymqu6oLtXOZ4AfV9XHq+op4AqeOc/+Il0n2nuq6n9W1XeAPwdOBmj7uKWqtlXVRrp8wRP/J3he1iDaj+Xbxi8H3tRi2qft73KYdZud6bv9yLVTO66H36PAPjNsc/88vt5Dfff/fpLHe7f7Pw28oU2BeCzJY8D/SvfBlyS/nuSWNs3jMbrG/Ly+fT1SVdv6Hv9d374n6h3/Yzt/ONLAW8w2fhntJAv8i/YYmFWb/X778D2dfbCdajjMpt31eyHwP3oPqmor3YiNg/q2uQj4VlW9b0Ldv+2733+um/Y8uquSvCLJF9qU48eB32f7tgyzP9dPZqrjmYz/E7QQbL/PmLL9JnlBkk8m+V6SHwJ/Mcm+/hx4Gd3AlSemCW0f4PGdPyJpSsu2HTc78536hRNieDfdDGiSvDjJtS01wQ+B97JjO/a8rEG0nNv4ZcBvplvb6TeBr7RBYrNtszN9tx+5dmrH9fD7Gt20h+nUDM8vhPuBT1TVfn2351TVutaAPwX8CbCiqvYDrqdLQbArfhbYWFU/nJfIpcGymG38PwFjSQ4GXkfruJ5lm51NDD8L/M08xSotpNm0u34P0H34BSDJc4ADge+1x2vpZg2dthP7nPI8uhP7mMxldGl7Dqmq59KlFdjV8+8uS7fgzKH4P0Hzz/Y7O/+W7tz9c1W1L93Is6f3lWRv4IN0HQHnJDlgmn15ftd8W87teGfcD9w3IYZ9qqo3IvSjwDeBw1o7fje7+D/B87IW2bJt41X1DbpO9l9nwmAwZtdmZ/pePXLnXDuuh9/17Di1YNaSnJNkfP7CedpfAP88yXFJdkvy7HQLuB1Ml59rT7q8RtuS/DpdHr5d9ct0q5lLy9GitfE2VXicbnHF+6rq7vbUnNtskoPocnXdsjP1pCUyU7t7iC7PZM9lwG8nOaJd6HkvXSqtja29vIMuD+3OpPOZ7jy6nVY+2wtY+9Dlov1xkqPoPlAvhaPoLjr/jxm3lHaO7Xf2+9oKPNbO0f/XhOf/FLijqv534Dq6TvKp+Flc8205t+OdcRvww3QLoO/V4nhZkl9sz+8D/BDYmuT/B7x1Dq/leVmLabm38ctaTL9ENzisZz7a7Midc+24Hn4fB16dLp/PrjgE+K/zGA8AVXU/cCLdFaTv013N+r+AZ1XVFrpGfCXdFJF/QTd6ZFe9iS43kLQcLXYbvwz4VfquDM9Tm/0XwPoZphpLg2KmdncOsL5NK3xjVd0E/D90MxMepFvI9OS27Ul0C5/dnWdWO5+uAwiY/jw6yeaHADfP8tj+JfCeJFvoFnu6cpb15tubmb4jTNpVtt/Z+Td0i1w9Ttcx/eneE0lOBI6nS0UC3UJQv5DkzRN3kuTZdOnD1s8hFmmi5dyOZ63lvP7ndDl576NbnO5jPJNT/l/RfcbeQpfa54o5vJznZS2m5d7GL6dbUPnzVfWDvvI5tdl20epHVXXbztQbdqlaiiwSmk9J3gs8XFUf3IW6XwWOqapH5juuxZDknwNvqao3LnUs0kIZ9jberor/DfBLVfXwUsUh7Yy5tLvFluRjwH+qqhuXOpbZSPIC4K+Al88iN76002y/iyfJ/0GXuuQPlzoWLS+248XjeVlLwTa+S3F8CrhowuKTy54d15IkSZIkSZKkgWKqEEmSJEmSJEnSQLHjWpIkSZIkSZI0UOy4liRJkiRJkiQNlN2XOoD59rznPa+e//zn85znPGepQ1kUP/rRjzzWZWimY73jjjt+UFXPX8SQFtXznve8WrVq1bTbDMLfwyDEYByDGcdsYrAdL65B+LuYyTDECMbZz3Y8WIblb3M+jMqx2o7nbiHb8SD+HRrT7A1iXLsa03Jux8Py3XhXDXPsMNzxD1Ls07bhqlpWtyOPPLK+8IUv1KjwWJenmY4VuL0GoL0t1O3II4+c83u0GAYhhirjmGgQ4phNDLbjxTUIfxczGYYYq4yz33y2Y2AjcCfw1d5+gQOADcA97ef+fdufBdwLfAs4rq/8yLafe4EP8cxi7HsCV7TyW4FVM8U0aO14JsPytzkfRuVYh60dD+JtIdvxIP4dGtPsDWJcuxrTcm7Hw/LdeFcNc+xVwx3/IMU+XRs2VYgkSZI0GH6lqo6oqtXt8Vrgpqo6DLipPSbJS4CTgZcCxwMfSbJbq/NR4HTgsHY7vpWfBjxaVYcCHwDetwjHI0mSJO2yOXdcJ9ktyV8nubY9PiDJhiT3tJ/79217VpJ7k3wryXF95UcmubM996EkaeV7Jrmild+aZNVc45UkSZKGxInA+nZ/PfDavvJPVtUTVXUf3Sjqo5KsBPatqpvb6JWPT6jT29dVwDG9z9ySJEnSIJqPEdfvBO7ue+zIEEmSJGnnFPCXSe5IcnorW1FVDwK0ny9o5QcB9/fV3dTKDmr3J5ZvV6eqtgGPAwcuwHFIkiRJ82JOizMmORg4ATgP+INWfCIw1u6vB8aBM+kbGQLcl6Q3MmQjbWRI22dvZMgNrc45bV9XAR9OkjaCRJIkSVouXllVDyR5AbAhyTen2XaykdI1Tfl0dbbfcddpfjrAihUrGB8fnzboQbJ169ahincuRuVYR+U4JUnS5ObUcQ18EPhDYJ++su1GhrQP39CN8rilb7veCJAnmeXIkCS9kSE/mGPckiRJ0sCoqgfaz4eTfAY4Cngoycr2mXol8HDbfBNwSF/1g4EHWvnBk5T319mUZHfgucDmSeK4ELgQYPXq1TU2NjY/B7gIxsfHGaZ452JUjnVUjlOSJE1ulzuuk7wGeLiq7kgyNpsqk5QtyMiQUboy77EuT6N0rJIkjbokzwGeVVVb2v1jgfcA1wBrgHXt59WtyjXAZUneD7yQLtXebVX1VJItSY4GbgVOAS7oq7MGuBl4PfB5ZzFKkiRpkM1lxPUrgd9I8mrg2cC+Sf6CARgZsvfee4/MlflRGoXgsUqSpGVqBfCZtlbi7sBlVfXZJF8GrkxyGvBd4A0AVXVXkiuBbwDbgLdV1VNtX28FLgH2oku9d0Mrvwj4REvXt5lu7RlJkiRpYO1yx3VVnQWcBdBGXP+rqvqtJP8OR4bstFVrr3v6/sZ1JyxhJNJwuPN7j3Nqaze2GUnDoP9cD/7v0jOq6jvAz09S/ghwzBR1zqNbZ2Zi+e3AyyYp/zGt43u583O1NBo8r0odvxtrOZtrjuvJrMORIbMy8UQrSZIkSZIkSZqnjuuqGgfG231HhkiSJOFFakmSJEnaVc9a6gAkSZIkSZLmYtXa67jze4970ViSlhE7riVJkiRJkiRJA8WOa0mSJEmSJEnSQLHjWpIkSZIkSZI0UOZlcUbNjrm2JEmSJEmSlkaSZwNfBPak6xO7qqrOTnIO8LvA99um766q61uds4DTgKeAd1TVja38SOASYC/geuCdVVVJ9gQ+DhwJPAKcVFUbW501wB+11/jjqlq/oAcsDTk7riVJkiQNrd7gkDMO38apDhSRJE3vCeBVVbU1yR7Al5Lc0J77QFX9Sf/GSV4CnAy8FHgh8LkkL66qp4CPAqcDt9B1XB8P3EDXyf1oVR2a5GTgfcBJSQ4AzgZWAwXckeSaqnp0gY9ZGlqmCpEkaRlIsluSv05ybXt8QJINSe5pP/fv2/asJPcm+VaS4/rKj0xyZ3vuQ0nSyvdMckUrvzXJqkU/QEmSJGmOqrO1Pdyj3WqaKicCn6yqJ6rqPuBe4KgkK4F9q+rmqiq6Edav7avTG0l9FXBM+1x9HLChqja3zuoNdJ3dkqZgx7U0ApJcnOThJF/vK7siyVfbbWOSr7byVUn+vu+5f99XZ6c7tZKsaR1n97RpUZIWxjuBu/serwVuqqrDgJva44mjRo4HPpJkt1anN2rksHbrfZB+etQI8AG6USOSJKmPF5Gl4dDa6leBh+k6km9tT709ydfa9+deez0IuL+v+qZWdlC7P7F8uzpVtQ14HDhwmn1JmoKpQqTRcAnwYbqrwABU1Um9+0nOpzuZ9ny7qo6YZD9OhZIGUJKDgROA84A/aMUnAmPt/npgHDiTvlEjwH1JeqNGNtJGjbR99kaN3NDqnNP2dRXw4SRpo0skSVKndxF53/a4dxF5XZK17fGZ85l6YPEObTC4bpTmQ2trRyTZD/hMkpfRtb1z6b63ngucD/wOkMl2MU05u1jnaUlOp/s/wIoVKxgfH5/maGDFXl26LGDGbQfN1q1bhy7mfsMc/7DEbse1NAKq6otTjcpoozjeCLxqun30T4Vqj2fs1KJvKlSr05sKdfncjkjSBB8E/hDYp69sRVU9CFBVDyZ5QSs/iO7LcE9vpMeTzHLUSJLeqJEf9Aexsx+yF9NSfTDrfYmYzMR4huXDo3FK0o68iCwNn6p6LMk4cHx/buskfw5c2x5uAg7pq3Yw8EArP3iS8v46m5LsDjwX2NzKxybUGZ8krguBCwFWr15dY2NjEzfZzgWXXs35d3bdexvfPP22g2Z8fJyZjm+QDXP8wxK7HdeS/hnwUFXd01f2oiR/DfwQ+KOq+i/sxFSovk4tp0JJCyzJa4CHq+qOJGOzqTJJ2a6OGtm+YCc/ZC+mpfpgNt1CcRO/WAzLh0fjlKRJfZABuIgMi3cheSkuEE53QRieGXk6SBcuB/VC6iDGtRgxJXk+8GTrtN4L+FXgfUlW9tor8Dqgl2bzGuCyJO+nmyFxGHBbVT2VZEuSo4FbgVOAC/rqrAFuBl4PfL6qKsmNwHv70pAcC5y1oAcsDTk7riW9ie1HQD8I/KOqeiTJkcB/TvJSFnAqFAzndKhB+bBnHIMXxyLH8ErgN5K8Gng2sG+SvwAe6n0AbzMmHm7bz+eoEUmSRt4gXUSGxbuQvBQXCKe7IAzd94Pz79x9oEadDuqF1EGMa5FiWgmsb2u8PAu4sqquTfKJJEfQtauNwO8BVNVdSa4EvgFsA97WUo0AvJUuLededDMjbmjlFwGfaLMpNtOlBqKqNic5F/hy2+49vdnJkiZnx7U0wloH1G8CR/bK2pTFJ9r9O5J8G3gxCzgVqr3W0E2HGpQPe8YxeHEsZgxVdRZtpEb7svyvquq3kvw7upEe69rPq1uVeRs1sgiHJ0nSMPAisjQkquprwMsnKX/LNHXOo0sDNLH8duBlk5T/GHjDFPu6GLh4J0KWRtqzdrVikmcnuS3J3yS5K8m/aeXnJPlekq+226v76rhysjRYfhX4ZlU9PSUxyfPb1WeS/Axdp9Z32rSpLUmObm30FLbvCFvT7vd3at0IHJtk/zYd6thWJmnhrQN+Lck9wK+1x1TVXUBv1Mhn2XHUyMeAe4Fvs/2okQPbqJE/oFtcSpIk0V1ErqqDq2oV3cjKz1fVb7H9Z+SJF5FPbt93X8QzF5F35fO2JEnL1lxGXD8BvKqqtibZA/hSkt4X3A/0J7YHcOVkaekkuZxu5PPzkmwCzq6qi+ja5MSFEn8JeE+SbcBTwO/3TV9yKpQ0wKpqnDaroaoeAY6ZYrt5GzUiSZKmtA64MslpwHdp59L5TD0gSdJytssd1+3q7tb2cI92m+6KrysnS0ukqt40Rfmpk5R9CvjUFNs7FUqSJEmagheRJUmaP7ucKgQgyW5JvkqXq2tDVd3annp7kq8lubhvtdSnV0FueiskH8QsV04Geisnj5RVa697+iZJkiRJkiRJy92cFmds05mOSLIf8JkkL6NL+3Eu3ejrc4Hzgd9hAVdOTnI6XaoRVqxYwdatWxkfH9+pY1kMZxy+bVbbTYy9v97E5wb1WBeCxypJkiRJkiSNhjl1XPdU1WNJxoHj+3NbJ/lz4Nr2cMFWTq6qC4ELAVavXl177703Y2Njcz+weXbqLEdMb3zz2JT1Jj43Pj4+kMe6EDxWSZIkSZIkaTTscsd1kucDT7ZO672AXwXel2RlWw0Z4HXA19v9a4DLkryfbnHG3srJTyXZkuRo4Fa6lZMv6KuzBriZEVo52ZQgkiQtf/3n+43rTljCSKTly3YmSZI0vOYy4nolsD7JbnS5sq+sqmuTfCLJEXQpPTYCvweunCxJkiRJkiRJmp1d7riuqq8BL5+k/C3T1HHlZEmSJEmStGAmzmJ2xoUkDadnLXUAkiRJkiRJkiT1s+NakiRJkiRJkjRQ5pLjWpIkSRO4yLJ2RVs35nbge1X1miQHAFcAq+jWjXljVT3atj0LOA14CnhHVd3Yyo/kmXVjrgfeWVWVZE/g48CRwCPASVW1cdEOTpIkSdoFdlwvML+8SpIkaRbeCdwN7NserwVuqqp1Sda2x2cmeQndguUvBV4IfC7Ji9ui5x8FTgduoeu4Pp5u0fPTgEer6tAkJwPvA05avEOTJEmSdp6pQiRJkqQllORg4ATgY33FJwLr2/31wGv7yj9ZVU9U1X3AvcBRSVYC+1bVzVVVdCOsXzvJvq4CjkmSBTocSZIkaV7YcS1JkiQtrQ8Cfwj8Q1/Ziqp6EKD9fEErPwi4v2+7Ta3soHZ/Yvl2dapqG/A4cOC8HoEkSUMgybOT3Jbkb5LcleTftPIDkmxIck/7uX9fnbOS3JvkW0mO6ys/Msmd7bkP9S4KJ9kzyRWt/NYkq/rqrGmvcU+SNYt46NJQMlXIAljI9CAT933J8c9ZsNeSJEnSwkryGuDhqrojydhsqkxSVtOUT1dnsnhOp0s3wooVKxgfH59FSEvrjMO3AbBir2fuT2YYjmW2tm7duqyOZyqjcpySFtUTwKuqamuSPYAvJbkB+E0WOEVXW7/ibGA13Xn4jiTX9NawkLQjO64lSZKW2Kq113HG4ds4tV2g3rjuhCWOSIvolcBvJHk18Gxg3yR/ATyUZGVVPdjSgDzctt8EHNJX/2DggVZ+8CTl/XU2JdkdeC6webJgqupC4EKA1atX19jY2NyPcIH12s0Zh2/j/Dun/nqz8c1jixTRwhsfH2cYfjdzNSrHKWnxtHRaW9vDPdqt6NJqjbXy9cA4cCZ9KbqA+5L0UnRtpKXoAkjSS9F1Q6tzTtvXVcCH22js44ANVbW51dlA19l9+YIcrLQMmCpEGgFJLk7ycJKv95Wdk+R7Sb7abq/ue86pUJIkLYKqOquqDq6qVXQjuj5fVb8FXAP0zptrgKvb/WuAk9u590XAYcBtLZ3IliRHt/PzKRPq9Pb1+vYak464liRpuUuyW5Kv0l0U3lBVt7I4Kbqm2pekKTjiWhoNlwAfpluoqd8HqupP+gucCiVJ0kBYB1yZ5DTgu8AbAKrqriRXAt8AtgFva+dogLfSnfP3ojs/39DKLwI+0UaJbaY7z0uSNJLaefOIJPsBn0nysmk2n88UXbNK3bWzabv6U2UNW3qlYU8JNczxD0vsdlxLI6Cqvtg/CnoGToWSJGkJVNU43dRkquoR4JgptjsPOG+S8tuBHb58V9WPaR3fkiSpU1WPJRmn+466GCm6NvFMOpJenfFJ4tqptF0XXHr106myhi0t1rCnhBrm+IcldjuupdH29iSnALcDZ7SR0AfRjaju6U1fepJZToVKstNToYbxqvKgXKE0jsGLYxBikCRJkrS9JM8Hnmyd1nsBv0o3Y7iXVmsdO6bouizJ++lmJPdSdD2VZEuSo4Fb6VJ0XdBXZw1wM30pupLcCLw3yf5tu2OBsxb2iKXhZse1NLo+CpxLNzXpXOB84HdYgqlQMJxXlQflCqVxDF4cgxCDJEmSpB2sBNYn2Y1u3bcrq+raJDezwCm6qmpzknOBL7ft3tObnSxpcrvccZ3k2cAXgT3bfq6qqrNbTtsrgFXARuCNvXy2Sc6iy4X7FPCOqrqxlR/JM439euCd7WrUnnQ5eY8EHgFOqqqNuxqzpGdU1UO9+0n+HLi2PVz0qVCSJEmSJC20qvoa8PJJyhclRVdVXQxcvHNRS6PrWXOo+wTwqqr6eeAI4Pg2RWItcFNVHQbc1B5PXPDteOAj7QoXPLPg22Htdnwrf3rBN+ADdNM3JM2Dlrer53XA19v9a4CTk+yZ5EU8MxXqQWBLkqNb/upT2H761Jp2/+mpUMCNwLFJ9m/ToY5tZZIkSZI0o1Vrr3v6JkkaLbs84rp1Sm1tD/dot6JbpG2sla+nG115JvO44Ft7bUmzlORyunb5vCSbgLOBsSRH0LXbjcDvgVOhJEmSJEmStPTmlOO6jZi+AzgU+LOqujXJijYyk7Ya6wva5vO54NsP5hK3NGqq6k2TFF80zfZOhZIkSZK0LPSP1t647oQljESStDPm1HHdRmEekWQ/4DNJdujQ6jOfC75tv+PkdLpUI6xYsYKtW7cyPj4+TSgL64zDty3aay31sS4mj1WSJEmSJEkaDXPquO6pqseSjNPlpn4oyco22nol8HDbbD4XfJv4+hcCFwKsXr269t57b8bGxubj0HbJqYuYe+uS45+zpMe6mMbHxz1WSdLAMeemJEmSJM2/Xe64TvJ84MnWab0X8Kt0iyf2Fmlb1372L952WZL3Ay/kmQXfnkqypS3seCvdgm8X9NVZA9zM9gu+qbnze48/3VHulCdJkiSNgl25YDSxjp+dJUmSBttcRlyvBNa3PNfPAq6sqmuT3AxcmeQ04Lu0vLfzueCbJEmSJEmSJGn52uWO66r6GvDyScofAY6Zos68LfgmSZIkSZIkSVqenrXUAUiSJEmSJEmS1M+Oa0mSJEmSJEnSQLHjWpKkIZbk2UluS/I3Se5K8m9a+QFJNiS5p/3cv6/OWUnuTfKtJMf1lR+Z5M723IeSpJXvmeSKVn5rklWLfqCSJEmSpJFix7UkScPtCeBVVfXzwBHA8UmOBtYCN1XVYcBN7TFJXkK32PFLgeOBj7SFlgE+CpwOHNZux7fy04BHq+pQ4APA+xbhuCRJGhpeSJYkaf7ZcS1J0hCrztb2cI92K+BEYH0rXw+8tt0/EfhkVT1RVfcB9wJHJVkJ7FtVN1dVAR+fUKe3r6uAY3pforUwVq297umbJGkoeCFZkqR5tvtSByBJkuamfdG9AzgU+LOqujXJiqp6EKCqHkzygrb5QcAtfdU3tbIn2/2J5b0697d9bUvyOHAg8IMJcZxO90WbFStWMD4+Pm/HOFdbt25dsHjOOHzbvOxnxV6T72uQ3kdY2PdyPg1LnJKWh3bRd6oLyWOtfD0wDpxJ34Vk4L4kvQvJG2kXkgGS9C4k39DqnNP2dRXw4SRpry1J0rJjx7UkSUOuqp4CjkiyH/CZJC+bZvPJRkrXNOXT1ZkYx4XAhQCrV6+usbGxacJYXOPj4yxUPKfO06joMw7fxvl37vjRbOObx+Zl//NlId/L+TQscUpaPkbtQvJiXSDcmQvEU10E7rfYFzUH9ULqIMa1GDElOYRuZuFPAf8AXFhVf5rkHOB3ge+3Td9dVde3OmfRzXh4CnhHVd3Yyo8ELgH2Aq4H3llVlWTP9hpHAo8AJ1XVxlZnDfBH7TX+uKp6sxolTcKOa0mSlomqeizJON2U4oeSrGxfklcCD7fNNgGH9FU7GHiglR88SXl/nU1JdgeeC2xesAORJGkIjdqF5MW6QLgzF4inugjcb7EvCA/qhdRBjGuRYtoGnFFVX0myD3BHkg3tuQ9U1Z/0bzwhrc8Lgc8leXFr7720PrfQdVwfTzc74um0PklOpkvrc1KSA4CzgdV0bfeOJNdU1aMLfMzS0DLHtSRJQyzJ89sXZJLsBfwq8E3gGmBN22wNcHW7fw1wclvg6UV0uTNva6PBtiQ5uuWvPmVCnd6+Xg983mnJkiRNrqoeo0sJ8vSFZIB5vJCMF5KlXVNVD1bVV9r9LcDdPDOrYTLzuT7MccCGqtrcOqs38EwOe0mTcMT1PHDhJA26JBcDrwEerqqXtbJ/B/xz4H8C3wZ+u43WXEV38v5Wq35LVf1+qzOQU6H62+DGdSfM9+6lQbcSWN+mJz8LuLKqrk1yM3BlktOA7wJvAKiqu5JcCXyDbsTJ29qIEYC38kwbv6HdAC4CPtHyb26mG3UiSZKaJM8Hnmyfp3sXkt/HMxd/17HjheTLkryfbhRn70LyU0m2tIUdb6W7kHxBX501wM14IVmas/bd9+V0be2VwNuTnALcTjcq+1HmN63P0+WT1JE0CTuupdFwCfBhus7lng3AWe1E+j7gLLqFYgC+XVVHTLIfp0JJA6aqvkb3gXti+SPAMVPUOQ84b5Ly24EdpjVX1Y9pHd+StFx44VvzzAvJ0hBJsjfwKeBdVfXDJB8FzqX73noucD7wO8xvWp9ZpfvZ2Tz1/bndBy1v+UwGMdf6zhjm+IcldjuupRFQVV9sV5P7y/6y7+EtdKM2ptQ/Fao9nnGFc/qmQrU6valQl8/tiCRJkqTB4YVkaXgk2YOu0/rSqvo0QFU91Pf8nwPXtofzuT7MJmBsQp3xifHtbJ76Cy69+unc7oO2qPdMBjHX+s4Y5viHJXY7riVBdyX5ir7HL0ry18APgT+qqv9CN4VpwaZCzeWqcr/FvGI4KFcojWPw4hiEGCRJkrQjZ1uMtjbA6iLg7qp6f1/5yrbmC8DrgK+3+/OW1ifJjcB7k+zftjuWbuazpCnscsd1kkPo0g78FPAPwIVV9adJzgF+F/h+2/TdVXV9q3MWXUqBp4B3VNWNrXyn8+ZKmh9J/m+66YmXtqIHgX9UVY+0tvmfk7yUBZwKBXO7qtxvMa8wD8oVSuMYvDgGIQZJkiRJO3gl8BbgziRfbWXvBt6U5Ai676sbgd+D+U3rU1Wbk5wLfLlt957e7GRJk5vLiOttdMnqv5JkH7rctRvacx+oqj/p3zjJS+ga60vprlJ9LsmLW4Pfqby5c4hZUp+2cOJrgGN6C7tU1RPAE+3+HUm+DbyYBZwKJUmSJEnSQquqLzH5AKvrp6kzb2l9qupi4OLZxiuNumftasWqerCqvtLubwHuZvrVUE8EPllVT1TVfcC9wFH9eXNbx1kvb26vzvp2/yrgmDatQ5NYtfa6p2/STJIcT7cY429U1d/1lT+/LSpDkp+hmwr1nTZtakuSo1s7PIXtV0Vf0+73r3B+I3Bskv3bdKhjW5kkSZIkSZI0pXnJcd0WfXs5XV6fVwJvT3IKcDvdqOxH6Tq1b+mr1st1+yQ7nzf3B/MRtzQqklxON/L5eUk2AWfT5dLaE9jQrgfdUlW/D/wS8J4k2+jS+vx+3/Qlp0JJEniRWPMqybOBL9Kdl3cHrqqqs5McQLcGxSq6actvbJ+rTcEnSZKkZW/OHddJ9qZbjfVdVfXDJB8FzqXLC3QucD7dwm+7kgN3VvlxJy7qttiLYk22QNxiGYQF6hbLKC12Nt/HWlVvmqT4oim2/RRdm57sOadCSZI0/54AXlVVW5PsAXwpyQ3AbwI3VdW6JGuBtcCZpuCTJEnSKJhTx3X7YP0p4NKq+jRAVT3U9/yfA9e2h70cuD29/Li7kjd3OxMXddt7770XdVGsU5dw1NUZh29b8gXqFssoLXY2SscqSdKoa+m1traHe7Rb0aXNG2vl6+nWiTiTvhR8wH1txtNRSTbSUvABJOml4Luh1Tmn7esq4MNJ0lvjQpIkSRo0u9xx3XLcXgTcXVXv7ytf2XLhArwO+Hq7fw1wWZL3040MOQy4raqeSrIlydF0qUZOAS7oq7MGuJnt8+ZqBhOnMG9cd8ISRSJJkqSZtPUl7gAOBf6sqm5NsqL3ubqqHkzygrb5gqXgmziTcVBnu00243CqmYizMajHOZVRmYk4Ksep7ZmOS5LUM5cR168E3gLcmeSrrezdwJuSHEE3SmQj8HsAVXVXkiuBbwDbgLe16Yywk3lzJUmSpOWkfS4+Isl+wGeS7JCaq8+CpeCbOJNxUGeATTbjcaqZiLMxbLMVR2V23qgcpyRJmtwud1xX1ZeY/APw9dPUOQ84b5Lync6bK0mSJC03VfVYknG63NQP9WYzJlkJPNw2W7AUfJIkSdKgeNZSByBJkiSNsiTPbyOtSbIX8KvAN3kmbR7t59Xt/jXAyUn2TPIinknB9yCwJcnRLa3fKRPq9PZlCr4JVq297umbJEmSBsOcFmeUJEnSwurvSHPNimVrJbC+5bl+FnBlVV2b5GbgyiSnAd+lzUQ0BZ8kSZJGgR3XkiRJ0hKqqq8BL5+k/BHgmCnqmIJPkiRJy5qpQiRJkiRJkiRJA8UR15IkSZIkaeRMzGtvSi5JGiyOuJYkSZIkSZIkDRRHXEuSJM1g4ogsSZIkSdLCcsS1JEmSJDWr1l633U2StHwkOSTJF5LcneSuJO9s5Qck2ZDknvZz/746ZyW5N8m3khzXV35kkjvbcx9Kkla+Z5IrWvmtSVb11VnTXuOeJGsW8dCloWTHtSRJkiRJkkbBNuCMqvpZ4GjgbUleAqwFbqqqw4Cb2mPacycDLwWOBz6SZLe2r48CpwOHtdvxrfw04NGqOhT4APC+tq8DgLOBVwBHAWf3d5BL2pEd19IISHJxkoeTfL2vzCvKkiRJkjQJZ18sT1X1YFV9pd3fAtwNHAScCKxvm60HXtvunwh8sqqeqKr7gHuBo5KsBPatqpurqoCPT6jT29dVwDHtu/NxwIaq2lxVjwIbeKazW9Ik7LiWRsMl7HhC9IqyJEmSJDV2VI+WNuDq5cCtwIqqehC6zm3gBW2zg4D7+6ptamUHtfsTy7erU1XbgMeBA6fZl6QpuDijNAKq6ov9o6CbE4Gxdn89MA6cSd8VZeC+JL0ryhtpV5QBkvSuKN/Q6pzT9nUV8OGJV5Rbnd4V5cvn+xglSZIkSZqNJHsDnwLeVVU/bJOJJ910krKapnxX6/THdjrdgDFWrFjB+Pj4VLEBsGIvOOPwbQAzbjtotm7dOnQx9xvm+IcldjuuR0T/FeON605Ywkg0QLa7opyk/4ryLX3b9a4CP8ksrygn8YqypKHnaCtJkqTlJ8kedJ3Wl1bVp1vxQ0lWtu/GK4GHW/km4JC+6gcDD7Tygycp76+zKcnuwHOBza18bEKd8YnxVdWFwIUAq1evrrGxsYmbbOeCS6/m/Du77r2Nb55+20EzPj7OTMc3yIY5/mGJfZc7rpMcQpfD56eAfwAurKo/bakBrgBWARuBN7bcPeT/z97fx8tW1vf9/+uNGEJEVDSeINCcJGIahajhlJCv36bHmCjRNJg+VLAmQEJC6s9UbWgLmP4q1ZBiG9Go0YYEChi8oUQDP4UYvNmxtoCCJSISK+qJHCGggMhJq/GQz++PdW2Ys9l7n30zN2tmXs/HYx575lprzf5cM/OZNeta67qu5Ey6IQUeAF5ZVR9q5UfRDWWwP3Al8KqqqiT7tf9xFHA3cHxV7dhozJLWZOxnlGFzZ5UHjfOMYV/OUBpH/+LoQwySJEmS9tR6Bp8P3FJV5w4sugI4CTin/b18oPxdSc4FnkQ3ZOYnq+qBJPcnOYZuqJETgbcuea5rgBcBH21tXB8Cfmdg+MznAmeOqKrSTNjMFdeLM7F+OsmjgRvaMAAn042be06SM+jGzT19ybi5TwI+nOQpVfUAD42bey1dw/WxdMMPPDhubpIT6MbNPX4TMQ+NV2FpBvTmjDJs7qzyoHGeYe7LGUrj6F8cfYhBkjQc9lyUpJnyLOCXgJuS3NjKXkPXYH1pklOArwAvBqiqm5NcCnyOrh3sFa0dC+DlPHQR5lXtBl3D+DvbsJv30LWFUVX3JHk98Km23usWh9WUtLwNN1y3IQYWhxm4P8ngTKzb22ojGTe3zdgqaXM8oyxJkiRJmhtV9QmW7xkM8JwVtjkbOHuZ8uuBI5Yp/xat4XuZZRcAF6w1XmneDWWM69VmYh3RuLlfX/L/9xhiYBxdtJcbomASVhouYTXT2n19nrreD7uuSd5Nd0LpCUl2Aq/FM8qSNHWW9vjy6k9JkiRJs2rTDdcTmol1z4IlQwwccMABI++ifXJPhgo57cjdyw6XsJppG6x/0Tx1vR92XavqpSss8oyyJEnqPYfpk2bbNOS4wwZJ0vjts5mNV5uJtS0f1ri5LBk3V5IkSZIkSZI0ozZ8xfUkZ2LdaMySJEmStFEO1yNJkjQ+m7nienEm1p9KcmO7PZ+uwfpnknwB+Jn2mKq6GVgcN/fPePi4uX8E3Ap8kT3HzX18Gzf3N4EzNhGvJEkzJ8lhST6W5JYkNyd5VSs/KMnVSb7Q/j5uYJszk9ya5PNJnjdQflSSm9qyt7ST1CTZL8l7W/l1bW4LSZLUuD+WJGn4NnzF9aRnYpUkSUA3ieppVfXpJI8GbkhyNXAy8JGqOifJGXQnf09P8lS6CVSfRtcD6sNJntJOJr+DbrLja4ErgWPpTiafAtxbVU9OcgLwBuD4sdZSkqR+c38sSdKQbWqMa0mSNFlVdUdVfbrdvx+4BTgEOA64qK12EfDCdv844D1V9e2q+jJdb6ej27wUB1bVNW1YrouXbLP4XJcBz1m8+kuSJLk/liRpFDZ8xbWml2PzSdJsal2Gn0k3Z8SWqroDuoPpJE9sqx1CdwXXop2t7Dvt/tLyxW1ua8+1O8l9wOOBry/5/6fSXSHGli1bWFhYGFbVNm3Xrl1riuemr9734P3TjhxhQMvYsj+cduTudW0zidd4ra/lpE1LnJJmz6T3x5IkzQobriVJmgFJDgD+BHh1VX1zlQuwlltQq5Svts2eBVXnAecBbNu2rbZv376XqMdnYWGBtcRz8pKTu+N02pG7eeNN6/tptuNl20cTzCrW+lpO2rTEKWm29GF/PK4TycM8QbjeE7cr2chJ4I1YT737eiK1j3H1MSZJk2XDtSRJUy7JI+kOki+pqve14juTHNyu7joYuKuV7wQOG9j8UOD2Vn7oMuWD2+xMsi/wGOCekVRmzJb2QpIkaaP6sj8e14nkYZ4gHNaJ442cBN6I9Zw47uuJ1D7G1ceYJE2WY1xLkjTF2tiW5wO3VNW5A4uuAE5q908CLh8oPyHJfkl+ADgc+GTrxnx/kmPac564ZJvF53oR8NE27qYkScL9sSRJo+AV15IkTbdnAb8E3JTkxlb2GuAc4NIkpwBfAV4MUFU3J7kU+BywG3hFVT3Qtns5cCGwP3BVu0F3IP7OJLfSXdl1wojrJEnStHF/LEnSkNlwLUnSFKuqT7D8mJcAz1lhm7OBs5cpvx44Ypnyb9EOtNUvg0OdONmyJE2O+2NJkobPoUIkSZKkCUpyWJKPJbklyc1JXtXKD0pydZIvtL+PG9jmzCS3Jvl8kucNlB+V5Ka27C1tqAHacATvbeXXJdk69orOoK1nfPDBmyRJkobLhmtJkiRpsnYDp1XVjwDHAK9I8lTgDOAjVXU48JH2mLbsBOBpwLHA25M8oj3XO4BT6cbLPbwtBzgFuLeqngy8CXjDOComSZIkbZQN15IkSdIEVdUdVfXpdv9+4BbgEOA44KK22kXAC9v944D3VNW3q+rLwK3A0UkOBg6sqmvahG0XL9lm8bkuA56zeDW2JE3CNPdYmObY512SC5LcleSzA2VnJflqkhvb7fkDy4bWwynJSa0X1ReSLE60KmkVNlxLcyzJDw/snG9M8s0krx7XjluSJO2p7SefCVwHbKmqO6Br3Aae2FY7BLhtYLOdreyQdn9p+R7bVNVu4D7g8SOphCRJ/XUhD/VGGvSmqnpGu10Jw+3hlOQg4LXAjwNHA68dHAJM0vKcnFGaY1X1eeAZAG0H/FXg/cAv0+24f3dw/SU77icBH07ylDYD+uKO+1rgSrod91UM7LiTnEC34z5+9LWTJGm6JDkA+BPg1VX1zVUuiF5uQa1Svto2S2M4lW5/zpYtW1hYWNhL1ONx2pG797rOlv3Xtt6ojPO12rVrV2/em1Gal3pKGp+q+vg6LqZ6sIcT8OUkiz2cdtB6OAEkWezhdFXb5qy2/WXA29pFXc8Drq6qe9o2V9MdM797CNWSZpYN15IWPQf4YlX99SoHykPbcbcuzJIkCUjySLpG60uq6n2t+M4kB1fVHW0YkLta+U7gsIHNDwVub+WHLlM+uM3OJPsCjwHuWRpHVZ0HnAewbdu22r59+xBqt3knr6E7/mlH7uaNN03w8Oamv33w7o5zXjDSf7WwsEBf3ptRmpd6SuqF30hyInA93bwT99L1Vrp2YJ3FnkzfYY09nJIs9nBaqbfUw6z3JPLgidtpO9k37Scopzn+aYl9U7/sklwA/BxwV1Ud0crOAn4N+Fpb7TUD3SzOpLv68gHglVX1oVZ+FF13jf3prtR8VVVVkv3oxuY7CrgbOL6qdmwmZkkrOoE9z/aOesf99VFUYuk4c6M+cJQkabPalVjnA7dU1bkDi64ATgLOaX8vHyh/V5Jz6XpAHQ58sqoeSHJ/kmPohho5EXjrkue6BngR8FFPIkuSBHS9h19P1xPp9cAbgV9huD2c1tTzCdZ/Evmtl1z+4InbHS9bfd2+mfYTlNMc/7TEvtlLEi4E3kbXuDzIIQakKZLku4CfB85sRePYcS+NYcNnlVczyjOIfTlDaRz9i6MPMUiaKs8Cfgm4KcmNrew1dA3WlyY5BfgK8GKAqro5yaXA54DdwCvab2qAl/PQBSFXtRt0DePvbL2l7qH7XS5J0tyrqjsX7yf5Q+AD7eEwezjtBLYv2WZhWHWQZtWmGq4nNTaQV4cM1+AVql6dOrd+Fvj04g57TDvuPWzmrPJqRnnGuS9nKI2jf3H0IQZJ06OqPsHyJ3qhG8pruW3OBs5epvx64Ihlyr9Fa/ieFkt7UUmSNAqLw3K1h78AfLbdH1oPpyQfAn5nYELG5/LQhWOSVjCqQeDGOsTA0is1x3Gl2yQnfhk07Elo+nyF4DxdwTiBur6UgWFCxrHjHnWFJGklNoZJkiTNpyTvprvy+QlJdgKvBbYneQZdz+AdwK/DcHs4VdU9SV4PfKqt97rFiRolrWwUDddjH2Jg6ZWaBxxwwMivdFvLBDHjMOxJaPo8HtI8XcE4zrom+R7gZ2g75+Y/jXrHLUkaLntQSZIkra6qXrpM8fmrrD+0Hk5VdQFwwZqDlTT8hutJDDEgaeOq6v/Q9WQYLPulVdaf+a7JkiRJkiRJmqx9hv2ESQ4eeLh0iIETkuyX5Ad4aIiBO4D7kxzTZlQ/kT1nTD+p3XeIAUmSJEmSJEmaA5u64npSYwNpdOxmLEmSJG3O0rH0/V0tSZK0fptquJ7k2ECSJEmSJGk6ODmyJGm9RjE5oyRJkiRJ0syzh4Ukjc7Qx7iWJEmSJEmSJGkzvOJ6jezWJEmSJEmSJEnj4RXXkiRJkiRJkqReseFakiRJkiRJktQrDhWiFTnJhCRpVtz01fs4eY6G/XIfLvXLYE6aj5IkSWvjFdeSJEmSJEmSpF7ximtJkiRJkjR0S3sASZK0HjZcS5IkSeoFG7kkSZK0yKFCJEmSJEmSJEm9YsO1JEmSJEmSZl6SC5LcleSzA2UHJbk6yRfa38cNLDszya1JPp/keQPlRyW5qS17S5K08v2SvLeVX5dk68A2J7X/8YUkJ42pytJUs+Faa7b1jA8+eNPsSLKj7XBvTHJ9KxvLjluSJEmSpDG6EDh2SdkZwEeq6nDgI+0xSZ4KnAA8rW3z9iSPaNu8AzgVOLzdFp/zFODeqnoy8CbgDe25DgJeC/w4cDTw2sHjbEnL21TD9STPVEkaqmdX1TOqalt7PPIdtyRJ0jzyYhBptpnj/VZVHwfuWVJ8HHBRu38R8MKB8vdU1ber6svArcDRSQ4GDqyqa6qqgIuXbLP4XJcBz2ltXM8Drq6qe6rqXuBqHt6ALmmJzU7OeCHwNrokXbTY4HVOkjPa49OXNHg9CfhwkqdU1QM81OB1LXAlXfJexUCDV5IT6Bq8jt9kzJL27jhge7t/EbAAnM7Ajhv4cpLFHfcO2o4bIMnijvuqts1Z7bkuA96WJG0HL0mSJEnSJG2pqjsAquqOJE9s5YfQtVMt2tnKvtPuLy1f3Oa29ly7k9wHPH6wfJlt9pDkVLo2MrZs2cLCwsLqwe8Ppx25G2Cv6/bNrl27pi7mQdMc/7TEvqmG66r6+DJXQdvgJU2XAv48SQF/UFXnMZ4d99cHg9jMznk1o/wi7ssXvXH0L44+xCANGrzqa8c5L5hgJJKkWeZVxpoxWaasVinf6DZ7FnbH5OcBbNu2rbZv375qkG+95HLeeFPXvLfjZauv2zcLCwvsrX59Ns3xT0vsm73iejljb/CStCnPqqrbW65eneSvVll3mDvuPQs2sXNezSh33H35ojeO/sXRhxgkSZIkrcmdSQ5ubVgHA3e18p3AYQPrHQrc3soPXaZ8cJudSfYFHkM3NMlOHrrIc3GbheFW4+Enj7xgQdNuFA3XKxlZg9fSKzVHcaXbWq7snIS1XnU6bJO4knCermAcZ12r6vb2964k76ebKGIcO+6x8CpDzbokFwA/B9xVVUe0soOA9wJbgR3AS9pYeiQ5k24orgeAV1bVh1r5UXRDgO1PN2zXq6qqkuxHNyTYUcDdwPFVtWNM1ZMkaSq4P5am2hXAScA57e/lA+XvSnIu3ZC3hwOfrKoHktyf5BjgOuBE4K1Lnusa4EXAR1sOfwj4nYF54J4LnDn6qknTbRQN12Nv8Fp6peYBBxww9CvdTu5pl6fTjty9pqtOh20S3U/m6QrGcdU1yaOAfarq/nb/ucDrGMOOe+SVk+bHhTjfxMMsvdrktCMnFIikVTmsgGbIhbg/1hJbz/ggpx25m5PP+KAX0fREknfTXfn8hCQ7gdfSHfdemuQU4CvAiwGq6uYklwKfA3YDr2h5CvByHjrJdFW7AZwPvLMNj3sPXa5TVfckeT3wqbbe66pqbBd0SdNqFC2eNnjNAa9inRlbgPd3kxyzL/CuqvqzJJ9ixDtuScPhfBOSJE2e+2NpOlTVS1dY9JwV1j8bOHuZ8uuBI5Yp/xbt+HmZZRcAF6w5WEmba7ie1JkqScNRVV8Cnr5M+d2MYcctaWQmMt/EeidZHaWlw2hNamit9RhXjJt9X6Zl6K5piVPSTHP+J0mSNmFTDdeTPFMlSZLWbWTzTcD6J1kdpaVDfE1qaK31GFeMmx3ua1qG7pqWODXfnERrbo10fzyuE8nLnSCc9EniPp6oXoypbydT+3iCt48xSZqsfh/BSZKkjZiZCValeeCkbtLMmsj+eFwnkpc7QTjpuaH6eKJ6MaZJzBO1mj6e4O1jTJIma59JByBJkoZucY4IePh8Eyck2S/JD/DQfBN3APcnOSbdoPcnLtlm8bmcb0IajQvpJmAbtDip2+HAR9pjlkzqdizw9iSPaNssTup2eLstPueDk7oBb6Kb1E3S6Lk/liRpE/p1KlKSJK2L801ovRyaoH+c1E2afu6PtTfufyVp/Wy4XsXSHYskSX3jfBPSzJqLSVaHORZtH8e2Xa+1vt7zMg7sNNXT/bEkScNnw7UkSZI0PWZqktVhjofbx7Ft1+2mv33w7mpXY87LOLDzUk9JkrQ8x7iWJEmS+ufONpkbQ5zUDSdZlSRJ0rSY8ksS1AeO1SVJkjR0ixOxncPDJ3V7V5JzgSfx0KRuDyS5P8kxwHV0k7q9dclzXYOTukkaMofYlCSNig3XGrrBHy42YkuS1G/utyfPSd0kSZKkh7PhWtLcsHeAJKmPnNRNy/F3izTbPHEsSXtnw7UkSZKksXFYAUmSJK2FDdeSJGkm2BgmSZIkSbPDhmuNlF0cJUmSJElamcfNkrS8fSYdgKTJSXJYko8luSXJzUle1crPSvLVJDe22/MHtjkzya1JPp/keQPlRyW5qS17S5K08v2SvLeVX5dk69grKkmSJEnSKpLsaMe0Nya5vpUdlOTqJF9ofx83sL7HxtKIjazhetQJL2kodgOnVdWPAMcAr0jy1LbsTVX1jHa7EqAtOwF4GnAs8PYkj2jrvwM4FTi83Y5t5acA91bVk4E3AW8YQ70kSZJmytYzPvjgTZI0Ms9ux8Db2uMzgI9U1eHAR9pjj42lMRn1FdejTHhJm1RVd1TVp9v9+4FbgENW2eQ44D1V9e2q+jJwK3B0koOBA6vqmqoq4GLghQPbXNTuXwY8xxNQktRPNoxJkjR57o97ZfB49iL2PM712FgasXGPcX0csL3dvwhYAE5nIOGBLydZTPgdtIQHSLKY8FeNNWppDrRuSs8ErgOeBfxGkhOB6+muyr6XrlH72oHNdray77T7S8tpf28DqKrdSe4DHg98fWSVkSRJkiRpfQr48yQF/EFVnQdsqao7oLvwK8kT27ojOzZOcirdBZxs2bKFhYWFVYPesj+cduTuZZftbdtJ27VrV+9jXM00xz8tsY+y4XrUCa8pNHjG2Akn+iPJAcCfAK+uqm8meQfwero8fj3wRuBXgOXOBtcq5exl2WAMQ9s5r9VbL7n8wftHHvKYdW/fly964+hfHH2IQZIkSdPPY+ixelZV3d7aqq5O8lerrDuyY+PWfnYewLZt22r79u2rBv3WSy7njTct37y342WrbztpCwsL7K1+fTbN8U9L7KNsuB51wj+08ZIGr2E1GGy2UWwchtF4NynrfY/mqSFonHVN8ki6RutLqup9AFV158DyPwQ+0B7uBA4b2PxQ4PZWfugy5YPb7EyyL/AY4J6lcQxz57wRG9mh9+WL3jj6F0cfYpgHdp8draWvrwfLUn/c9NX7OLnlqLkpScNRVbe3v3cleT9wNHBnkoPbxZcHA3e11Ud2bDxMnvjQtBtZw/UYEn7wf+3R4HXAAQcMpcHg5Ck4ID7tyN1Dbbwbp/U2FM5TQ9C46trG0zofuKWqzh0oP3ixdwTwC8Bn2/0rgHclORd4Et2485+sqgeS3J/kGLqhRk4E3jqwzUnANcCLgI+2sb4kSZIkSZq4JI8C9qmq+9v95wKv46Hj2XPa38Vuux4bS2MwkhbPMSW8pM17FvBLwE1JbmxlrwFemuQZdD0cdgC/DlBVNye5FPgcsBt4RVU90LZ7OXAhsD/dOPSLY9GfD7yzjV1/D91ErJIkSZIk9cUW4P1trsR9gXdV1Z8l+RRwaZJTgK8ALwaPjaVxGdWluuNIeEmbVFWfYPkhea5cZZuzgbOXKb8eOGKZ8m/Rcl2SJEmSpL6pqi8BT1+m/G7gOSts47GxNGIjabgeR8Jr+jnWkiRJ02Nxv33akbvZPtlQJA1wPHpNwtYzPshpR+6eiuE1Z4n5LmneTOfgyJo57oA1aZ5IkSRJkiRJ6g8briVJkiSNzNILFCRJkqS1sOF6CX9YS5IkSZpW9iKTJEmzwoZrSZIkrYsNY5IkTZ77Y0mzzoZr9ZI7YE2SY65L/WXPKElaO3/TSJKkaWbDtSRJkjbMhjFJkqT+8wJBTSMbrtV7i1+upx25m5PP+KBfsJIkSZI0RvZ46j8bJSXNIhuuJWkv/BEoSZIkSZI0XjZcS5KkXvMqr+niyT6pv8xPSZI0TWy41tTxB7cmafDzd9qRu9k+uVAkSZIk6WGcf0LSrLDhWlPNRmxJkvrLA+f5ZU+J/vN3tDQ/zHct5W80TQsbrjUz/OLVJPgjUJLWzu9MSZIma3FffNqRuzn5jA+6P5bUazZcS9KQ2CAjSWvnd6bUH14AIknzzd9l6qupaLhOcizwe8AjgD+qqnOG9dx2Y5xdfvH2yyjzuI88ANSsmbcc1nj5nTke5rE0/caZxx4rzwePm8fP/bG0dr1vuE7yCOD3gZ8BdgKfSnJFVX1uspFpmqz2o8ud8+iZxyt/Bv38aRqMO4c9UJYH0cPnvljrsdbvYfNzvMxjjZr739Gbhjz2ggL1Se8broGjgVur6ksASd4DHAf0Jqk13WzUHgvzeAXraaDz86gJMoc1MTagDc3I89iTTvPH/Bw798caG49TRmbq8tiLsDRJ09BwfQhw28DjncCPb+YJ/VGttRrlZ2XOvuSHnsfzaKXP4+LEKpO21jjm7LM/K0aSw+6PNUxLP0/D+G6cse8rf1NrYjb6WdlsHs9YDoN5rJ4a1udoTiaMnJlj42G876M+lp3xz9JcmIaG6yxTVnuskJwKnNoe7nr2s599N/D1UQfWB6+EJ2Bdp1LesOrivdX1+4cazOitO4+TfH4vzznxz0NfPpPTFsdePvvD0IfXYy0xTFMe7zWHYUN5PDZ9yZPVTEOMMF9xruH7yjzukWn5bA7DvNR1s/Vc428O83iD+vg5NKa162NcizFt4HhhpvJ4Go+NN2rUn8M5OfbcqD7FvmIOT0PD9U7gsIHHhwK3D65QVecB5y0+TnJ9VW0bT3iTZV1n0wzWdd15vDd9eI36EINx9DOOPsQwZHvNYVh/Ho/TNLwn0xAjGOcUm/o83pt5es/npa7zUs916FUe9/H9Maa162NcfYxpBGby2Hijpjl2mO74pyX2fSYdwBp8Cjg8yQ8k+S7gBOCKCcckaX3MY2m6mcPS9DOPpelnHkvTzzyW1qH3V1xX1e4kvwF8CHgEcEFV3TzhsCStg3ksTTdzWJp+5rE0/cxjafqZx9L69L7hGqCqrgSuXMcmU9m9cYOs62yaubpuII/3pg+vUR9iAONYqg9x9CGGoRpBDo/bNLwn0xAjGOfUmoE83pt5es/npa7zUs8161ke9/H9Maa162NcfYxp6Gb02Hijpjl2mO74pyL2VD1sLgdJkiRJkiRJkiZmGsa4liRJkiRJkiTNkZlquE5ybJLPJ7k1yRmTjmeYkhyW5GNJbklyc5JXtfKDklyd5Avt7+MmHeuwJHlEkv+V5APt8UzWNcljk1yW5K/a+/sTs1rXYRhnnie5IMldST47ULbie5PkzBbX55M8b0gxrDv3RxTHdyf5ZJK/bHH8h0nE0Z53zd8NI4xhR5KbktyY5PpJxaHOKnlyVpKvtvfpxiTPH9hmrO9Jn3Jog3H25rUc+L8T/y7Q+ExLDg3TvHzG3af2Ux/3rX39HujrvrOv3yHLxNW73xjTIj1sA1vlu2Pdn78kR7X9w61J3pIkY6zHpvNnEvFnne1LfYp9RVU1Eze6Qe2/CPwg8F3AXwJPnXRcQ6zfwcCPtfuPBv438FTgPwFntPIzgDdMOtYh1vk3gXcBH2iPZ7KuwEXAr7b73wU8dlbrOoTXaqx5Dvwk8GPAZwfKln1vWj7+JbAf8AMtzkcMIYZ15f4I4whwQLv/SOA64Jhxx9Gee03fDSOOYQfwhCVlY4/D24Ov/Up5chbwr5dZf+zvSZ9yaINx9ua1HPjfE/8u8Da+27Tk0JDrPBefcfep/bz1cd/a1++Bvu47+/odskxcE32dpvVGT9vAVvnuWPfnD/gk8BMtx64CfnaM9dh0/kwiftbRvtS32Fe6zdIV10cDt1bVl6rq74D3AMdNOKahqao7qurT7f79wC3AIXR1vKitdhHwwokEOGRJDgVeAPzRQPHM1TXJgXSNo+cDVNXfVdU3mMG6DslY87yqPg7cs6R4pffmOOA9VfXtqvoycGuLd7MxrDf3RxVHVdWu9vCR7VbjjmOd3w0jiWEVfYlj7qySJysZ+3vSlxzaRJwrmUicPf8u0AhMSw4Ni5/xuaprL/Vx39rX74E+7jv7+h2yQlwrMd9X18s2sGEdvyY5GDiwqq6priX1YsbULjKM/JlE/BtoX+pN7KuZpYbrQ4DbBh7vZPUd69RKshV4Jt2Z3C1VdQd0XxDAEycY2jC9Gfi3wN8PlM1iXX8Q+BrwX1s3lD9K8ihms67D0Ic8X+m9GXlsa8z9kcXRukvdCNwFXF1Vk4jjzaz9u2GU70kBf57khiSnTjAOLbEkTwB+I8ln0g39s9gtbiLvSU9yaKNxQo9eS/rzXaAxmpYcGpI3Mz+fcfepPdenfWtfvwd6uO98M/38DlkuLujXb4xp0fvXZ5PHr4e0+0vLx+HNbD5/JhH/etuX+hT7imap4Xq58VZWO8s5lZIcAPwJ8Oqq+uak4xmFJD8H3FVVN0w6ljHYl24oindU1TOBv6XruqHl9TnPRxrbOnJ/ZHFU1QNV9QzgULozsUeMM44NfDeM8j15VlX9GPCzwCuS/OSE4tCAZfLkHcAPAc8A7gDeuLjqMpuP/D2ZdA6t1Qpx9ua17Nl3gcZoWnJos+bwM+4+tcf6tm/t6/dAn/adff0OWSWu3vzGmDK9fn2GcPw6kfoNMX8mEf9625f6FPuKZqnheidw2MDjQ4HbJxTLSCR5JF3iX1JV72vFd7bL+Gl/75pUfEP0LODnk+yg6+7yU0n+mNms605g58AZ+cvovmhmsa7D0Ic8X+m9GVls68z9kb9GrbvRAnDsmONY73fDyF6Lqrq9/b0LeD9dV72JvSdaPk+q6s52IPn3wB/yUPfSib4nE8yhdRmMs2evZW++CzQZ05JDmzBXn3H3qf3V531rX78HerLv7Ot3yLJx9eUzNYV6+/oM6fh1Z7u/tHzUhpU/k4h/ve1LfYp9RbPUcP0p4PAkP5Dku4ATgCsmHNPQJAndODW3VNW5A4uuAE5q908CLh93bMNWVWdW1aFVtZXuffxoVf0is1nXvwFuS/LDreg5wOeYwboOSR/yfKX35grghCT7JfkB4HC6CQ02ZQO5P6o4vjfJY9v9/YGfBv5qnHFs4LthVK/Fo5I8evE+8Fzgs+OOQw9ZKU8Wf6A1v0D3PsEE3pM+5NBm4uzTa9mX7wKN17Tk0DDM02fcfWp/9XHf2tfvgb7tO/v6HbJSXH36jTFl+nBs/DDDOn5tQ1rcn+SY9pwnMoZ2kWHlzyTi30D7Um9iX1VNeMbRYd6A59PNWPpF4LcmHc+Q6/b/0l2a/xngxnZ7PvB44CPAF9rfgyYd65DrvZ2HZnGdybrSdYm6vr23fwo8blbrOqTXa2x5Drybrrvad+jOOp6y2nsD/FaL6/MMadbdjeT+iOL4UeB/tTg+C/z7Vj7WOAaee03fDSN6LX6QbvblvwRuXvwcTuq18LZqnrwTuKmVXwEcPKn3pG85tIE4e/NaLol3Yt8F3sZ7m5YcGkG9Z/oz7j61v7c+7lv7+j3Q531nX79DlsQ18ddpWm/0sA1sle+OdX/+gG0tp74IvA3ImOuyqfyZRPyss32pT7GvdEsLSJIkSZIkSZKkXpiloUIkSZIkSZIkSTPAhmtJkiRJkiRJUq/YcC1JkiRJkiRJ6hUbriVJkiRJkiRJvWLDtSRJkiRJkiSpV2y4liRJkiRJkiT1ig3XkiRJkiRJkqReseFakiRJkiRJktQrNlxLkiRJkiRJknrFhmtJkiRJkiRJUq/YcC1JkiRJkiRJ6hUbriVJkiRJkiRJvWLDtSRJkiRJkiSpV2y4liRJkiRJkiT1ig3XkiRJkiRJkqReseFakiRJkiRJktQrNlxLkiRJkiRJknrFhmtJkiRJkiRJUq/YcC1JkiRJkiRJ6hUbriVJkiRJkiRJvWLDtSRJkiRJkiSpV2y4liRJkiRJkiT1ig3XUyrJf0zy6g1sd3KST4wgpKFJ8rIkf76B7bYkuSXJfqOISxqWWc7fYUnyyiTnTDoOaTnm8N65T9Y0m6ccT/K+JMdOOg5pbzaal7Muyc1Jtm9gu3OT/IvhRyQ9nPk7Okn2S/JXSZ446VhGxYbrKZTke4ETgT9oj7cnWRjScy+sdceXZEeSrcP4v4Oq6pKqeu4aYzg5yYVtuzuBjwGnDjsmaVhmPX83Y0n85wG/OMs7YE0nc3hl7pM1C+Yhx5PUwMNzgLNH8X+kYRllXq4jhkry5IHHa45hcP84bFX1tKpaaxyD3yv/GfitJN81irikRebv8A3GX1XfBi4ATp9oUCNkw/V0Ohm4sqr+76QD6aFLgF+fdBDSKk7G/N2rqvoWcBXdjxypT07GHF4r98maRiczozmeZN+lZVX1SeDAJNsmEJK0Viczo3k5KVV1B/BXwM9POhbNvJMxf0ftXcBJs9rT0Ybr6fSzwF+stDDJc5N8Psl9Sd6e5C+S/OqSdX43yb1JvpzkZzcbUJIXJPlfSb6Z5LYkZw0s29rOcP1yW3Zvkn+R5B8l+UySbyR528D6e3SzbNv+iyRfaNv+fpKsEMp1wA8m+f7N1kkakVnP3x9K8tEkdyf5epJLkjx2YNk9SX6sPX5SW2f7CqEtAC/YbP2kIetjDl/Y/tdVSXYl+R9Jvi/Jm9v/+askzxxY/0lJ/iTJ11oMrxxYdnSSa1pu35HkbYNXY7lP1hzoVY4nOSbJ3yR5xEDZLyT5TLu/lpx9RZIvAF9Y4d8s4P5W/ba3vKx0w8x9qf22/M9J9mnL/jrJUe3+L7Z1n9oe/2qSP233V8ylJB9v/+ov2372+M1UJsl/a3l9X5KPJ3nawLL17tN3JPnpdv+sJJcmuTjJ/emGEVntpNQC5r5Gz/xdOX/PSPLFlq+fS/ILA8vekeSygcdvSPKR5X53V9VO4F7gmM3UrbeqytuU3YCvAf9ohWVPAL4J/DNgX+BVwHeAX23LT26Pfw14BPBy4HYgm4xpO3Ak3cmQHwXuBF7Ylm0FCvgvwHcDzwW+Bfwp8ETgEOAu4J8MxPiJgecu4APAY4F/0Op/7CqxfAb4+Um/T968LXebg/x9MvAzwH7A9wIfB9488L9+DbgF+B7gQ8DvrhLXjwH3TPo98+Zt8NbTHL4Q+DpwVMvTjwJfpuux8Ajgt4GPtXX3AW4A/j3wXcAPAl8CnteWH0X3o3fflv+3AK8e+F/uk73N9K2nOf5F4GcGHv834Ix2fy05ezVwELD/Cs//m8D7Jv3ae/O20m21vGzLi254qoPavul/D+TlxcBp7f55LZ9ePrDsX7X7a8mlJw+pPr8CPJru9/KbgRsHlq15n97W3wH8dLt/Ft3v9Oe3df8jcO0qcfwz4NOTfn+9zfbN/F01f18MPInu9/nxwN8CB7dl39Nei5OBf9ye99BV4roCeOWk3+9R3Lziejo9Frh/hWXPB26uqvdV1W7gLcDfLFnnr6vqD6vqAeAi4GBgy2YCqqqFqrqpqv6+qj4DvBv4J0tWe31Vfauq/pwuId9dVXdV1VeB/w48k5WdU1XfqKqv0H2pPWOVde+ne42kPnosM5y/VXVrVV1dVd+uqq8B5w4+V1X9Id0VX9e12H9rldDuBx6zmbpJI/BYepbDzfur6obqhtl5P/Ctqrq4/Z/38tA+9h8B31tVr6uqv6uqLwF/CJwA0J7j2qraXVU76MYjXPp94D5Zs+yx9C/H3w28FCDJo1sc74Y15+x/rKp7auVu2uap+u6xrJyXi97QPudfoWtMemkr/wseyol/TNeYu/j4n7Tla82loaiqC6rq/urGpj0LeHqSwd+8a92nL+cTVXVlW/edwNNXWdfc1zg8FvN32fytqv9WVbe34/D30h0nH92W/R/gF+mOp/8Y+JfVXVm9kpnNZxuup9O9dGd4lvMk4LbFB1VVwNIP998MLP8/7e4BmwkoyY8n+Vi6bsf3Af+C7qqUQXcO3P+/yzxeLYbBg4L/s5d1Hw18Y69BS5Mx0/mb5IlJ3pPkq0m+SbeTXfpcfwgcAby17fBX8mjgvvXXSBqp3uVws9Z97PcDT2pdKb+R5BvAa2gNa0mekuQDrQvkN4Hf4eE57D5Zs6yPOf4u4J+lG7ty8QrJv4Y15+xtrM48Vd+tlpeLBj/nf02Xr9A1bP3jJN9Hd8Xje4FnpZuk8DHAjbDmXNq0JI9Ick4bHuCbdFdMs+R/DfO4+buzzPj2jbmvcTB/93z8YP4mOTHJjQO/yY8YfK7q5qH4EhDg0r2EN7P5bMP1dPoM8JQVlt0BHLr4oI1/c+gK6w7Tu+i6JhxWVY+hG1ZgpTEvR6btlJ8M/OW4/7e0RrOev/+RrivWj1bVgXRniR98riQH0J1FPx84K8lBqzzXj2Auq3/6mMPrcRvw5ap67MDt0VX1/Lb8HXSTNR3ecvg1bPD7wH2yplTvcryqPkd3IP+zwD+n228vWkvO1l7+hftb9d1qebnosIH7/4BumB6q6la6BtxXAh+vqvvpGndPpbs6+e/bNkPb/+3FPweOA36aruFtaysf+7Ez5r7Gw/xdRro5YP4Q+A3g8VX1WOCz7Hns/Aq6IUluB/7tXp5yZvPZhuvpdCUrd3v4IHBkkhe2A8ZXAN+3kX+SZHuSvf3QXfRourFov5XkaLqEnoSjgR2LV6FIPTTr+ftoYBfwjSSHAP9myfLfA26oql+lq+9/WeW5/glw1SZikUahjzm8Hp8Evpnk9CT7tytHjkjyj9ryR9ON4bsryT+kG6N3o9wnaxr1NcffRXfg/pN0Y1wvGkbOur9V362Wl4v+TZLHJTmMbvz59w4s+wu6xqHFCeIWljyGvefSnXTzQiwryUIGJjhfxaOBbwN3041h+ztr2GZUzH2Ng/m7vEfRnVj+Wovhl+muuF6M6Sl0Y2L/IvBLwL9N8ozlnqgddx8EXLuJeHrLhuvpdDHw/CT7L11QVV+nG+D9P9El01OB6+mSa70OA65Z47r/H+B1Se6nm/Bpb90YRuVlrN4QJk3arOfvf6CbVPE+ugP89y0uSHIccCzdUCTQTQb1Y0letvRJknw33RieF20iFmkU+pjDa9bG1/undONSf5luopc/4qHx5P813cmr++muAnnvw59lzdwnaxr1NcffTTeZ8kdbHIs2lbPtpNXftu7IUl+tmJcDLqebfPhGut+g5w8s+wu6BqePr/AY9p5LZwEXtS79L1nm/x8G/I811uWvga8Cn2NCDU1JDqb7DvvTSfx/zRXzdxmtN9Ub6X4L3AkcuRhDOzn+x3Rjf/9lVX2B7iryd7Zhw5b658BFexmGc2qlG5pN0ybJ7wB3VdWb97LePnRj772sqj62zv/xR8B/q6oPbTjQMUryRLovsWe2gfClXjJ/9y7Jv6QbumRvXaKksTOH9859sqbZPOV4kj8Bzq+qKycZh7Q3q+Vl671weBtWYOySHEqXzz8xif+/EUneCHyxqt4+6Vg0+8zf0WkN2X8J/GRV3TXpeEbBhusZlOR5wHV0A7//G7pujD+4ykziknrC/JWmmzkszTZzXOqfSTd8Sdo481d741Ahs+kngC/Sdf/9p8AL/TEtTQ3zV5pu5rA028xxSZKkMfGKa0mSJGnE2tj9H6ebHX5f4LKqem2Sg+jGYtwK7ABeUlX3tm3OBE4BHgBeuTisRJKjgAuB/ekmPXpVVVXrLnoxcBTdGMzHV9WOts1JwL9r4fx2VTmHgCRJknrNK64lSZKk0fs28FNV9XS6yTGPTXIMcAbwkao6HPhIe0ySpwInAE+jm9j27Uke0Z7rHcCpwOHtdmwrPwW4t6qeDLwJeEN7roOA1wI/DhwNvDbJ40ZaW0mSJGmTbLiWJEmSRqw6u9rDR7ZbAccBi1c/XwS8sN0/DnhPVX27qr4M3AocneRg4MCquqa6rpMXL9lm8bkuA56TJMDzgKur6p52NffVPNTYLWlIkjw2yWVJ/irJLUl+IslBSa5O8oX293ED65+Z5NYkn2/jpy+WH5XkprbsLS2PSbJfkve28uuSbJ1ANSVJGpt9Jx3AsD3hCU+orVu3rrrO3/7t3/KoRz1qPAH1hHWeLTfccMPXq+p7Jx3HqCyXx7P8fg6ynrNltXrOYx4PmpfPwCLrO5vWm8ftiukbgCcDv19V1yXZUlV3AFTVHUme2FY/BLh2YPOdrew77f7S8sVtbmvPtTvJfcDjB8uX2WZF85bHs1Qf67J2Q94f/x7wZ1X1oiTfBXwP8Bq6XhXnJDmDrlfF6Ut6VTwJ+HCSp1TVAzzUq+JauuGAjgWuYqBXRZIT6HpVHL9aQGs5Pl7Ul89NX+KA/sTSlzigP7EMxjHLv6sHc7gvr/1GGf9k9Tn+1XJ45hqut27dyvXXX7/qOgsLC2zfvn08AfWEdZ4tSf560jGM0nJ5PMvv5yDrOVtWq+c85vGgefkMLLK+s2m9edwapJ6R5LHA+5McsdrTL/cUq5RvdJs9/2lyKl2DGVu2bOF3f/d3Vwxw165dHHDAASsunzazVB/rsnbPfvazh7I/TnIg8JPAyQBV9XfA3yU5DtjeVrsIWABOZ6BXBfDlJIu9KnbQelW0513sVXFV2+as9lyXAW9Lklpl4qq1HB8v6st3d1/igP7E0pc4oD+xDMYxy7+rB3O4L6/9Rhn/ZPU5/tVyeOYariVJkqQ+q6pvJFmgu4ryziQHt6utDwbuaqvtBA4b2OxQ4PZWfugy5YPb7EyyL/AY4J5Wvn3JNgsrxHYecB7Atm3barUDnD4fAG3ELNXHukzEDwJfA/5rkqfT9a54FTCOXhVfHwxk6QmohYWFNVVg165da153lPoSB/Qnlr7EAf2JpS9xSBotG64lSZKkEUvyvcB3WqP1/sBP03XzvwI4CTin/b28bXIF8K4k59INI3A48MmqeiDJ/W1ix+uAE4G3DmxzEnAN8CLgo1VVST4E/M7A2LrPBc4cbY2lubMv8GPAv2zDAP0ebbLVFQyzV8WeBes4ATWoLycJ+hIH9CeWvsQB/YmlL3FIGi0briVJkqTROxi4qI1zvQ9waVV9IMk1wKVJTgG+ArwYoKpuTnIp8DlgN/CKNtQIwMuBC4H96YYPuKqVnw+8sw05cA/d+LlU1T1JXg98qq33uqq6Z6S1lebPTmBnVV3XHl9G13A9jl4VkiTNJBuuJUmSpBGrqs8Az1ym/G7gOStsczZw9jLl1wMPGx+7qr5Fa/heZtkFwAXri1rSWlXV3yS5LckPV9Xn6fL6c+020l4V46mhJEnjZ8O1JEmSJEmb9y+BS5J8F/Al4JdpPSxG2atCkqRZZcO1JEmSJEmbVFU3AtuWWTTyXhWSJM2ifSYdgCRJkiRJkiRJg7ziWlqjrWd88MH7O855wQQjkbRWg3kL5q40C9wf98dNX72Pk30/JA3wt5c0e/ztpUnyimtJkiRJkiRJUq/YcC1JkiRJkiRJ6hWHCpFWsbSrmyRJkiRJ0qyyHUR94hXXkiRJkiRJkqReseFakiRJkiRJktQrNlxLkiRJkiRJknrFMa4lSZI0lQbHYNxxzgsmGIkkSZKkYbPhWhrgJASSJEmSJEnS5NlwLUmaG16dKUmSJEnSdLDhWpIkSZIkSdKqvBBI4+bkjNIcS/LdST6Z5C+T3JzkP7Tyg5JcneQL7e/jBrY5M8mtST6f5HkD5Ucluakte0uStPL9kry3lV+XZOvYKypJkiRp7Lae8cEHb5IkrZcN19J8+zbwU1X1dOAZwLFJjgHOAD5SVYcDH2mPSfJU4ATgacCxwNuTPKI91zuAU4HD2+3YVn4KcG9VPRl4E/CGMdRLkiRJkiRJU8yhQqQ5VlUF7GoPH9luBRwHbG/lFwELwOmt/D1V9W3gy0luBY5OsgM4sKquAUhyMfBC4Kq2zVntuS4D3pYk7X9L2qQkhwEXA98H/D1wXlX9XpKzgF8DvtZWfU1VXdm2OZPupNIDwCur6kOt/CjgQmB/4ErgVVVVSfZr/+Mo4G7g+KraMZYKau55lZ4kSdJo+XtLfWXDtTTn2hXTNwBPBn6/qq5LsqWq7gCoqjuSPLGtfghw7cDmO1vZd9r9peWL29zWnmt3kvuAxwNfH1GVNOfm8EfXbuC0qvp0kkcDNyS5ui17U1X97uDKS3pOPAn4cJKnVNUDPNRz4lq6hutj6U5APdhzIskJdD0njh9D3SRJUg85zq0kaRxsuJbmXGusekaSxwLvT3LEKqtnuadYpXy1bfZ84uRUugYztmzZwsLCwh7Ld+3a9bCyWWQ9N++0I3evab1xvM7jeD/bSabFE033J7mFh04cLceeE+q9OTwBJUlTy+9sSdKo2HAtCYCq+kaSBborLO9McnC72vpg4K622k7gsIHNDgVub+WHLlM+uM3OJPsCjwHuWeb/nwecB7Bt27bavn37HssXFhZYWjaLrOfmnbzGg6cdLxvN/x807vezTX76TOA64FnAbyQ5Ebie7qrse7HnhCRJkiRpCthwLc2xJN8LfKc1Wu8P/DTdEABXACcB57S/l7dNrgDeleRcuiEGDgc+WVUPJLm/Tex4HXAi8NaBbU4CrgFeBHzUqzSl4UtyAPAnwKur6ptJ3gG8nq6Hw+uBNwK/wgR7Tgyal94Fi6zv+qy158SgeXp9JUmSpHlgw7U03w4GLmrjXO8DXFpVH0hyDXBpklOArwAvBqiqm5NcCnyOblzdV7ShRgBezkOTul3VbgDnA+9swxHcQze2rqQhSvJIukbrS6rqfQBVdefA8j8EPtAeTqznxKB56V2wyPquz1p7TgwaRy+KzZj0RKpJTgL+Xfsfv11VF420wpIkTaF2bHw98NWq+rkkBwHvBbYCO4CXtF6MTngujYEN19Icq6rP0A0rsLT8buA5K2xzNnD2MuXXAw8bH7uqvkVr+JY0fElCd4Lolqo6d6D84MVJVoFfAD7b7ttzQpqMiU2k2g66Xwtso+stcUOSKxYPvCVJ0oNeBdwCHNgenwF8pKrOSXJGe3y6E55L42HDteaek4lImnLPAn4JuCnJja3sNcBLkzyDrpFqB/DrYM8JaVImOZEq8Dzg6qq6p21zNd1B9LuHWUdp3rX8vJ/u6svdVbXNqzWl6ZHkUOAFdBdq/WYrPg7Y3u5fBCwAp+OE5w9rS9lxzgsmFIlm2T4b3TDJYUk+luSWJDcneVUrPyjJ1Um+0P4+bmCbM5PcmuTzSZ43UH5Ukpvasre0H9gk2S/Je1v5dW3SKUmS1FTVJ6oqVfWjVfWMdruyqn6pqo5s5T8/cPU1VXV2Vf1QVf1wVV01UH59VR3Rlv3G4o/oqvpWVb24qp5cVUdX1ZcmUVdpViyZSBW6iVQ/k+SCgd/OD06K2ixOmHoIa5xIFVicSHWl55I0fM9u++Jt7fHi1ZqHAx9pj5f2qjgWeHsbogAeulrz8HY7tpU/eLUm8Ca6qzUlDc+bgX9LN6TXoi2Lv6Pb3ye28mHupyWtYDNXXK/U3fFk7EYhSZIkPcyEJlJd0wSrLb41T7K6Zf89J9Kc9gkyZ2kSVevSK16tKU2BJD8H3FVVNyTZvpZNlikb6YTno/w+3MjE2EvtLbZp/z43/snYcMP1Kt0d3TFLksbKIX8kTYMJTqS6k4d+ny9us7BcjOuZZPWtl1zOG2966HCi7xNk7s0sTaJqXSamgD9PUsAftHza42rNJINXa147sO3iVZnfYY1XayZZvFrz64NBrOcE1KD1NGpspJFrFHGMWl9i6Usc0J9YRhDHs4CfT/J84LuBA5P8MXDn4twxSQ4G7mrrj33C81F+H25kYuyl9vY7YMq+zx/G+CdjKGNcL+nuOPYdsyRJktRnk5xINcmHgN8ZGIbkucCZo6qrNMeeVVW3t2Pgq5P81SrrjuxqzfWcgBq0nkaNjTRyrfXkVp8aV/oSS1/igP7EMuw4qupM2r6xXXH9r6vqF5P8Z7p96znt7+VtEyc8l8Zg0w3Xy3R3XHHVZcpG2o1iJX05QzhO1nllo7xaQJIkqZnYRKpVdU+S1wOfauu9bnGiRknDU1W3t793JXk/cDQTuFpT0lCdA1ya5BTgK8CLwQnPpXHZVMP1ct0d6VE3ipX05QzhOFnnlY3yagFJkiToJlJl+Ysyrlxlm7OBs5cpvx44Ypnyb9EOqJdZdgFwwVrjlbQ+SR4F7NOG0XwUXc+G1/HQFZZzf7Xm0qHddpzzgglFIq2uqhZoQ2pV1d3Ac1ZYb2j7aUnL23DD9UrdHXHHLEmSJEmaL1uA97ceyPsC76qqP0vyKbxaU9IcGDw55YkpDctmrrheqbuj3SgkSZIkSXOjqr4EPH2Zcq/WlCRpgzbccL1Kd0dwxyxJkiRJkiRJ2qBNT84oSZIkbcbScU8lSZIkaZ9JByBJkiRJkiRJ0iAbriVJkiRJkiRJveJQIZo7dkeWJmvrGR/ktCN3c3LLRWecliRJkiRJS3nFtSRJkiRJkiSpV7ziWpIkSZIkSdJQDPZ0t4etNsOGa0mSJEmSJGlOOISqpoUN19IcS3IYcDHwfcDfA+dV1e8lOQv4NeBrbdXXVNWVbZszgVOAB4BXVtWHWvlRwIXA/sCVwKuqqpLs1/7HUcDdwPFVtWMsFdRE9f0s+9Ifa32MUZIkSZKkeWXDtTTfdgOnVdWnkzwauCHJ1W3Zm6rqdwdXTvJU4ATgacCTgA8neUpVPQC8AzgVuJau4fpY4Cq6Ru57q+rJSU4A3gAcP4a6qUc8oy9JkiRJktbDhmvNhWE3ms3KlZpVdQdwR7t/f5JbgENW2eQ44D1V9W3gy0luBY5OsgM4sKquAUhyMfBCuobr44Cz2vaXAW9Lkqqq4ddIkiRJkiT1xWL7yWlH7ubkMz44te0nmox9Jh2ApH5IshV4JnBdK/qNJJ9JckGSx7WyQ4DbBjbb2coOafeXlu+xTVXtBu4DHj+KOkiSJEmSJGk2eMW1JJIcAPwJ8Oqq+maSdwCvB6r9fSPwK0CW2bxWKWcvywZjOJVuqBG2bNnCwsLCHst37dr1sLJZNEv1PO3I3Ssu27L/Q8vfesnlD5Yfechjhv6/1moUr/ssvZ+SJEmSJI2TDdfSnEvySLpG60uq6n0AVXXnwPI/BD7QHu4EDhvY/FDg9lZ+6DLlg9vsTLIv8BjgnqVxVNV5wHkA27Ztq+3bt++xfGFhgaVls2iW6nnyKkP0nHbkbt5408N3QTtetn1Nz/3w4X82vztb6/9ej1l6PyVJkiRJGieHCpHmWJIA5wO3VNW5A+UHD6z2C8Bn2/0rgBOS7JfkB4DDgU+2sbLvT3JMe84TgcsHtjmp3X8R8FHHt5YkSZIkSdJqbLiW5tuzgF8CfirJje32fOA/JbkpyWeAZwP/CqCqbgYuBT4H/Bnwiqp6oD3Xy4E/Am4Fvkg3MSN0DeOPbxM5/iZwxniqJs2HJIcl+ViSW5LcnORVrfygJFcn+UL7+7iBbc5McmuSzyd53kD5US33b03ylnYiinay6r2t/Lo2Jr4kSZIkSSPjUCHSHKuqT7D8GNRXrrLN2cDZy5RfDxyxTPm3gBdvIkxJq9sNnFZVn07yaOCGJFcDJwMfqapzkpxBd9Lo9CRPBU4AngY8Cfhwkqe0k1DvoBtr/lq674Fj6U5CnQLcW1VPTnIC8Abg+LHWUpIkSZI0V2y4liRpirWheu5o9+9PcgtwCHAcsL2tdhGwAJzeyt9TVd8Gvtx6QxydZAdwYFVdA5DkYuCFdA3XxwFntee6DHhbkjjsjzbj4WPVS5IkSdJDbLiWJA3FMBqhBp9jxzkv2PTzzZs2hMczgeuALa1Rm6q6I8kT22qH0F1RvWhnK/tOu7+0fHGb29pz7U5yH/B44OujqYkkSZIkad7ZcC1J0gxIcgDwJ8Crq+qbbXjqZVddpqxWKV9tm6UxnEo31AhbtmxhYWFhxXh37dq16vJZY30f7rQjdw/1f771ksv3eHzkIY8Z6vNvVpLDgIuB7wP+Hjivqn4vyUHAe4GtwA7gJVV1b9vmTLqheh4AXllVH2rlRwEXAvvTDevzqqqqJPu1/3EUcDdwfFXtaNucBPy7Fs5vV9VFI66yJEmStCk2XEuSNOWSPJKu0fqSqnpfK74zycHtauuDgbta+U7gsIHNDwVub+WHLlM+uM3OJPsCjwHuWRpHVZ0HnAewbdu22r59+4oxLywssNryWWN9H+7kEQ8VsuNlq///CZjYePStcfy1wDa6k043JLlisYFc0nAkeQRwPfDVqvq5cZ2YkiRpVtlwLUmaCo6Hu7x0l1afD9xSVecOLLoCOAk4p/29fKD8XUnOpWsMOxz4ZFU9kOT+JMfQDTVyIvDWJc91DfAi4KOObz3fVstHh/lZ3iTHoweeB1xdVfe0ba6ma+x+98gqLM2nVwG3AAe2x2fgRMkrcog4SdLe2HAtSdJ0exbwS8BNSW5sZa+ha7C+NMkpwFeAFwNU1c1JLgU+R3cF6CvagTLAy3noKq+r2g26hvF3toaze+gOtjWDPEE0HhMYj/7B8mW2kTQESQ4FXgCcDfxmK3ai5DUa3P9ceOyjJhiJJKlPbLiWJGmKVdUnWH4MaoDnrLDN2XQH1kvLrweOWKb8W7SGb0mbM6Hx6Nc0Tn2Lb81j1W/Zf8+xyqd9HPdZGoveukzEm4F/Czx6oMyJkiVJ2gQbriVJveSVn9J02nrGBzntyN2cfMYH7fq9xATHo9/JQ1d9Lm6zsFyM6xmr/q2XXM4bb3rocKKH44qvyyyNRW9dxivJzwF3VdUNSbavZZNlyoYyUXKLZ80noAat5yTBsCfY3Wgco9aXWPoSB/Qnlr7EIWm0bLjW1FnrWGg3ffW+kU/8JEmStBaTHI8+yYeA30nyuLbec4EzR1RVaR49C/j5JM8Hvhs4MMkfM4GJkmF9J6AGreckwSiPsy489lG9OVnRlxMnfYkD+hNLX+KQNFo2XEuSJGks5rwnxcTGo6+qe5K8HvhUW+91ixM1Stq8qjqTdjKoXXH9r6vqF5P8Z2ZkouQ5//6WJE2IDdfqvdV+JDkTtSRJmgaTHo++qi4ALlhrvJKGwomSpSmR5LuBjwP70bWVXVZVr01yEPBeYCuwA3hJVd3btjkTOAV4AHhlVX2olR/FQ3l8JfCq1gNqP+Bi4CjgbuD4qtoxpipKU8mGa82MpQ3cpx05oUCkOeLVN5JW43eEpHlTVQu0MeSr6m6cKFmaFt8GfqqqdrU5KT6R5CrgnwEfqapzkpwBnAGcnuSpdCeQnkbXc+LDSZ7STkK9g26c+WvpGq6PpTsJdQpwb1U9OckJwBuA48dbTWm6bKrhOskFwOJEFEe0srOAXwO+1lZ7TVVd2ZZ5NkrLWnpg69XTkiSNh43LkiRp3rWhd3a1h49stwKO46EJji+iOzF1eit/T1V9G/hy6w1xdJIdwIFVdQ1AkouBF9I1XB8HnNWe6zLgbUkyzmF/pGmz2SuuLwTeRte4POhNVfW7gwWejZIkSZIkSVIfJXkEcAPwZOD3q+q6JFuq6g6ANtHqE9vqh9C1YS3a2cq+0+4vLV/c5rb2XLuT3Ac8Hvj6kjhOpWsjY8uWLSwsLACwa9euB+9v1mlH7h7K86zHlv27/zusOozbMF//SZjW+DfVcF1VH0+ydY2rezZKkiRJkiRJvdMurHxGkscC70/ysGF7Biw3b0WtUr7aNkvjOA84D2Dbtm21fft2ABYWFli8v1knT6DH3WlH7uaNN+3LjpdtH/v/HoZhvv6TMK3x7zOi5/2NJJ9JckGSx7WyB88sNYtnnQ5hjWejgMWzUZIkSZIkSdJQVdU36IYEORa4M8nBAO3vXW21ncBhA5sdCtzeyg9dpnyPbZLsCzyGbrJVSSsYxeSM7wBeT3fW6PXAG4FfYYRno1bqRrGSab08fjP6Xuel3VTeesnlA8s29pyL3VDGoc+vrSRJkiRJWlmS7wW+U1XfSLI/8NN0w9VeAZwEnNP+LjZWXAG8K8m5dMPhHg58sqoeSHJ/kmOA64ATgbcObHMScA3wIuCjjiggrW7oDddVdefi/SR/CHygPdzM2aidq52NWqkbxUqm9fL4zehjnfecDGr451AWu6GMw7R2dZEkzR8nY5QkSXqYg4GL2jjX+wCXVtUHklwDXJrkFOArwIsBqurmJJcCnwN2A69oQ40AvJxuTrj96YbBvaqVnw+8sw2dew/dPHCSVjH0Vr0kBy8OXA/8AvDZdt+zUVLPJDmMbnLV7wP+Hjivqn4vyUHAe4GtwA7gJVV1b9vmTLqJUx8AXllVH2rlR/HQzvlK4FVVVUn2a//jKOBu4Piq2jGmKmrIbPCSJEnSKN301fv2GH93xzkvmGA0mhdV9RngmcuU3w08Z4VtzgbOXqb8euBh42NX1bdoDd+S1mZTDddJ3g1sB56QZCfwWmB7kmfQDemxA/h18GyUOjZ69c5u4LSq+nSSRwM3JLkaOBn4SFWdk+QM4Azg9CRPpcvDp9GdgPpwkqe0XH4H3ZA919I1XB9Ll8unAPdW1ZOTnEDX3er4sdZSkiRJkiRN3FZPTGkdNtVwXVUvXab4/FXW92yU1COtd8Qd7f79SW6hmxT1OLqTUgAX0U1McXorf09VfRv4cjupdHSSHcCBVXUNQJKLgRfSNVwfB5zVnusy4G1JYu8JSZIkSZIkrWQ8AwBL6r0kW+m6Rl0HbFkc8qeq7kjyxLbaIXRXVC/a2cq+0+4vLV/c5rb2XLuT3Ac8Hvj6kv+/6iSrfZ9gdFj6Xs9hTXg6zslT12oUr3vf309JkiRJkvrKhmtJJDkA+BPg1VX1zSQrrrpMWa1Svto2exbsZZLVPk4wOgp9r+fJQxruZ5yTp67VKCZZ7fv7qfnhUF2SJEmSps0+kw5A0mQleSRdo/UlVfW+VnxnkoPb8oOBu1r5TuCwgc0PBW5v5YcuU77HNkn2BR5DN2a9JEmSJEmStKx+Xe4maazSXVp9PnBLVZ07sOgK4CTgnPb38oHydyU5l25yxsOBT1bVA0nuT3IM3VAjJwJvXfJc1wAvAj7q+NaSNFpeYS1JkiRp2tlwLc23ZwG/BNyU5MZW9hq6ButLk5wCfIU2SWpV3ZzkUuBzwG7gFVX1QNvu5cCFwP50kzJe1crPB97ZJnK8BzhhxHWSJEmSJEnSlLPhWppjVfUJlh+DGuA5K2xzNnD2MuXXA0csU/4tWsO3ptO8XLk5WM8d57xggpFIkiRJ0nDNy3GdZotjXEuSJEmSJEmSesUrrjVSntGTJGk83OdKkiRJmiVecS1J0hRLckGSu5J8dqDsrCRfTXJjuz1/YNmZSW5N8vkkzxsoPyrJTW3ZW9rkrSTZL8l7W/l1SbaOtYKSJEmSpLlkw7UkSdPtQuDYZcrfVFXPaLcrAZI8lW6C1Ke1bd6e5BFt/XcApwKHt9vic54C3FtVTwbeBLxhVBWRJEmSJGmRQ4XMoaVdiVeahGyt6+1tO0nS6FTVx9dxFfRxwHuq6tvAl5PcChydZAdwYFVdA5DkYuCFwFVtm7Pa9pcBb0uSqqqhVUKSJEnS3Nlou5Pmhw3XsqFZkmbTbyQ5EbgeOK2q7gUOAa4dWGdnK/tOu7+0nPb3NoCq2p3kPuDxwNdHG77Wwn34dElyAfBzwF1VdUQrOwv4NeBrbbXXDPSSOJOu18MDwCur6kOt/Ci63hb7A1cCr6qqSrIfcDFwFHA3cHxV7WjbnAT8u/Y/fruqLhppZSVJkqRNsuF6yng2SpK0Bu8AXg9U+/tG4FeALLNurVLOXpbtIcmpdMONsGXLFhYWFlYMcNeuXasunzWjqu9pR+4e+nMOw5b9Jx9bTz9fFwJvo2tcHvSmqvrdwYIlQ/s8CfhwkqdU1QM8NLTPtXQN18fS9ZB4cGifJCfQDe1zfJKDgNcC2+jy94YkV7QTWpKGIMl3Ax8H9qM7zr6sql7b8u+9wFZgB/CSxdwb5skpSZJmkQ3XU6AvV1MNxjHYYN6X+CRJnaq6c/F+kj8EPtAe7gQOG1j1UOD2Vn7oMuWD2+xMsi/wGOCeFf7vecB5ANu2bavt27evGOPCwgKrLZ81o6rvyT3dB5925G7eeNNkf2bueNn2if7/5UxqaB/gecDVVXVP2+Zqusbudw+hWpI63wZ+qqp2JXkk8IkkVwH/DPhIVZ2T5AzgDOD0YZ6cGm81x2+l41BJ0uyz4XpO3PTV+4Z6cGtjtST1V5KDq+qO9vAXgM+2+1cA70pyLt1B8uHAJ6vqgST3JzkGuA44EXjrwDYnAdcALwI+6vjW0tCNemifB8uX2UbSELR946728JHtVnQnlLa38ouABeB0nHdCkqS9suG6h9bTKOzZZ0mab0neTXdA/IQkO+mGA9ie5Bl0B8w7gF8HqKqbk1wKfA7YDbyiXdkF8HIe6pZ8VbsBnA+8sx1Q30N3dZik4RnH0D4jGfJn6ZAwPR2eZc1maQgj6zIZSR4B3AA8Gfj9qrouyZbFk8lVdUeSJ7bVnXdCkqS9sOF6Ri1t/D7tyAkFIkkaqap66TLF56+y/tnA2cuUXw8csUz5t4AXbyZGDY89nmbPmIb22clDV3wubrOwQjxrHvLnrZdcvseQMH0cnmU9ZmkII+syGe1k8DOSPBZ4f5KH7VcHjGzeifWcgBq02kmCcc5bsNo8CeM+idGXEyd9iQP6E0tf4pA0WnPZcD04bMYsXaXswawkSbPH/ftsG8fQPkk+BPxOkse19Z4LnDnquknzqqq+kWSBbmzqOxfzPMnBwF1ttZHNO7GeE1CDVjtJMM45FVabJ2HcJ8j6cuKkL3FAf2LpSxySRmsuG677yIPS6eaQLZIkaW8mNbRPVd2T5PXAp9p6r1ucqFHScCT5XuA7rdF6f+Cn6SZPXDyhdE77e3nbxHknJEnaCxuuJUmSpDGY5NA+VXUBcMGag5W0XgcDF7VxrvcBLq2qDyS5Brg0ySnAV2g56rwTkiTtnQ3XkqSHsReIJEnS2lXVZ4BnLlN+N/CcFbZx3glJklZhw7UkSVLPePJIkiRJ0rzbZ9IBSJIkSZIkSZI0yCuuJ8irqTRpSS4Afg64q6qOaGVnAb8GfK2t9pqqurItOxM4BXgAeGVVfaiVH8VD4/BdCbyqqirJfsDFwFHA3cDxVbVjLJWTJEmSJElTY7CdbMc5L5hgJOoLG66l+XYh8Da6xuVBb6qq3x0sSPJUuglgnkY38/mHkzylTSLzDuBU4Fq6hutj6SaROQW4t6qenOQEupnVjx9ddSRpOnkyW5IkSZL25FAh0hyrqo/TzUi+FscB76mqb1fVl4FbgaOTHAwcWFXXVFXRNYK/cGCbi9r9y4DnJMnQKiBJkiRJkqSZ5BXXkpbzG0lOBK4HTquqe4FD6K6oXrSzlX2n3V9aTvt7G0BV7U5yH/B44OtL/2GSU+mu2mbLli0sLCzssXzXrl0PK5tFfannaUfuHunzb9l/9P9jM4b1HvTl/ZQkSZK0siSH0V2E9X3A3wPnVdXvJTkIeC+wFdgBvKQdHzuUpjQGNlxLWuodwOuBan/fCPwKsNyV0rVKOXtZtmdh1XnAeQDbtm2r7du377F8YWGBpWWzqC/1PHnEwxacduRu3nhTf3dBO162fSjP05f3U5IkSdKqdtNdtPXpJI8GbkhyNXAy8JGqOifJGcAZwOkOpSmNx6aGCklyQZK7knx2oOygJFcn+UL7+7iBZWcmuTXJ55M8b6D8qCQ3tWVvWRxKIMl+Sd7byq9LsnUz8Urau6q6s6oeqKq/B/4QOLot2gkcNrDqocDtrfzQZcr32CbJvsBjWPvQJJIkSZIkjVxV3VFVn2737wduoetBPDj85UXsOSymQ2lKI7bZy90u5OETu52BZ6OkqZXk4Kq6oz38BWDxxNQVwLuSnEuXw4cDn6yqB5Lcn+QY4DrgROCtA9ucBFwDvAj4aNt5q2ecGE6SJEl9N/ibdcc5L5hgJJpl7aLJZ9Id325ZPD6uqjuSPLGtNtKhNCV1NtVwXVUfX+Yq6OOA7e3+RcACcDoDZ6OALydZPBu1g3Y2CiDJ4tmoq9o2Z7Xnugx4W5LY8CUNR5J30+XrE5LsBF4LbE/yDLohPXYAvw5QVTcnuRT4HF03qle0E08AL+ehMbyuajeA84F3tny/h+7klSQJTxhJkiT1TZIDgD8BXl1V31zlguiRDaW50vxPm50/Z9JzDK13nqO+zRU07fMXTWv8oxhgdOxno/Y2qdvDAhxIlkm+aeP80uj7RGijMKk6T9MXQVW9dJni81dZ/2zg7GXKrweOWKb8W8CLNxOjJEmSJEmjluSRdI3Wl1TV+1rxnYu9ktswIHe18s0MpblztaE0V5r/abPz54x6HqO9We88R8Oad2hYpn3+ommNf5wzY43sbNTeJnVb6q2XXP5gskwyEcb5pdH3idBGYVJ17tuXqyRJ88ju5JIkaa3aWNPnA7dU1bkDixaHvzyn/b18oNyhNKURG0Wr3tjPRkmSJEmSJEkb9Czgl4CbktzYyl5D12B9aZJTgK/QehQ7lKY0HqNouPZs1Aocy1KSJEmSJKlfquoTLN/rH+A5K2zjUJrSiG2q4XqFid08GyVJkiRJkiRpQxz2TbDJhusVJnYDz0ZJkiRJkiRJkjZovmbrkyRJmhCHDJMkSZKktbPhWpLmlI1okiRJkiSpr/aZdACSJGnjklyQ5K4knx0oOyjJ1Um+0P4+bmDZmUluTfL5JM8bKD8qyU1t2VuSpJXvl+S9rfy6JFvHWkFJkiRJ0lyy4VqSpOl2IXDskrIzgI9U1eHAR9pjkjyVbqLjp7Vt3p7kEW2bdwCnAoe32+JzngLcW1VPBt4EvGFkNZEkSZIkqbHhWpKkKVZVHwfuWVJ8HHBRu38R8MKB8vdU1ber6svArcDRSQ4GDqyqa6qqgIuXbLP4XJcBz1m8GlvS+kyyh0SSk9r/+EKSk8ZUZUmSJGnDbLiWJGn2bKmqOwDa3ye28kOA2wbW29nKDmn3l5bvsU1V7QbuAx4/sshnzNYzPsjWMz7ITV+9b9KhqB8uZAI9JJIcBLwW+HHgaOC1gw3kkjYvyWFJPpbkliQ3J3lVK3f4LkmSNsjJGSVJWmJw4sod57xggpEM3XJXStcq5att8/AnT06la0xjy5YtLCwsrBjIrl27Vl0+K047cjcAW/Z/6P486Ft9+/JZq6qPL9PQdBywvd2/CFgATmeghwTw5SSLPSR20HpIACRZ7CFxVdvmrPZclwFvaw1ezwOurqp72jZX0zV2v3vYdZTm2G7gtKr6dJJHAze0XDuZ7uTUOUnOoDs5dfqSk1NPAj6c5ClV9QAPnZy6FriSLl+vYuDkVJIT6E5OHT/WWkqSNEY2XEuSNHvuTHJwVd3RhgG5q5XvBA4bWO9Q4PZWfugy5YPb7EyyL/AYHj40CQBVdR5wHsC2bdtq+/btKwa4sLDAastnxcntJMhpR+7mjTfNz8+uvtV3x8u2TzqE1ezRQyLJYA+JawfWW+wJ8R3W2EMiyWIPiZV6W0gakpbHi7l8f5Jb6PJs5Cen2jBfkiTNnP4cUUiSpGG5AjgJOKf9vXyg/F1JzqW7uutw4JNV9UCS+5McA1wHnAi8dclzXQO8CPioB8jSWAyzh8RIek4svbK+L1e2b9Qs9QSxLpPVelY8k26fOo6TU18fTU0kSZosG64lSZpiSd5NdyXXE5LspBvH9hzg0iSnAF8BXgxQVTcnuRT4HF2X5le0LskAL6cbf3d/uqu6rmrl5wPvbFeC3UPXrVnS8Iyjh8ROHrric3GbheWCWU/PibdecvkeV9b3/Mr2vZqlniDWZXKSHAD8CfDqqvrmKvMZj2z4rvWcgBq02kmCcQ7/tNbhpsZxQqMvJ076Egf0J5a+xCFptGy4HrHBcVIlaZL8PppNVfXSFRY9Z4X1zwbOXqb8euCIZcq/RWv41tqYa1qnkfeQSPIh4HcGJoV7LnDm6KsmzZckj6RrtL6kqt7Xisc+fNd6TkANWu0kwclj3LetdbipcZws68uJk77EAf2JpS9xaDyW/r6esXmItIp9Jh2AJEmSNA9aD4lrgB9OsrP1ijgH+JkkXwB+pj2mqm4GFntI/BkP7yHxR8CtwBfZs4fE41sPid+kmwSONinj64FPtdvrFidqlDQcbSLU84FbqurcgUWLJ5Tg4SenTkiyX5If4KGTU3cA9yc5pj3niUu2WXwuh++SJM08r7iW5liSC4CfA+6qqiNa2UHAe4GtwA7gJVV1b1t2Jt1s5g8Ar6yqD7Xyo3hoiIErgVe1K7z2Ay4GjgLuBo6vqh1jqp4kSb0yyR4SVXUBcMGag5W0Xs8Cfgm4KcmNrew1OHyXJEkbZsO1NN8uBN5G17i86AzgI1V1TpIz2uPTkzyV7sfx0+i6LH84yVPaD+x30I2jdy1dw/WxdD+wTwHuraonJzkBeANw/FhqJkmSJI1JVX2C5cegBofvkiRpQ2y4luZYVX28zXo+6DgemsDpIrrJm05v5e+pqm8DX25XehydZAdwYFVdA5DkYuCFdA3XxwFntee6DHhbktilUZIkSdJmOOattHfOvaJp5xjXkpba0sbWo/19Yis/BLhtYL2dreyQdn9p+R7bVNVu4D7g8SOLXJIkSZIkSTPBK64lrdVyXR9rlfLVtnn4kyen0g03wpYtW1hYWNhj+a5dux5WNotGWc/Tjtw9kufdiC379yue1Wzm/ZiXz60kwZ5XdXnloyRJkjbLhmtJS92Z5OCquiPJwcBdrXwncNjAeocCt7fyQ5cpH9xmZ5J9gcfQTSTzMFV1HnAewLZt22r79u17LF9YWGBp2Swadj337BrWn6/8047czRtv6k88q9nxsu0b3nZePrfzzi6YkiRJkjR8DhUiaakrgJPa/ZOAywfKT0iyX5IfAA4HPtmGE7k/yTFJApy4ZJvF53oR8FHHt5YkSZIkSdLeTMflbpJGIsm76SZifEKSncBrgXOAS5OcAnyFNnN5Vd2c5FLgc8Bu4BVV9UB7qpcDFwL7003KeFUrPx94Z5vI8R7ghDFUS5IkSZIkzSiHJ5sfNlxLc6yqXrrCouessP7ZwNnLlF8PHLFM+bdoDd+SJEmSpse0DYVlQ5YkzR6HCpEkSZIkSZIk9YpXXI/AtJ2Z1nB5pl+T5neQJEmSJEmadjZcS5IkrZMniCRJkiRptBwqRJIkSZIkSZLUKzZcS5IkSZIkSZJ6xYZrSZIkSZIkSVKv2HAtSZIkSZKkuZbkgiR3JfnsQNlBSa5O8oX293EDy85McmuSzyd53kD5UUluasvekiStfL8k723l1yXZOtYKSlNoZA3XSXa0RL0xyfWtbGgJL0mSJEmSJA3JhcCxS8rOAD5SVYcDH2mPSfJU4ATgaW2btyd5RNvmHcCpwOHttvicpwD3VtWTgTcBbxhZTebI1jM+uMdNs2XfET//s6vq6wOPFxP+nCRntMenL0n4JwEfTvKUqnqAhxL+WuBKuoS/asRxS5IkPcgfwdNt8P3bcc4LJhiJJEnqq6r6+DJXQR8HbG/3LwIWgNNb+Xuq6tvAl5PcChydZAdwYFVdA5DkYuCFdO1YxwFntee6DHhbklRVjaZG0vQbdcP1UsNM+N7wYFaSJEmSJGnmbKmqOwCq6o4kT2zlh9BdYLloZyv7Tru/tHxxm9vac+1Och/weGDwgk+SnEp3ASdbtmxhYWEBgF27dj14f61OO3L3utYfpS37jyee9b5Ga7WR179PpjX+UTZcF/DnSQr4g6o6j+Em/INWSuqVDCbLMN60Pn0RrGRcXxB90oc6T+OXgqaPJ88kSZKkhyz9fWxvG43AcsPY1irlq22zZ0HXfnYewLZt22r79u1A176weH+tTu7RseJpR+7mjTeN/vrZHS/bPpLn3cjr3yfTGv8oPzHPqqrbW+P01Un+apV1N5LwDxWskNQreesllz+YLMP4QPfpi2Al4/qC6JM+1HlUX5iSJEmSJGnk7kxycLv48mDgrla+EzhsYL1Dgdtb+aHLlA9uszPJvsBjgHtGGbw07UY2OWNV3d7+3gW8HzialvAAQ0h4SZK0CidKliRJkjblCuCkdv8k4PKB8hOS7JfkB+gmYfxkG2Xg/iTHtN/MJy7ZZvG5XgR81PGtpdWNpOE6yaOSPHrxPvBc4LMMN+ElSdLePbuqnlFV29rjYc6MLmlIRn2iqf3Ofm8rv26ZyackSZprSd4NXAP8cJKdSU4BzgF+JskXgJ9pj6mqm4FLgc8Bfwa8oqoeaE/1cuCPgFuBL/LQPG3nA49v87r9Ju13uKSVjWochS3A+9vv5H2Bd1XVnyX5FHBpS/6vAC+GLuGTLCb8bh6e8BcC+9Mle68mZpSkSXBca23CTE6UPArmmSbg2VU1OEHT4ommc5Kc0R6fvuRE05OADyd5Svv9vHii6VrgSroTTVcBpwD3VtWTk5wAvAE4flwVk2ZdkguAnwPuqqojWtlBwHuBrcAO4CVVdW9bdiZdXj4AvLKqPtTKj+Kh498rgVdVVSXZD7gYOAq4Gzi+qnaMqXrSXKiql66w6DkrrH82cPYy5dcDRyxT/i1aO5iktRnJFddV9aWqenq7Pa0lM1V1d1U9p6oOb3/vGdjm7Kr6oar64aq6aqD8+qo6oi37DbtRSJK0ZosTJd/QJjKGJRMlA4MTJd82sO3ihMiHsIaJkiWNxHF0J5hof184UP6eqvp2VX2Z7oquo9tQfAdW1TXtN/PFS7ZZfK7LgOc47I80VBfy8B5Jw+zl9ODJJ+BNdCefJElLbD3jgw/eNP3ma7a+ITIBJElTYGwTJQO0xvFTAbZs2cLCwsKK/2zXrl2rLu+D047cPbTn2rL/cJ+v7/pc3x5/7hZPNBXwB23y8T1ONLVchu7k0bUD2y6eUPoOK59oevDkVFXtTnIf8Hhg8ApvSRtUVR9fZgieYfZyOg44qz3XZcDbksQLuyRJs8yGa0kraj+e76frwri7qrYNs8vjOOsizaPBiZKT7DFR8pBmRl/6/84DzgPYtm1bbd++fcXYFhYWWG15H5w8xJPUpx25mzfeND8/u/pc3x0v2z7pEFYy6hNNazoJtZ4TUKudoHjrJQ9NS3PkIY9Z8Tn6ZBpOqK2VdemNiZx8Wk8eD1r6Wk/qBOQoTn5u9DPUl89fX+KA/sTSlzgkjVY/jygk9ckox9uUNCJtcuR9qur+gYmSX8dDEyWfw8MnSn5XknPpcnhxouQHktyf5BjgOrqJkt863tpIs28MJ5oWt9mZZF/gMcA9LLGeE1BvveTyNZ2g6PHJgj1Mwwm1tbIuvTeyk0+wvjwetPS1HuYJ3PUYxcnPjX4P9eXz15c4oD+x9CUOSaNlw7Wk9XJitwlxiKLJWPq67zjnBROKZN2cKFmaEmM60bT4XNcALwI+au8naeTGfvJJyxv8PTdFv+Ukae7ZcL1GNhhpTo16vE1JI1JVXwKevkz53QxpZvRZ5P5eEzKOE03nA+9sJ5bvoeslJWm0PPkkSdIm2HAtaTVjm9htb2PxzcsYZqvVs68TnW1Enydu25v1fA7n5XMraXPGcaKpqr5Fa/iWNHxJ3k3XK/EJSXYCr6VrsPbkkyRNiL0tpp8N15JWNM6J3fY2Ft+8jGG2Wj0nNc7gKPR54ra9Wc8YifPyuZUkad5V1UtXWOTJJ0mSNmg6Ww0kjZwTu02eQxZIkiRJkqR5ZcP1Kmw00pxzYjdJkiRJkiRNhA3XkpblxG6SJEmSJE0PL8Bc2dLXxjGvp4MN15LUIzd99b6ZGsta6jt/3Euj54GipD5xsjZJmh42XC/hAayGyQM1SZIkSZIkaf1suJakCVp6cuO0IycUiCRJkiRJUo/MfcO1V1hLkiRJkiRJ88Nhg6bDPpMOQJIkSZIkSZKkQXN/xbUkjZs9PaTJMgelyfIKJ0l94ZxEktRvNlxLkiRpbthIIUmSJE0HG64lSZIkSZIkzSV7g/WXDdeSNAYOTSBNljkoSZIkSdPFhmtJkiRJE+HQLZIkqU/8bdIvNlxLkiRJkqS553ABktQvNlxL0gg4LIE0WeagNJ1sNJIkSdKifSYdgCRJkiRJUp9sPeOD3PTV+9h6xgc9IS5JE+IV19IYeRXRbPMH7XwwjyVJkiRpPiwe/5125G62TzaUuWTDtSRJmgmePJJmiycKJfWJ30mSnLhx/Gy4lqRNsKFMkqTR80BRUp/YiC1J42HDtSRJmkqeOJLml41G0vDd9NX7ONl9qyStmb9HRs+Ga0laBxvKpMkyBzVsHnBIkjbDHiGSwN+UozIVDddJjgV+D3gE8EdVdc6EQ5K0TuaxNN3MYWn6zWoer3ZCywNHzZpZzeNZstJ3kt9HWjTsPPbCjv7xhNbw9L7hOskjgN8HfgbYCXwqyRVV9bnJRiZpraY9j/0hoJXMy1n1SeewOSht3qTzeFJs1NYsmdc8nhUb/T3jd9VsMY/n07wcN45C7xuugaOBW6vqSwBJ3gMcB5jUmmpzdgau93lsw5i0qpHnsDmoPpjxfXPv98XjtvT9Pu3I3ese33fGPiPqP/N4Dm0944Pr/n7yu6nXzOM5t9bjHvO4Mw0N14cAtw083gn8+OAKSU4FTm0PdyX5/F6e8wnA14cW4RR4pXXuvbxhXat//4jCGJVh5PFUvZ8bNW2f242axXqukMOr1XOa8nivOQzr3h/P3GdgNbP4mV/NrNR3Dftm83gG3udFG/ncrvP32zjN0nsz6rrMex4P6sXnpk/7kL7Est44Rvzd1IvXhD3jmKk8XiWH+/Lab0hf8mmjxh3/CPK4z6//ijk8DQ3XWaas9nhQdR5w3pqfMLm+qrZtNrBpYp01YZvO43l5P63nbJmheu41h2F9++MZem3WxPqqB8zjvZil+liXmTX0PN7jyXvyWvclDuhPLH2JA/oTS1/i2IANHxtPcZ0B45+0aY1/n0kHsAY7gcMGHh8K3D6hWCRtjHksTTdzWJp+5rE0/cxjafqZx9I6TEPD9aeAw5P8QJLvAk4ArphwTJLWxzyWpps5LE0/81iafuaxNP3MY2kdej9USFXtTvIbwIeARwAXVNXNm3zadXebmgHWWRMzpDyel/fTes6Wmain++KhsL6aKPN4TWapPtZlBo0ojwf15bXuSxzQn1j6Egf0J5a+xLEum8zjqazzAOOfrKmMP1UPGxJLkiRJkiRJkqSJmYahQiRJkiRJkiRJc8SGa0mSJEmSJElSr8xVw3WSY5N8PsmtSc6YdDybkeSwJB9LckuSm5O8qpUflOTqJF9ofx83sM2Zre6fT/K8gfKjktzUlr0lSSZRp7VI8ogk/yvJB9rjma6vOtOcu/OWq/OQo0kem+SyJH/V3tefmMV6jtI05/SiecvtRfOQ41qbWcjjRUkuSHJXks9OOpbNWum7aRol+e4kn0zyl60u/2HSMc2qSebzcvm32r5lhHGse78+ojiW/dxP4jUZiGnN+/4RxrCj/Xa4Mcn1k4pjkqZtv9uXnNqsPnz+NyrrPG7tq7lpuE7yCOD3gZ8Fngq8NMlTJxvVpuwGTquqHwGOAV7R6nMG8JGqOhz4SHtMW3YC8DTgWODt7TUBeAdwKnB4ux07zoqs06uAWwYez3p9594M5O685eo85OjvAX9WVf8QeDpdfWexniMxAzm9aN5ye9E85Lj2YobyeNGFzM7ncKXvpmn0beCnqurpwDOAY5McM9mQZk8P8vlCHp5/y+5bRmxd+/URWulzP4nXZNGa9v1j8OyqekZVbZtwHGPXgzzdiL7k1Gb15fO/EWs+bu2zuWm4Bo4Gbq2qL1XV3wHvAY6bcEwbVlV3VNWn2/376T6Ah9DV6aK22kXAC9v944D3VNW3q+rLwK3A0UkOBg6sqmuqm6nz4oFteiXJocALgD8aKJ7Z+upBU52785Sr85CjSQ4EfhI4H6Cq/q6qvsGM1XPEpjqnF81Tbi+ahxzXms1EHi+qqo8D90w6jmFY5btp6lRnV3v4yHarCYY0qyaazyvk30r7llHGsd79+qjiWOlzP/bXBNa97x+3vsQxDlO33+1LTm1Gzz//q9rAcWtvzVPD9SHAbQOPdzKlP+KWSrIVeCZwHbClqu6A7osCeGJbbaX6H9LuLy3vozcD/xb4+4GyWa6vOjOTu3OQq29m9nP0B4GvAf+1dRn7oySPYvbqOUozk9OL5iC3F72Z2c9xrc3M5fEsWvLdNJVaF+0bgbuAq6tqauvSY33M55X2LWOxxv36KP//cp/7Sb0mb2bt+/5RKuDPk9yQ5NQJxjEpfczTNZt0Tm3Cm+nH538j1nvc2lvz1HC93PiJU3/GPskBwJ8Ar66qb6626jJltUp5ryT5OeCuqrphrZssUzY19dUeZuI9m/VcnaMc3Rf4MeAdVfVM4G9ZvXvVtNZzlGaq7rOe24vmKMe1Nr6PPbeO76Zeq6oHquoZwKF0vTaOmHBIs8h8HtCH3OnL534D+/5RelZV/RjdUBmvSPKTkw5ozKY2T/uQUxvRs8//Rqz3uLW35qnheidw2MDjQ4HbJxTLUCR5JN0XwCVV9b5WfGfrhkv7e1crX6n+O9v9peV98yzg55PsoOsW81NJ/pjZra8eMvW5Oye5Oi85uhPYOXDF12V0PwhmrZ6jNPU5vWhOcnvRvOS41mZm8ngWrfDdNNVa9+YFZmcs8j7pYz6vtG8ZqXXu10duyed+EnGsd98/MlV1e/t7F/B+uqEzJvbeTEAf83Sv+pZT69Sbz/8Grfe4tbfmqeH6U8DhSX4gyXfRTRh0xYRj2rAkoRur5paqOndg0RXASe3+ScDlA+UnJNkvyQ/QTYb0ydY14P4kx7TnPHFgm96oqjOr6tCq2kr33n20qn6RGa2v9jDVuTsvuTovOVpVfwPcluSHW9FzgM8xY/UcsanO6UXzktuL5iXHtWYzkcezaJXvpqmT5HuTPLbd3x/4aeCvJhrUbOpjPq+0bxmZDezXRxXHSp/7sb8mG9j3j0SSRyV59OJ94LnAZ8cdx4T1MU9X1Zec2qi+fP43agPHrf1VVXNzA54P/G/gi8BvTTqeTdbl/6XrGvIZ4MZ2ez7weLqZQb/Q/h40sM1vtbp/HvjZgfJtdF/8XwTeBmTS9dtL3bcDH2j3Z76+3qY7d+cxV2c9R+lmeL++vad/CjxuFus54tdwanN6oA5zl9sD8c50jntb8+dg6vN4oC7vBu4AvkN3hdIpk45pE3VZ9rtp0nFtsC4/CvyvVpfPAv9+0jHN6m2S+bxc/q22bxlhHOver48ojmU/95N4TZbEtaZ9/4j+9w8Cf9luNy9+Rif9moz7Nm373b7k1JDqMrHP/ybjfgbrOG7t6y2tMpIkSZIkSZIk9cI8DRUiSZIkSZIkSZoCNlxLkiRJkiRJknrFhmtJkiRJkiRJUq/YcC1JkiRJkiRJ6hUbriVJkiRJUyXJBUnuSvLZNa7/kiSfS3JzkneNOj5JkrR5qapJxyBJkiRJ0pol+UlgF3BxVR2xl3UPBy4Ffqqq7k3yxKq6axxxSpKkjfOKa0mSJEnSVKmqjwP3DJYl+aEkf5bkhiT/Pck/bIt+Dfj9qrq3bWujtSRJU8CGa0mSJEnSLDgP+JdVdRTwr4G3t/KnAE9J8j+SXJvk2IlFKEmS1mzfSQcgSZIkSdJmJDkA+H+A/5ZksXi/9ndf4HBgO3Ao8N+THFFV3xhzmJIkaR1suJYkSZIkTbt9gG9U1TOWWbYTuLaqvgN8Ocnn6RqyPzXG+CRJ0jo5VIgkSZIkaapV1TfpGqVfDJDO09viPwWe3cqfQDd0yJcmEackSVo7G64lSZIkSVMlybuBa4AfTrIzySnAy4BTkvwlcDNwXFv9Q8DdST4HfAz4N1V19yTiliRJa5eqmnQMkiRJkiRJkiQ9yCuuJUmSJEmSJEm9YsO1JEmSJEmSJKlXbLiWJEmSJEmSJPWKDdeSJEmSJEmSpF6x4VqSJEmSJEmS1Cs2XEuSJEmSJEmSesWGa0mSJEmSJElSr9hwLUmSJEmSJEnqFRuuJUmSJEmSJEm9YsN1DyX5j0lePek4ZlWSc5P8i0nHofllju9dkh9N8j8nHYem26zmWpIdSX560nFsVpLXJPmjDWzn98McMH/ng7/LZ5t5PB/cL88vc3y6JPlkkqdNOo71suG6Z5J8L3Ai8Aft8fYkC2OOoZI8eeDxmmNIcnKSC0cV20a1L56t7eF/Bn4ryXdNMCTNKXN81ec+K8lZAFX1GeAbSf7pKP6XZl8fcm2jklyY5LeH8Dz/KsnfJLkvyQVJ9htYNrhf3Nvz1GZjWU5V/U5V/eoaY/D7YY6Yv/3P383wd/l8MI9nPo/dL885c7z/OZ5ka5IdA0W/C7xuFP9rlGy47p+TgSur6v9OOpBZVVV3AH8F/PykY9FcOhlz/GGS7LtM8SXAr487Fs2MkxlDrq3w2Z24JM8DzgCeA2wFfhD4D5OMacj8fphtJ2P+znL+Psjf5TPtZMzjucjjxv3y/DkZc7y3Ob7C63YF8OwkB487ns2w4bp/fhb4i5UWtislX5nkS0m+nuQ/J9mnLfvrJEe1+7/Y1n1qe/yrSf603T86yTVJvpHkjiRvW7zKIcnH27/6yyS7khy/0Yq0szuV5JeT3Jbk3iT/Isk/SvKZ9v/ftmSbX0lyS1v3Q0m+f2DZ77Xn+WaSG5L844FlZyW5NMnFSe5PcnOSbauEtwC8YKN1kzZhlnL8z5L8xpKyv0zyz9r9veXsZUn+OMk36X74LLUAPGfwzLW0DpvJtR9K8tEkd7dllyR57MC2O5KcnuQzwN8m2TfJGUm+2PZBn0vyCwPrn5zkfyR5U8vLLyX5f1r5bUnuSnLSKrH+Usv/u5P81hrrfxJwflXdXFX3Aq9n+Txbl7ZPv6XV80tJfn1g2fYkO5P821anO5K8MMnzk/zvJPckec3A+mcl+eN2f/E3w0lJvtJe99XquoDfD7PM/O1//q72W+P/aa/9Ye3x09t6/3CF0Bbwd/ksMo9Hk8cLSX47yf9M91v+/5fk8e01+maST2XgKs8k/zDJ1S2HP5/kJQPLXpDkf7Xtbku7grotc7+svTHHh5zjSU5Icv2Ssn+V5Ip2fy05e0qSrwAfXfr8VfUt4AbguZuJc+yqyluPbsDXgH+0yvICPgYcBPwD4H8Dv9qWXQyc1u6fB3wRePnAsn/V7h8FHAPsS3dm6Bbg1Uv+x5OHUJet7bn+C/DddMnxLeBPgScChwB3Af+krf9C4FbgR1ps/w74nwPP94vA49uy04C/Ab67LTurPffzgUcA/xG4dpXY/hnw6Um/397m7zZjOX4i8D8GHj8V+AawX3u8t5z9Tsv7fYD9V/gf3wR+dNLvm7fpu20y154M/AywH/C9wMeBNw9suwO4EThs8bMLvBh4Uvs8Hw/8LXBwW3YysBv45baP+m3gK8Dvt//xXOB+4IC2/oXAb7f7TwV2AT/Z1j23PddP76X+fwkcP/D4Ca3Oj9/k6/oC4IeAAP8E+D/Aj7Vl21ts/x54JPBr7X14F/Bo4Gl0++ofbOufBfxxu7+1xfeHwP7A04FvAz+ySix+P8zozfydivzd22+Ns+kOmvcHPgP8xipx+bt8Bm/m8cjyeIHumPmHgMcAn2uv3U+3fLwY+K9t3UcBt7V67wv8GPB14Glt+XbgyPaa/ShwJ/DCtmwr7pe9rXIzx4ef48D3tDgPHyj7FHBCu7+WnL245f5Kx9dvAc6d9OdnXa/LpAPwtuQN6Rpy/uEqyws4duDx/wf4SLt/CnBFu38L8KvAe9rjv6b9MF3mOV8NvH/J/xhmw/UhA2V3L0nuP6H9yAWuAk4ZWLYP3Q/q71/h+e8Fnt7unwV8eGDZU4H/u0psPwN8adLvt7f5u81Yjj+a7gfD97fHZwMXrLL+0pz9+Br+x1eBn5z0++Zt+m6bybVl1n0h8L8GHu8AfmUv//9G4Lh2/2TgCwPLjmz/f8tA2d3AM9r9C3nox/S/X8zz9vhRwN+x9x/TX1xSv0e2/7l1yK/znwKvave3A/8XeER7/Oj2P398YP0beOgH9lk8vOH60IF1P0n7ob7C//b7YUZv5m//83eZ53o1e/7WeGRb/ybgz4CsEoe/y2fwZh6PJo/pGq5/a+DxG4GrBh7/U+DGdv944L8v2f4PgNeu8NxvBt7U7m/F/bK3VW7m+Mhy/I+Bf9/uH07XkP09K6y7XM7+4F6ef9Vj9j7eHCqkf+6l+6G4mtsG7v813Vkn6Lpp/OMk30d3lum9wLNaV6HH0CU2SZ6S5APpBpH/JvA7dGeHRuXOgfv/d5nHB7T73w/8Xuva8Q3gHrorQg5pcZ+WrnvjfW35Y5bE/TcD9/8P8N1ZeTykR9NdGSqN28zkeFXdD3wQOKEVnUA3vh0tjr3l7GA9V2KuaqM2nGtJnpjkPUm+2nLoj3l4Du3x+U1yYpIbB/ZhRyzZZum+j6paaX846EmD/6uq/pbuh/fe7AIOHHi8eP/+NWy7oiQ/m+Ta1uX4G3Q9nQbreXdVPdDuL455uJZ6Llq6L19tXb8fZpf52/P83dtvjar6Dl3DwBHAG6sdLa/AXJ5N5vEI8rhZz/H1jy++Ju11eRnwfQBJfjzJx5J8Lcl9wL/g4a+z+2WtxBwfTY6/C3hpu//PgT+tqv8Da87ZvR1jT12e2nDdP58BnrKXdQ4buP8PgNsBqupWup3JK+muZLyfbkdzKvCJqvr7ts076CZBObyqDgReQ9dAPGm3Ab9eVY8duO1fVf8z3di4pwMvAR5XVY8F7mPjcf8IXdcOadxmLcffDbw0yU/QdSP8GMAac3a1g1iSPAn4LuDzww9bc2DDuUY33FTRdXc9kG7Ym6U59ODnN918DH8I/AZd98DHAp9dZpuNuGMwziTfQzcEz97cTNetd9HTgTurai0/xJeVbtzKP6GbkXxLq+eVTOA3hN8PM8/87X/+rvpbI8khwGuB/wq8MauPe+vv8tlkHg85jzfgNuAvlhxfH1BVL2/L30U3WdthVfUYuiE+N/SauV+eS+b4aHL8z4EnJHkGXQP2uwaWrSVnVz3GZgr3uTZc98+VdGPOrebfJHlcuglPXkV31eWiv6BL5sVB8heWPIbuDMs3gV3pJkl5OXu6k25G1GWlmxDirL3EuBH/BTgzydPa/3lMkhe3ZY+mG2foa8C+Sf49e57dWq9/Qjc0iTRus5bjV9JdzfE64L0DjefDyNntwEer6tvr3E6CzeXao+muovhGa3z5N3t5nkfR/Uj8GnQToNFdBTIMlwE/l+T/TTfx2etY2++3i4FTkjw1yePo5o24cLkV001cs2MNz/lddGP/fQ3YneRnmdzkLtvx+2GWmb/9z98Vf2skSYv3fLphzu6gm7RqJf4un03m8fDzeL0+ADwl3cRzj2y3f5TkR9ryRwP3VNW3khxNd3XnRm3H/fK8McdHkONVtbvF9J/pxge/emDxpnK2nUQ+aslz9p4N1/1zMfD8JPuvss7ldGPG3UjXTf/8gWV/Qfdh/vgKjwH+Nd0H/H66s1aDjWLQjTl5UeuC8RIe7jDgf6yhLutSVe8H3gC8p3UX+SzdTLUAH6L7Qfu/6bqYfIu1DTPwMEkOphsD+083GbK0ETOV4+3H6fvoJoQZPBs8jJx9Gd0JLWkjNpNr/4FuAqP7Wvn7VvtHVfU5ujEmr6E7MXQkQ9pPVtXNwCvo8usOum6ZO9ew3Z8B/4muF8Rft9trV1h9TTnfenm8Eri0xfHP6a76mAS/H2ab+dv//F3tt8YrgS3A/7cNEfLLwC+33lh78Hf5TDOPh5zH69Xy/rl0w/ndTtdT8w10J7GgG3P4dUnupxvn99JN/Dv3y/PHHB9djr+L7vj6v7WG7EWbzdmfBxaq6va9rtkjWX24MU1Ckt8B7qqqNy+zrOi65N069sC6/38oXfL8xCT+/zAkeSPwxap6+6Rj0Xwyx9cUx5HAeZOOQ9Otz7nWJ0n+nG6CtlsmHcta+P0wH8zftZm2/F3K3+WzzTxemxnIY/fLc8ocX5u+5HiS64BTquqzk4xjvWy4njImvzTbzHFpPMw1aXqZv9L0M4+l2WaOa1gcKkSSJGkKJbkqya5lbq+ZdGySVmf+StPPPJZmmzneD15xLUmSJEnSGrQJtu4HHgB2V9W2JAfRjfO9FdgBvKSq7m3rn0k3SeUDwCur6kOt/Ci6ibz2p5vk7FVVVW3yrIvpJtC6Gzi+qna0bU6imwAM4Ler6qIRV1eSpInyimtJkiRJktbu2VX1jKra1h6fAXykqg4HPtIek+SpdBPjPQ04Fnh7kke0bd4BnAoc3m7HtvJTgHur6snAm+gm06M1jr8W+HHgaOC1SR430lpKkjRh+046gGF7whOeUFu3bn3w8d/+7d/yqEc9anIB9SAG///s/f8bbrjh61X1vUN90h5ZmsfLmfT7OqhPsUC/4jGWlZnH/XpP+hQLGM/e9CWeec/jvrwPazEtsRrncK0lziHl8XHA9nb/ImABOL2Vv6eqvg18OcmtwNHtqu0Dq+oagCQXAy8ErmrbnNWe6zLgbUkCPA+4uqruadtcTdfY/e7VApulPN4o6zj99la/9ebxNPWcmLbf1GA8e2M8D7daDs9cw/XWrVu5/vrrH3y8sLDA9u3bJxdQD2Lw/8/e/0/y10N9wp5ZmsfLmfT7OqhPsUC/4jGWlW0kj5M8Fvgj4AiggF8BPk8Pf2Sbx5tjPKvrSzzzvj/uy/uwFtMSq3EO11ri3EAeF/DnbeKxP6iq84AtVXUHQFXdkeSJbd1DgGsHtt3Zyr7T7i8tX9zmtvZcu5PcBzx+sHyZbZbW6VS6q7nZsmULv/u7v7tiZXbt2sUBBxywtzpPNes4/fZWv2c/+9kb2R8/u6q+PvB4sefEOUnOaI9PX9Jz4knAh5M8paoe4KGeE9fS/aY+lu4E1IM9J5KcQNdz4viBnhPb6L5LbkhyxeJv9+VM229qMJ69MZ6HW21fPHMN15IeLslhdI1R3wf8PXBeVf1ekrOAXwO+1lZ9TVVd2bZxPD6pf34P+LOqelGS7wK+B3gNPfyRLUnSjHpWVd3eGqevTvJXq6ybZcpqlfKNbrNnYdeYfh7Atm3barUGiT40WIyadZx+Y6pfb3tOSPPMhmtpPuwGTquqTyd5NF2j09Vt2Zuqao/LMGzwkvonyYHATwInA1TV3wF/l8Qf2ZIkjUlV3d7+3pXk/XTjTd+Z5OB2tfXBwF1t9Z3AYQObHwrc3soPXaZ8cJudSfYFHgPc08q3L9lmYXg1k+ZKr3tOLO01sbCwsGpldu3atdd1xsl4Vmc862PDtTQH2g54cSd8f5JbWKFrYWODl9Q/P0jXO+K/Jnk6cAPwKnr0I1uSpFmW5FHAPu339KOA5wKvA64ATgLOaX8vb5tcAbwrybl0F4McDnyyqh5Icn+SY4DrgBP//+z9fbxdZX3g/X++AsVUAQHlNBCmsQN0KlCxZJC5vadzKhVScQrtTyX+rISWKa1DR5wyUxLrDIwYJ06LWqEyQyUlWAQyqCUjIEbsua338GytEZEhyilEUlIJYtIODInf+491bbKys8/Oedp7r33O5/167dfZ+1rrWuu79tnXXmtf63oArqzlWQ7cDbwV+HLp3Xgn8KHahIynAyt7e8TSnNXonhNT6TUBzWtxbzzdGc/UWHEtzTMRsRh4HdVF8huA34mIc4EHqFplP4N3laekSbFAs+Ixllm1P/BzwL/JzHsj4o+ohgWZSN8vsi3Hs8d4umtaPJLmjRHgc1XbDPYHPp2ZX4iI+4F1EXE+8DjwNoDMfCgi1gHfouoBeWHpwQjwbnYPv3dHeQBcC3yqNBzZRtULkszcFhGXA/eX9T7QahgiaWrsOSENDyuupXkkIl4OfAZ4b2b+MCKuBi6nqoC6HLiCarI37ypPQZNigWbFYyyzajOwOTPvLa9voaq4bsxFtuV49hhPd02LR9L8kJnfBV7bIf1p4LQJ8qwCVnVIf4BqsuX29OcoFd8dlq0B1kwtakl19pyQhstLBh2ApP6IiAOoKq1vyMzPAmTmU5m5KzN/BPwJ1Z1mmFmFFx0qvDptS9IUZObfAk9ExE+XpNOoWnC1Loxh74vsZRFxYES8mt0X2VuA7RFxahnO59y2PK1tvXiRDdwJnB4Rh5YL7dNLmiRJkjRMRoCvRsRfA/cBt2XmF6gqrN8UEY8CbyqvycyHgFbPiS+wd8+JTwKbgO+wZ8+Jw0vPid+l9JIsvSRaPSfux54T0j7Z4lqaB0rl1LXAw5n5kVr6wtbYuMCvAN8sz72rLDXTvwFuiIgfA74L/DrVTWi7J0uSJEn7YM8JabhYcT0LFq+4bY/X46vPHFAk0oTeALwL2BgRXy9p7wPeEREnUQ3dMQ78FgxHhdfG7z3LeaXsWeY0X2Tm14ElHRYN5UW25VgabvUyDJZjaRh5LpaGn+VYc5kV19I8kJlfpfNY07d3ydPoCi9JkiRJkiTNXVZcT1N7K2tJkiRJkiRJ0uxwckZJkiRJkiRJUqNYcS1JkiRJkiRJahSHCumBxU5SI0mSJEmSJEnTZotrSZIkSZIkSVKjWHEtSZIkSZIkSWoUK64lSZIkSZIkSY1ixbUkSZIkSZIkqVGsuJYkSZIkSZIkNYoV15IkSZIkSZKkRrHiWpIkSZIkSZLUKFZcS5IkSZIkSZIaxYrrHlu84jY2fu9ZFq+4bdChSJIkSZIkSdJQsOJakiRJkiRJktQoM664joj9IuKvIuLz5fVhEbEhIh4tfw+trbsyIjZFxCMRcUYt/eSI2FiWfTwioqQfGBE3l/R7I2LxTOOVJEmSJEmSJDXbbLS4vgh4uPZ6BXBXZh4L3FVeExGvAZYBxwNLgU9ExH4lz9XABcCx5bG0pJ8PPJOZxwAfBT48C/FKkiRJAzGoRh8Rsbzs49GIWN7HQ5YkSZKmZf+ZZI6IRcCZwCrgd0vyWcBoeb4WGAMuKek3ZebzwGMRsQk4JSLGgYMz8+6yzeuBs4E7Sp7LyrZuAa6KiMjMnEnc0zEbY1S3b2N89Zkz3qYkSZKGSqvRx8HldavRx+qIWFFeX9LW6ONI4EsRcVxm7mJ3o497gNupGn3cQa3RR0Qso2r0cU5EHAZcCiwBEngwItZn5jP9OWRJkiRp6mba4vpjwO8BP6qljWTmFoDy94iSfhTwRG29zSXtqPK8PX2PPJm5E3gWOHyGMUuSJEl9V2v08cla8llUjT0of8+upd+Umc9n5mNAq9HHQkqjj9KY4/q2PK1t3QKcVlpjnwFsyMxtpbJ6A7t7OEqSJEmNNO0W1xHxFmBrZj4YEaOTydIhLbukd8vTHssFVK1OGBkZYWxs7MVlO3bs2OP1dF184s5p5x1Z0Dn/bMQ1GbP1Hrj/4dy/JElqjI9RNfo4qJa2R6OPiKg3+rintl6rcccLTLLRR0S0Gn1M1IBkL92uq9u1X+M2+XpnWK7HjHN2DUuckiSps5kMFfIG4Jcj4s3AS4GDI+LPgKciYmG58F4IbC3rbwaOruVfBDxZ0hd1SK/n2RwR+wOHANvaA8nMa4BrAJYsWZKjo6MvLhsbG6P+errOm8FQIRefuJMrNu79Vo+/c3QGEU3ebL0H7n849y9JkgZvwI0+JtUYBLpfV7e78oZb97jG7de17XQMy/WYcc6uYYlTkiR1Nu2hQjJzZWYuyszFVOPvfTkzfw1YD7QmfFkOzMX7FwAAp35JREFU3FqerweWlUljXk01CeN9pYXJ9og4tXRlPLctT2tbby376Pv41pIkNUFEjJcJ2b4eEQ+UNCd2k4ZDq9HHOHAT8MZ6ow+AWWz0QVujj4m2JUmSJDXWTMe47mQ18KaIeBR4U3lNZj4ErAO+BXwBuLBMLgPwbqqx/jYB36GaXAbgWuDwMpHj71JNViNJ0nz2C5l5UmYuKa9bE7sdC9xVXtM2sdtS4BMRsV/J05rY7djyaI11++LEbsBHqSZ2ozax2+uBU4BL6xXkkvZtwI0+7gROj4hDS9k9vaRJkiRJjTWToUJelJljwFh5/jRw2gTrrQJWdUh/ADihQ/pzwNtmI8YmWlwbfmR89ZkDjESSNMTOAkbL87VU5+NLqE3sBjxWbgKfUlp7HpyZdwNERGtitztKnsvKtm4Brmqf2K3kaU3sdmNvD02aF1YD6yLifOBxyrVvZj4UEa1GHzvZu9HHdcACqrJbb/TxqVLet1FVkJOZ2yLicuD+st4HWuVZkiRJaqpetLiWJEm9kcAXI+LBMoEatE3sBtQndus0GdtRTHJiN2DKE7tJ2rfMHMvMt5TnT2fmaZl5bPm7rbbeqsz8x5n505l5Ry39gcw8oSz7ndZQepn5XGa+LTOPycxTMvO7tTxrSvoxmfmn/Txeaa6JiP0i4q8i4vPltcN2SZLUA7PS4lqSJPXFGzLzyYg4AtgQEd/usm7fJ3YrlekXAIyMjDA2NtYlPBhZUE1gDOxz3V7bsWPHwGOoM57umhaPpHnnIuBh4ODyujVs1+qIWFFeX9I2bNeRwJci4rjSe6I1bNc9wO1UPZnuoDZsV0Qsoxq265zasF1LqM7BD0bE+sx8pj+HLM0tZQi9B4DvZeZbShm7GVgMjANvb5WviFhJVTZ3Ae/JzDtL+sns7gF1O3BRZmZEHAhcD5wMPA2ck5njJc9y4P0ljA9m5tqeH6w0xKy4liRpSGTmk+Xv1oj4HNV4009FxMLM3DKLE7tt7jCx22hbnrEO8V0DXAOwZMmSHB0dbV9lD1fecCtXbKwuRcbf2X3dXhsbG2Nf8faT8XTXtHgkzR8RsQg4k2oIzN8tyQ7bJQ0fb0BJQ8CKa2keiIijqe74/gTwI+CazPwj7ypLwyMiXga8JDO3l+enAx9g92Rsq9l7YrdPR8RHqC6yWxO77YqI7RFxKnAv1cRuV9byLAfupjaxW0TcCXyo1vX5dGBlb49YkqRG+hjwe8BBtbQ9hu0qPaOgGlbrntp6raG2XmCSw3ZFxJSH7ZpKD6gm9X7qlfnQS2euH+NsH583oKThYcW1ND/sBC7OzK9FxEFUd3Y3AOfhXWVpWIwAnytDYO4PfDozvxAR9+PEbpIk9VxEvAXYmpkPRsToZLJ0SOvpsF0wtR5QTer91CvzoZfOXD/GHhzfx2j4DShJFSuupXmgnIBbJ+HtEfEw1QnSu8rSkCiTrL22Q/rTwGkT5FlF1ZKkPf0B4IQO6c9RKr47LFsDrJla1JIkzSlvAH45It4MvBQ4OCL+jAYN2yWpu2G4ATXM88ZA83oAGE93TYunnRXX0jxTZiZ/HdUQAd5VliRJkiYhM1dShsoqFV7/LjN/LSL+AIftkoZF429ADfO8MdC8HgDG013T4mlnxbU0j0TEy4HPAO/NzB+WIQc6rtohzbvKE2jaHcomxWMskiRpHliNw3ZJQ8EbUNJwseJamici4gCqSusbMvOzJdm7yrOgaXcomxSPsUiSpLkoM8co17QO2yXNCd6AkhrIimtpHihjTV8LPJyZH6ktat0J9q6yJEmSJGne8AaU1HxWXDfE4hW3vfh8fPWZA4xEc9QbgHcBGyPi6yXtfXhXWZIkSZIkSQ1kxbU0D2TmV+k81jR4V1mSJEmSJEkN85JBByBJkiRJkiRJUp0V15IkSZIkSZKkRrHiWpIkSZIkSZLUKFZcS5IkSZIkSZIaxYprSZIkSZIkSVKjWHEtSZIkSZIkSWoUK64lSZIkSZIkSY1ixbUkSZIkSZIkqVGsuJYkSZIkSZIkNYoV15IkSZIkSZKkRrHiWpIkSZIkSZLUKPsPOgDtbfGK2/Z4Pb76zAFFIkmSJEmSJEn9Z4trSZIkSZIkSVKjWHEtSZIkSZIkSWoUK64lSRoSEbFfRPxVRHy+vD4sIjZExKPl76G1dVdGxKaIeCQizqilnxwRG8uyj0dElPQDI+Lmkn5vRCyu5Vle9vFoRCzv4yFLkiRJkuYpK64lSRoeFwEP116vAO7KzGOBu8prIuI1wDLgeGAp8ImI2K/kuRq4ADi2PJaW9POBZzLzGOCjwIfLtg4DLgVeD5wCXFqvIJckSZIkqResuJYkaQhExCLgTOCTteSzgLXl+Vrg7Fr6TZn5fGY+BmwCTomIhcDBmXl3ZiZwfVue1rZuAU4rrbHPADZk5rbMfAbYwO7KbkmSJEmSesKKa0mShsPHgN8DflRLG8nMLQDl7xEl/Sjgidp6m0vaUeV5e/oeeTJzJ/AscHiXbUmSJEmS1DP7TzdjRLwU+ApwYNnOLZl5aelSfDOwGBgH3l5aaBERK6m6Iu8C3pOZd5b0k4HrgAXA7cBFmZkRcSBVa7CTgaeBczJzfLoxS5I0jCLiLcDWzHwwIkYnk6VDWnZJn26ePXcacQHVMCSMjIwwNjbWNciRBXDxiTsB9rlur+3YsWPgMdQZT3dNi2cyBn3tXManf38J54OZ2ephIUmSJDXStCuugeeBN2bmjog4APhqRNwB/CrVeJurI2IF1Xibl7SNt3kk8KWIOC4zd7F7vM17qC6+lwJ3UBtvMyKWUY23ec4MYp6SxStu69euJEnq5g3AL0fEm4GXAgdHxJ8BT0XEwszcUoYB2VrW3wwcXcu/CHiypC/qkF7Pszki9gcOAbaV9NG2PGOdgszMa4BrAJYsWZKjo6OdVnvRlTfcyhUbq0uR8Xd2X7fXxsbG2Fe8/WQ83TUtnkka2LVzbaz6JVQ3nh6MiPWtCnJJkiSpiaY9VEhWdpSXB5RH0p/xNiVJmjcyc2VmLsrMxVQVWV/OzF8D1gPLy2rLgVvL8/XAsog4MCJeTTUJ431lOJHtEXFqOZ+e25anta23ln0kcCdwekQcWiZlPL2kSZqCAV87O1a9JEmShs5MWlwTEfsBDwLHAH+cmfdGxB7jbUZEfbzNe2rZW2NkvsAkx9uMiNZ4m99vi2PCrskz6Ura6r48U/Wu0NMx066wg+5O6/6HrzuzpKGxGlgXEecDjwNvA8jMhyJiHfAtYCdwYWmlCfBudg8xcEd5AFwLfCoiNlG1tF5WtrUtIi4H7i/rfSAzt/X6wKS5aIDXzpMeq34qQ/60X+M2+XpnWK7HjHN2DUuckiSpsxlVXJcfwSdFxCuAz0XECV1Wn83xNtvjmLBr8ky6kp43S0OFXHzizhe7Qk/HTLtPD7o7rfsfyu7MkhoqM8coQ3Vk5tPAaROstwpY1SH9AWCv83VmPkep+O6wbA2wZroxS6oM8Np50mPVT2XIn/pwPzD4IX+6GZbrMeOcXcMSpyRJ6mxGFdctmfmDiBij6nLYj/E255X6WNvjq88cYCSSJEmaqQFcO096rHpJkiSpKaY9xnVEvKq0FiEiFgC/CHyb/oy3KUmSJA2NAV87O1a9NAsi4qURcV9E/HVEPBQR/6mkHxYRGyLi0fL30FqelRGxKSIeiYgzauknR8TGsuzjrbmcSpm/uaTfGxGLa3mWl308GhHLkSRpjpt2xTWwEPiLiPgG1biXGzLz81Tjbb4pIh4F3lRek5kPAa3xNr/A3uNtfpJq0pnvsOd4m4eX8TZ/l2qWdUmSJGnYDOzauYxL3xqr/n4cq16arueBN2bma4GTgKURcSpVWbsrM48F7iqviYjXUM0ZcTxVD4tPlLHuAa6mGk/+2PJoTZh6PvBMZh4DfBT4cNnWYcClwOuBU4BL6xXkkibHG1DScJn2UCGZ+Q3gdR3S+zLepqTJi4g1wFuArZl5Qkm7DPhN4O/Kau/LzNvLspVUF827gPdk5p0l/WR2T+p2O3BRZmZEHAhcD5wMPA2ck5njJc9y4P1lHx/MzLU9PVhJkhpo0NfOjlUvzVzpwbCjvDygPBI4i93D8aylGornkpJ+U2Y+DzxWbiqdEhHjwMGZeTdARFwPnE11E+os4LKyrVuAq0pl2BlUN7y2lTwbqCq7b+zJwUpzV+sG1I6IOAD4akTcAfwq1Q2o1RGxguoG1CVtN6COBL4UEceVm8mtG1D3UP0+XkpVjl+8ARURy6huQJ1TuwG1hOq748GIWJ+Zz/Tv8KXhMitjXEtqvOuAq6gql+s+mpl/WE/wxCxJkiR1VlpMPwgcA/xxZt4bESNlGB/KePVHlNWPorpubtlc0l4oz9vTW3meKNvaGRHPAofX0zvkaY/xAqprdkZGRhgbG5vweEYWwMUn7gTout4w27Fjx5w9tpa5foyzeXzegJKGixXX0jyQmV+pd0/aB0/MkiRJUgelMcdJZcz6z0XEXr0faqLTJrqkTzdPe4zXANcALFmyJEdHRycM8MobbuWKjVW1wPg7J15vmI2NjdHtPZgL5voxzvbxDcMNKEkVK66l+e13IuJc4AHg4tIS2hOzJEmS1EVm/iAixqgaZTwVEQtLZddCYGtZbTNwdC3bIuDJkr6oQ3o9z+aI2B84BNhW0kfb8ozN4iFJ80bTb0BNpdcENK/nRNN6ABhPd02Lp50V19L8dTXVRE1Z/l4B/AYDahkyzCfnpn3RNykeY5EkSXNFRLwKeKFUWi8AfpFqiLz1wHKqyVWXA7eWLOuBT0fER6iG4DsWuC8zd0XE9jKx473AucCVtTzLgbuBtwJfLnPK3Al8qDZh3OnAyt4esTS3NfUG1FR6TUDzek40rQeA8XTXtHjaWXEtzVOZ+VTreUT8CfD58nIgLUOG+eTctC/6JsVjLJIkaQ5ZCKwtwwy8BFiXmZ+PiLuBdRFxPvA4ZZLUzHwoItYB3wJ2AheWlp4A72b3pOd3lAfAtcCnynB926jmniEzt0XE5cD9Zb0PtIbjkzR53oCShosV19I81bqbXF7+CvDN8twTsyRJktQmM78BvK5D+tPAaRPkWQWs6pD+ALDX8ASZ+Ryl4rvDsjXAmqlFLamNN6CkIWLFtTQPRMSNVC2fXxkRm4FLgdGIOIlq6I5x4LfAE7MkSZIkaW7yBpQ0XKy4luaBzHxHh+Rru6zviVmSJEmSJEkD85JBByBJkiRJkiRJUp0V15IkSZIkSZKkRnGokCGzeMVte7weX33mgCKRJEmSJEmSpN6wxbUkSZIkSZIkqVGsuJYkSZIkSZIkNYoV15IkSZIkSZKkRnGMa0lDrz72u+O+S5IkSZIkDT9bXEuSJEmSJEmSGsWKa0mSJEmSJElSo1hxLUnSEIiIl0bEfRHx1xHxUET8p5J+WERsiIhHy99Da3lWRsSmiHgkIs6opZ8cERvLso9HRJT0AyPi5pJ+b0QsruVZXvbxaEQs7+OhS5IkSZLmISuuJUkaDs8Db8zM1wInAUsj4lRgBXBXZh4L3FVeExGvAZYBxwNLgU9ExH5lW1cDFwDHlsfSkn4+8ExmHgN8FPhw2dZhwKXA64FTgEvrFeSSJEmSJM02K64lSRoCWdlRXh5QHgmcBawt6WuBs8vzs4CbMvP5zHwM2AScEhELgYMz8+7MTOD6tjytbd0CnFZaY58BbMjMbZn5DLCB3ZXdkiRJkiTNuv0HHYAkSZqc0mL6QeAY4I8z896IGMnMLQCZuSUijiirHwXcU8u+uaS9UJ63p7fyPFG2tTMingUOr6d3yFOP7wKqltyMjIwwNjbW9XhGFsDFJ+4E2Oe6vbZjx46Bx1BnPN01LR5JkiRJs8+Ka0mShkRm7gJOiohXAJ+LiBO6rB6dNtElfbp56vFdA1wDsGTJkhwdHe0SHlx5w61csbG6FBl/Z/d1e21sbIx9xdtPxtNd0+KRJEmSNPscKkSSpCGTmT8AxqiG63iqDP9B+bu1rLYZOLqWbRHwZElf1CF9jzwRsT9wCLCty7YkSZIkSeoJK64lSRoCEfGq0tKaiFgA/CLwbWA9sLysthy4tTxfDyyLiAMj4tVUkzDeV4YV2R4Rp5bxq89ty9Pa1luBL5dxsO8ETo+IQ8ukjKeXNEmSJEmSesKhQiRJGg4LgbVlnOuXAOsy8/MRcTewLiLOBx4H3gaQmQ9FxDrgW8BO4MIy1AjAu4HrgAXAHeUBcC3wqYjYRNXSelnZ1raIuBy4v6z3gczc1tOjlSRJkiTNa1ZcS5I0BDLzG8DrOqQ/DZw2QZ5VwKoO6Q8Ae42PnZnPUSq+OyxbA6yZWtSSJEmSJE2PFddDbvGK2158Pr76zAFGIkmSJEmSJEmzwzGuJUmSJEmSJEmNYsW1JEmSJEmSJKlRpl1xHRFHR8RfRMTDEfFQRFxU0g+LiA0R8Wj5e2gtz8qI2BQRj0TEGbX0kyNiY1n28YiIkn5gRNxc0u+NiMUzOFZJkiRpIAZ97RwRy8s+Ho2I5X08dEmSJGlaZtLieidwcWb+DHAqcGFEvAZYAdyVmccCd5XXlGXLgOOBpcAnImK/sq2rgQuAY8tjaUk/H3gmM48BPgp8eAbxSpIkSYMysGvniDgMuBR4PXAKcGm9glySJElqomlXXGfmlsz8Wnm+HXgYOAo4C1hbVlsLnF2enwXclJnPZ+ZjwCbglIhYCBycmXdnZgLXt+VpbesW4LRWixJJkiRpWAz42vkMYENmbsvMZ4AN7K7sliRJkhpp/9nYSOmG+DrgXmAkM7dAdYEeEUeU1Y4C7qll21zSXijP29NbeZ4o29oZEc8ChwPfb9v/BVStThgZGWFsbOzFZTt27Njj9VRcfOLOaeVrN7Jg9rbVzUTHOZP3YDa4/8HuX5IkNcsArp1fTO+QR5IkSWqkGVdcR8TLgc8A783MH3ZpEN1pQXZJ75Znz4TMa4BrAJYsWZKjo6MvLhsbG6P+eirOW3HbtPK1u/jEnVyxcVbuEXQ1/s7RjukzeQ9mg/sf7P4lSVJzDOjaeVLX1CW+CRuEtGtvnNHkG/XD0pDAOGfXbMcZEUdT9XL4CeBHwDWZ+UdlOJ6bgcXAOPD20ruBiFhJNYzPLuA9mXlnST8ZuA5YANwOXJSZGREHln2cDDwNnJOZ4yXPcuD9JZwPZmarh4UkSXPSjGpTI+IAqgvvGzLzsyX5qYhYWFqMLAS2lvTNwNG17IuAJ0v6og7p9TybI2J/4BBg20xiliRJkgZhgNfOm4HRtjxjnWLs1iCk3ZU33LpH44yJGlE0wbA0JDDO2dWDOFtj1X8tIg4CHoyIDcB5VGPVr46IFVRj1V/SNlb9kcCXIuK4zNzF7rHq76GquF4K3EFtrPqIWEY1Vv05tbHql1DdeHowIta3KsglTY43oKThMu0xrst4edcCD2fmR2qL1gOtmcqXA7fW0peV2c5fTTWRzH2la+T2iDi1bPPctjytbb0V+HIZy0/SFETEmojYGhHfrKUdFhEbIuLR8vfQ2rKVEbEpIh6JiDNq6SdHxMay7OOtMedLub65pN9bukC38iwv+3i0nKQlSZp3BnztfCdwekQcWs73p5c0SVPgWPXSnOBkydIQmXbFNfAG4F3AGyPi6+XxZmA18KaIeBR4U3lNZj4ErAO+BXwBuLDcaQZ4N/BJqhP5d6juNEN1cX94RGwCfpfyxSFpyq5j7wtbT8ySJPXPwK6dM3MbcDlwf3l8oKRJmqZuY9UD9bHqO40vfxSTHKsecKx6aRZ5A0oaLtMeKiQzv0rn8fIATpsgzypgVYf0B4ATOqQ/B7xtujFKqmTmV+qtoIuz2N1teC1Vl+FLqJ2YgcfKj99TImKccmIGiIjWifmOkueysq1bgKvaT8wlT+vEfONsH6MkSU026GvnzFwDrJlsvJImNlfHqh+GccunY1jGZJ+JuX6MvTq+pk6WPJUyDM0rx037PBpPd02Lp13vZwyU1FSNOTHDzE7OdYP4wm3aF32T4jEWSZI0l8zlseqbPE79TAzLmOwzMdePsRfH1+QbUFMpw9C8cty0z6PxdNe0eNpZcS2p3UBahszk5Fw3iBN1077omxSPsUiSpLliEmPVr2bvseo/HREfoZqcsTVW/a6I2B4Rp1K19DwXuLJtW3dTG6s+Iu4EPlQbdu90YGWPDlWa04bhBpSkykzGuJY03J4qJ2Rm8cRMhxNzp21JkiRJw8ax6qUh52TJ0nCxxfUcsnjFbS8+H1995gAj0ZCwZYgkSZI0SY5VL80JrRtQGyPi6yXtfVS/i9dFxPnA45RymJkPRUTrBtRO9r4BdR2wgOrmU/0G1KfKDahtwLKyrW0R0boBBd6AkvbJimtpHoiIG6m6JL0yIjYDl+KJWZIkSZI0j3gDShouVly3qbdaluaKzHzHBIs8MUuSJEmSJKlxHONakiRJkiRJktQoVlxLkiRJkiRJkhrFimtJkoZARBwdEX8REQ9HxEMRcVFJPywiNkTEo+XvobU8KyNiU0Q8EhFn1NJPjoiNZdnHy0zolNnSby7p90bE4lqe5WUfj0bEciRJkiRJ6iErriVJGg47gYsz82eAU4ELI+I1wArgrsw8FrirvKYsWwYcDywFPhER+5VtXQ1cABxbHktL+vnAM5l5DPBR4MNlW4dRTer6euAU4NJ6BbkkSZIkSbPNimtJkoZAZm7JzK+V59uBh4GjgLOAtWW1tcDZ5flZwE2Z+XxmPgZsAk6JiIXAwZl5d2YmcH1bnta2bgFOK62xzwA2ZOa2zHwG2MDuym5JkiRJkmadFdeSJA2ZMoTH64B7gZHM3AJV5TZwRFntKOCJWrbNJe2o8rw9fY88mbkTeBY4vMu2JEmSJEnqif0HHYAkSZq8iHg58BngvZn5wzI8dcdVO6Rll/Tp5qnHdgHVECSMjIwwNjY2UWwAjCyAi0/cCbDPdXttx44dA4+hzni6a1o8kiRJkmafFdeSJA2JiDiAqtL6hsz8bEl+KiIWZuaWMgzI1pK+GTi6ln0R8GRJX9QhvZ5nc0TsDxwCbCvpo215xtrjy8xrgGsAlixZkqOjo+2r7OHKG27lio3Vpcj4O7uv22tjY2PsK95+Mp7umhaPJEmSpNnnUCFz1OIVt7342Pi9ZwcdjiRphspY09cCD2fmR2qL1gPLy/PlwK219GURcWBEvJpqEsb7ynAi2yPi1LLNc9vytLb1VuDLZRzsO4HTI+LQMinj6SVNkiRJkqSesMW1JEnD4Q3Au4CNEfH1kvY+YDWwLiLOBx4H3gaQmQ9FxDrgW8BO4MLM3FXyvRu4DlgA3FEeUFWMfyoiNlG1tF5WtrUtIi4H7i/rfSAzt/XoOCVJkiRJsuJakqRhkJlfpfNY0wCnTZBnFbCqQ/oDwAkd0p+jVHx3WLYGWDPZeCVJkiRJmgmHCpEkSZIkSZIkNYoV15IkSZIkSZKkRrHiWpIkSZIkSZLUKFZcS5IkSZIkSZIaxYprSZIkSZIkSVKjWHEtSZIkSZIkSWqU/QcdgCTNpsUrbtvj9fjqMwcUiSRJkiRJkqbLFteSJEmSJEmSpEax4lqSJEmSJEmS1CgOFTJP1IdPcOgESZIkSZIkSU1mi2tJkiRJkiRJUqNYcS1JkiRJkiRJapQZVVxHxJqI2BoR36ylHRYRGyLi0fL30NqylRGxKSIeiYgzauknR8TGsuzjEREl/cCIuLmk3xsRi2cSryRJkjQog7x2jojlZR+PRsTyPh2yJEmSNG0zbXF9HbC0LW0FcFdmHgvcVV4TEa8BlgHHlzyfiIj9Sp6rgQuAY8ujtc3zgWcy8xjgo8CHZxivJEmSNCjXMYBr54g4DLgUeD1wCnBpvYJckiRJaqIZVVxn5leAbW3JZwFry/O1wNm19Jsy8/nMfAzYBJwSEQuBgzPz7sxM4Pq2PK1t3QKc1mpRIkmSJA2TAV47nwFsyMxtmfkMsIG9K9AlTYI9J6ThZhmWhksvxrgeycwtAOXvESX9KOCJ2nqbS9pR5Xl7+h55MnMn8CxweA9iluatiBgvJ9yvR8QDJc0hfyRJ6o9+XDtPtC1JU3cd9pyQhtl1WIalobF/H/fVqaV0dknvlmfPDUdcQPWFwcjICGNjYy8u27Fjxx6v9+XiE3dOet3JGlnQm+1Od/9TeT9mw1T/B+5/IH4hM79fe906ca+OiBXl9SVtJ+4jgS9FxHGZuYvdJ+57gNupTtx3UDtxR8QyqhP3Of06MEmShtRsXjtP6poaul9Xtxv0NeZUDMn1mHHOsl7EmZlf6dAQ4yxgtDxfC4wBl1DrOQE8FhGtnhPjlJ4TABHR6jlxR8lzWdnWLcBV7T0nSp5Wz4kbZ/UApTnOMiwNl15UXD8VEQszc0vpyri1pG8Gjq6ttwh4sqQv6pBez7M5IvYHDmHv7pVk5jXANQBLlizJ0dHRF5eNjY1Rf70v5624bdLrTtbFJ+7kio39vEfQff/j7xzt6/6n+j9w/43Q8xN36d4sSdJ8149r583sPq+38ox1CqbbdXW7K2+4daDXmFMxLNdjxjm7+hjnHj0nIqLec+Ke2nqt3g4vMMmeExFhzwmp9yzDUkP1ojZ1PbAcWF3+3lpL/3REfISqpeaxwH2ZuSsitkfEqcC9wLnAlW3buht4K/BlK7tmbnFb5fz46jMHFIkaIoEvRkQC/638YO3HibvewntKLbxg8j0Z+tEaqGmtjpoUj7FI0j71/No5Iu4EPlTrknw6sLL3hybNe0PVc2KuXifNh2vAuX6MAzy+xpdhaF45btrn0Xi6a1o87WZUcR0RN1K13nhlRGymGq9nNbAuIs4HHgfeBpCZD0XEOuBbwE7gwjK8AMC7qcYZWkDVQvOOkn4t8KnSqnMb1RAFkmbXGzLzyVI5vSEivt1l3Z4N+TOVFl6wdyuvifSj9VfTWh01KR5jkaTdBnXtnJnbIuJy4P6y3gdaXZUlzYo50XOiyb0mZmI+XAPO9WPsw/ENbRmG5pXjpn0ejae7psXTbkYV15n5jgkWnTbB+quAVR3SHwBO6JD+HOXiXVJvZOaT5e/WiPgc1UQRfR/yR5KkuW6Q186ZuQZYM+lgJU2FPSek4WYZlhrqJYMOQNLgRMTLIuKg1nOqk+c32X2yhb1P3Msi4sCIeDW7T9xbgO0RcWqZeOLctjytbTnkjzRNEbEmIrZGxDdraYdFxIaIeLT8PbS2bGVEbIqIRyLijFr6yRGxsSz7eCmzlHJ9c0m/tz5pTUQsL/t4NCJa5VmSpHmn9Jy4G/jpiNhcekusBt4UEY8CbyqvycyHgFbPiS+wd8+JTwKbgO+wZ8+Jw0vPid+lmiSd0kui1XPifuw5IU2LZVgaLoObMVBSE4wAnyv1VvsDn87ML0TE/Tjkj9Q01wFXAdfX0lYAd2Xm6ohYUV5fEhGvoSprx1O1DvlSRBxXyuvVVGPm3QPcTjWb+R3A+cAzmXlMRCwDPgycExGHUQ1nsIRqmJ8HI2J9Zj7T8yOWJKlh7DkhDTfLsDRcrLiW5rHM/C7w2g7pT+OQP1KjZOZX6q2gi7PYPVbeWqpx8i4p6Tdl5vPAY+XG0SkRMQ4cnJl3A0TE9cDZVBXXZwGXlW3dAlxVWmOfAWxotQiJiA1Uld03zvYxSpIkSZLUYsW1JEnDa6QM1UMZk/6Ikn4UVYvqls0l7YXyvD29leeJsq2dEfEscHg9vUOePQzzDOhNm03beLprWjySJEmSZp8V15IkzT3RIS27pE83z56JQzwDetNm0zae7poWjyRJkqTZZ8W1WLzithefj68+c4CRSLPPz7fmuKciYmFpbb0Q2FrSNwNH19ZbBDxZ0hd1SK/n2RwR+wOHUI1Lv5ndw5G08ozN7mFIkiRJkrSnlww6AEmSNG3rgeXl+XLg1lr6sog4MCJeDRwL3FeGFdkeEaeW8avPbcvT2tZbgS9nZgJ3AqdHxKERcShwekmTJEmSJKlnbHEtSdIQiIgbqVo+vzIiNgOXAquBdRFxPvA4ZSLUzHwoItYB3wJ2Ahdm5q6yqXcD1wELqCZlvKOkXwt8qkzkuA1YVra1LSIuB+4v632gNVGjJEmSJEm9YsW1JElDIDPfMcGi0yZYfxWwqkP6A8AJHdKfo1R8d1i2Blgz6WAlSZIkSZqheV9xXR//VpIkSZIkSZI0eI5xLUmSJEmSJElqlHnf4lp7qrdAH1995gAjkSRJkiRJkjRf2eJakiRJkiRJktQoVlxLkiRJkiRJkhrFoUIkzRvtk7E6HI4kSZIkSVIz2eJakiRJkiRJktQotrjWhGydKkmSJEmSJGkQbHEtSZIkSZIkSWoUK64lSZIkSZIkSY3iUCGatPrQIQ4bIkmSJEmSJKlXrLiWNG95M0aSJEmSJKmZHCpEkiRJkiRJktQotrjWtNhSVZIkSZIkSVKv2OJakiRJkiRJktQotrjWjNVbX4MtsDWc7EUgSZIkSZLUHFZca9ZZAShJkiRJw8UGSZKkprHiWpLaeNEuSZKk+a79mrjO62NJUj84xrV6avGK29j4vWdZvOK2rhc+kiRJkiRJktRii2v1lcOIaBj5uZUkSZIkNZ29hzXXDEXFdUQsBf4I2A/4ZGauHnBImgWTbYHtF+3cMFfKsRcCmq/mShmW5jPLsTT8mlKObdghTV8/y7FlVcOu8RXXEbEf8MfAm4DNwP0RsT4zvzXdbTpkxXCxonD49aIcN8XiFbdx8Yk7Oc/PqeawuVyGpfnCciwNv6aWY8fCliZvkOXYxoMaRo2vuAZOATZl5ncBIuIm4CzAi+x5aqIvW79cG23elWMv4DXHzLsyLM1BPS/HE7Xq6tYIYbLny7m6jfZrgo3fe3avm+FT3cZsxDHZ621b8vXd0J2PrSiT9tL4cjzTxp6dGnbVWd41FcNQcX0U8ETt9Wbg9QOKRQ020Zfrvr40Z4NfvPtkOa7pda+PyX7m/dxqCizD0vDraznudq6b7HlwNrdx8Yk7af/pM4g49pV+8Ykz38ZsxLGv/J2uNWbj+sZrk32as+fjXl4ft39e/ZxpwOZsOZ6sfo+C0I86IfC7pVeGoeI6OqTlHitEXABcUF7uiIhHaotfCXy/R7FNynsGHIP77/3+48NdF/di/z85y9vrtZmW404GXrZbBv0ZbzfZePbxuZ0tTXpvmhQLwE8POoAp2GcZhpmV4z59Hrtp2ufDeLprSjzDdD7uRTluyv9hn5p2rp7IfI+zB+eCycRpOR6Cz9xMtH9eG3DN0Qtz/f+4r+ObU+V4mH8bQ/POZf2KZwrfLY16f2hGPBOW4WGouN4MHF17vQh4sr5CZl4DXNMpc0Q8kJlLehfevg06Bvc/v/ffEDMqx5006X1tUizQrHiMZWIR8cCgY5iCfZZhsBzPJuPprmnxDIlZL8fD9H8YlliNc3YNS5xTMK/L8XR5jMNvjh3fnP5tDMazL8YzNS8ZdACTcD9wbES8OiJ+DFgGrB9wTJKmxnIsDTfLsDT8LMfS8LMcS8PPcixNQeNbXGfmzoj4HeBOYD9gTWY+NOCwJE2B5VgabpZhafhZjqXhZzmWhp/lWJqaxldcA2Tm7cDt08w+6e4VPTToGNz//N5/I8ywHHfSpPe1SbFAs+Ixlok1LZ6uelCGoVnvQZNiAePZl6bFMxTm+Ll4X4YlVuOcXcMS56TN83I8XR7j8JtTxzcPyrHxdGc8UxCZe83lIEmSJEmSJEnSwAzDGNeSJEmSJEmSpHlkTldcR8TSiHgkIjZFxIo+7G9NRGyNiG/W0g6LiA0R8Wj5e2gP9390RPxFRDwcEQ9FxEX9jCEiXhoR90XEX5f9/6d+7r8Wx34R8VcR8fkB7X88IjZGxNcj4oFBxDCX9btcTxDDwP7HU/2eiYiV5b16JCLO6FM8l0XE98r78/WIeHM/4pnOd2Cv4ukSy0Dem6YZdDke9Pm6QzwDPX93iKcR5/O2mAZ6btfeBl2OJ9K08rQvw/DZjohXRMQtEfHt8r7+sybGCRAR/7b8378ZETeW77NGxtoETS3H0zVs5X8mhuG7YyaG6XtnkAZRhruUs4H9zokp/j7vQzw/XXsfvh4RP4yI9/bzPYpZqjeIiJPLe7spIj4eETHT2KYsM+fkg2qQ++8APwX8GPDXwGt6vM+fB34O+GYt7b8AK8rzFcCHe7j/hcDPlecHAf8LeE2/YgACeHl5fgBwL3BqP9+Dso/fBT4NfL7f/4Oyj3HglW1pfY1hrj4GUa6b9j+eyvdMKf9/DRwIvLq8d/v1IZ7LgH/XYd2exjPV78BextMlloG8N016NKEcT6Uc9SmegZ6/O8TTiPN5W0wDPbf72Ov/MfBy3CW2RpWnScTb+M82sBb4V+X5jwGvaGicRwGPAQvK63XAeU2MtQmPJpfjGRzTUJX/GR5r4787Znh8Q/G9M+D3aCBluEs5u4wB/c5hCr/P+xFPh//T3wI/2c/3iFmqNwDuA/4Z1e+DO4Bf6vdnfS63uD4F2JSZ383M/wPcBJzVyx1m5leAbW3JZ1F96VL+nt3D/W/JzK+V59uBh6ku4PoSQ1Z2lJcHlEf2a/8AEbEIOBP4ZC25b/vvogkxzAV9L9dT0K9yNpXvmbOAmzLz+cx8DNhE9R72Op6J9DSeaXwH9iyeLrFMpOf/qwYZeDke9Pm6QzwDPX93iGfg5/O6Bp/b57OBl+OJNK08dTMMn+2IOJjqx++1AJn5fzLzBzQszpr9gQURsT/w48CTNDfWQWtsOZ6uYSr/MzEM3x0zMYTfO4MykDI8RL9zBvYbuc1pwHcy82+6rDPrMc1GvUFELAQOzsy7s6rFvp4BlLu5XHF9FPBE7fVmuhemXhnJzC1QFXDgiH7sNCIWA6+jaiXVtxhKl6WvA1uBDZnZ1/0DHwN+D/hRLa3f/4MEvhgRD0bEBQOKYa5qSrlu2v94on0P8v36nYj4Rumi1OqC1Ld4Jvkd2Jd42mKBAb83DdDUY23E9/Sgzt8d4hj0+bzuYwz+3K49NbUc76Ep5amLj9H8z/ZPAX8H/GkZluCTEfEymhcnmfk94A+Bx4EtwLOZ+UUaGGtDDEU5nq4hKP8z8TGa/90xE0PzvTNgAy/DDfqdM5Xf5/1+35YBN9ZeD/K34FTfk6PK817H1dVcrrjuNO5K9j2KAYiIlwOfAd6bmT/s574zc1dmngQsorpDc0K/9h0RbwG2ZuaD/drnBN6QmT8H/BJwYUT8/IDjmUuaUq6H5X88qPfrauAfAydR/XC8op/xTOE7sOfxdIhloO9NQ8ynY52SQZ6/2w3yfF7XoHO79tT4ctyk8tTJEH2296fqanx1Zr4O+Huq7sWNUyoAzqLq5nwk8LKI+LXBRtVojS/H09X08j8TQ/TdMRND870zYAMtww37nTOV3+d9e98i4seAXwb+e0lq6m/BifY/6LiAuV1xvRk4uvZ6EVVXsX57qjSvp/zd2sudRcQBVF8eN2TmZwcRA0DpyjMGLO3j/t8A/HJEjFN1k3ljRPxZH/cPQGY+Wf5uBT5H1cWj7/+DOaoR5bqB/+OJ9j2Q9ysznyqVXj8C/oTd3Zx6Hs8UvwN7Gk+nWAb53jRIU491oN/TTTl/txvQ+byuEed27aWp5RhobnlqMyyf7c3A5tLrAuAWqgqlpsUJ8IvAY5n5d5n5AvBZ4P+imbE2QaPL8XQNSfmfiWH57piJYfreGaSBleGm/c6Z4u/zfr5vvwR8LTOfKvEN+rfgVN+TzeV5r+Pqai5XXN8PHBsRry53OZYB6wcQx3pgeXm+HLi1Vzsqs3teCzycmR/pdwwR8aqIeEV5voDq4vHb/dp/Zq7MzEWZuZjq//3lzPy1fu0fICJeFhEHtZ4DpwPf7GcMc9zAy3VD/8cT7Xs9sCwiDoyIVwPHUk2u0FOtk2HxK1TvT8/jmcZ3YM/imSiWQb03DTPwcjyBgZXhQZ+/O8Qz0PN5XRPO7eqoqeW4ceVpIsPy2c7MvwWeiIifLkmnAd+iYXEWjwOnRsSPl8/BaVRjrzYx1iZobDmermEp/zMxLN8dMzFk3zuDNJAy3LTfOdP4fd7P313voDZMSAN+C07pPSnDiWyPiFPL//1cBlHuss+zQfbzAbyZaobT7wC/34f93UjV3P8FqjsT5wOHA3cBj5a/h/Vw//83VbP9bwBfL4839ysG4GeBvyr7/ybwH0t6396DWiyj7J5huZ//g5+imo31r4GHWp+7QbwHc/XR73LdtP/xVL9ngN8v79Uj9GAG4Ani+RSwsXwXrAcW9iOe6XwH9iqeLrEM5L1p2qMB5Xig5+spfF4GEhMNOp+3xTXKAM7tPib8fwy0HHeJq1HlaZIxN/qzTdWl+YHynv45cGgT4yyx/ieqG23fLOfcA5saaxMeTS3HMzieoSv/MzzeRn93zPDYhuZ7Z8DvU9/LcJdyNqjfgFP+fd7LeGr7+HHgaeCQWlrf3iNmqd4AWFLOqd8BrgKi35/zKIFIkiRJkiRJktQIc3moEEmSJEmSJEnSELLiWpIkSZIkSZLUKFZcS5IkSZIkSZIaxYprSZIkSZIkSVKjWHEtSZIkSZIkSWoUK64lSZIkSZIkSY1ixbUkSZIkSZIkqVGsuJYkSZIkSZIkNYoV15IkSZIkSZKkRrHiWpIkSZIkSZLUKFZcS5IkSZIkSZIaxYprSZIkSZIkSVKjWHEtSZIkSZIkSWoUK64lSZIkSZIkSY1ixbUkSZIkSZIkqVGsuJYkSZIkSZIkNYoV15IkSZIkSZKkRrHiWpIkSZIkSZLUKFZcS5IkSZIkSZIaxYprSZIkSZIkSVKjWHEtSZIkSZIkSWoUK64lSZIkSZIkSY1ixXXDRcR/joj3DjqOQYuI90XEJ6eR72cj4n/2IiZpXyy/sysiPhIRvz3oODS/WI5nl+dlNY1lvLOIuC8ijh90HFKLZXV2RcRnI2LpoOOQpH2x4rrBIuJVwLnAfyuvRyNibABxjEfE4kmum72IITM/lJn/apIxXBYRl5V83wB+EBH/shdxSROx/M6Otvj/APj9iPixAYakecRyPDs8L6upLON7bHdxRIzXkv4Q+EAv9iVNlWV16iJiLCL+VVtaPabVwKr+RqXZ4o2cuScirouID/Zo2xkRx0xy3cVl/f17FMuBEfHtiDhisnmsuG6284DbM/N/z3RDvfrQDYkbgN8adBCad87D8jurMnML8G3glwcdi+aN87Ac94LnZTXFeVjGJ4p9PfALEbGw3/FIHZyHZXVWZeZ9wMERsWTQsWhqmnIjZ74pN4NGJ7nupG9yzQf1z2hmPg+sAS6ZbH4rrpvtl4D/Z6KF5S7IeyLiuxHx/Yj4g4h4SVl2XkT8vxHx0YjYBlxW7mz8YUQ8HhFPRcR/jYgFZf1XRsTnI+IHEbEtIv6yta3piohfj4iHI2J7ifG3astGI2JzRPxeRGyNiC0RcXZEvDki/leJ4X219S+LiD8rz1t3gJaXY/l+RPx+l1DGgNMi4sCZHI80RZbf3eufEhF3l/i2RMRVUVpNR8T/VY7/6PL6tWW9fzJBaGPAmTM5NmkKhr0cj0XEByPif0bEjoj4HxFxeETcEBE/jIj7o3ZRHRH/JCI2lP0/EhFvry07MyL+quR7IkoL6rLM87KG1dCW8YhYFhEPtKX924hYX55PpsyeHxGPA19u335mPgc8CJw+3RilWTS0ZbVs87CI+NOIeDIinomIP68t+82I2FT2tT4ijizpe7V6jFor6nJcXy3H8UxEPBYRv1SWrQL+OXBVOf9fNUFoY3hdPYzOY5Zu5MyW8IbQtETEyKBjmInyXXrINLJ+Glg+6d8CmemjoQ/g74B/2mV5An8BHAb8I+B/Af+qLDsP2An8G2B/YAHwMarWE4cBBwH/A/jPZf3/DPxX4IDy+OdAzDD+M4F/DATwL4B/AH6uLBst8f3Hsr/fLMf76RLb8cBzwE+V9S8D/qw8X1yO/U/Kcb0WeB74mS6x/BD42UH/T33Mn4fld4/yezJwajmWxcDDwHtr+1pF9aN5AfAN4He6xPWrwNcG/f/1MT8ec6AcjwGbSlk+BPhWifEXS0zXA39a1n0Z8ATw62XZzwHfB44vy0eBE6kaPfws8BRwdlm2GM/LPobwMcxlHPhxYDtwbC3tfmBZeT6ZMnt9KfsLJtjHx4GPDPr/5MPHMJfVss3bgJuBQ8s2/0VJf2M51/4ccCBwJfCVsqxVTvevbWes7bheoLoO3w94N/BkK9b6ul3i+l3gs4P+//qY8ufpy8Cv1V6PAmO11wn8a+DRcp64nOpa8G6q6691wI+VdQ8FPl/K2DPl+aLatg4D/rR8tp4B/ry2z81UrWb/FvhU+Qx/rKz7ZHl+4D6O5ZVlnz8AtgF/CbykLDsS+EyJ7THgPbV8p5Tj+QGwBbiqdkwBfBTYCjxL9fvyhLLsEKpz398BfwO8v7a/84CvUg2V9UzZ5y/V9jkGjE7yfzQOLJ5g2QHA2cCtwPZa+nXAByf5fxkDPgj8T2AH1XfY4VS9Gn9IdT2wuLZ+Au8Bvkv1nfMHtePerxzz98vyC6l991D9NniY6rP0XeC3ats9quzvBqrfFy+Z4JhHqX1GS9qjlO/Cfb6fgy50Prp+2F8A/kmX5Qksrb3+18Bd5fl5wOO1ZQH8PfCPa2n/DHisPP9AKTjH9PB4/hy4qDwfBf43sF95fVA5ntfX1n+Q3RfYl7F3xXW94N5HuVCfYN/fA35+0P9TH/PnYfndXX47bOu9wOdqrw8o628EvkCXHwfAm4DvDvr/62N+PIa9HFNd1P5+7fUVwB211/8S+Hp5fg7wl235/xtw6QTb/hjw0fLc87KPoXzMgTL+Z8B/LM+PpfpR+eMTrNupzP7UPra/Clgz6P+TDx/DXFaBhcCPgEM7LLsW+C+11y8vx7qYyVVcb6ot+/Gy/k+0r9sltt8Evjzo/6+PKX+mJnMjZz1wMFWDoueBu4CfYndDhuVl3cOB/1/5/BwE/HdK5XRZPtFNl1GqG0IfpqqwXlDKzj3AEcCrqCpVL9/HsXS8UUR10/VBqoZSP1Zi/y5wRsk3YcMo4IyS9xVlWz8DLCzLri/l+6CS738B55dl59HlZtAs/N9OBD5CVaF+N/DbwCtqy69jd8X1vv4vY0yycUrtM/EXdL6599tUw3EeXZb/BXtWXE/YoK0s/wngYqobBH9TPgddry9KvvXUbkZ0ezhUSLM9Q/Uh7eaJ2vO/obor1WnZq6g+9A+Wbk8/oKogelVZ/gdUH/wvli5WK2YSOEBE/FJE3FO6Pf0AeDPVHbWWpzNzV3ne6ubyVG35/6Y6eU/kb2vP/2Ef6x5EdTdO6hfLbymTEXFc6XL5txHxQ+BD9W1l5gtUJ+oTgCuynMkmYFlWPw11OS7ay+VE59mfBF7fiq3E906qi1Ei4vUR8RcR8XcR8SzVRW79OwE8L2v4DHsZ/zTwjvL8/0/1o/YfYNJl9gm6s5yqKYa5rB4NbMvMZzosO7LECkBm7gCepmrFOBkvnndbZZ/u5952lvHh9AqqG5XdfDgzf5iZDwHfBL6Ymd/NzGeBO4DXAWTm05n5mcz8h8zcTnXD8l8ARDXHwS8Bv52Zz2TmC5lZH7LnR1QNHJ7PatiSdwIfyMytmfl3wH8C3rWPOF+gurnzk2X7f1l+C/5T4FWZ+YHM/D+Z+V2qnn3LStwPZuY9mbkzM8epGlv8i9o2DwL+CVWl88OZuSUi9qNqqLEyM7eXfFe0xfg3mfkn5Xfu2hLbjIbziIg3lqG9bqfqlfzPM/OfZeZ/zcwfdMrT7f9S86eZ+Z3a//Q7mfmlzNxJVdH9urb1P5yZ2zLzcaqb2a3rh7cDH8vMJzJzG9XNhHost5X9ZPn/f5HqBkNr+d9m5hWZ+bPAr1B9Pu8pQxu9tstbs72su09WXDfbN4Dj9rHO0bXn/4jqjlBLvfLn+1Q/UI/PzFeUxyGZ+XKAUnAvzsyfomqB9bsRcdp0Ay9j1XyGqsvBSGa+gqqgxnS3OYNYjqS6S/dIv/etec3yu9vVVHdxj83Mg4H31bcVEUcBl1J1Q7tiH2Nd/Qzw19OMQ5qqoS3H0/AE8P/UYntFZr48M99dln+aqmXE0Zl5CFXrmGl9J3heVoMMexn/IvDKiDiJ6gfop2vLJlNmu90oBs+5ao5hLqtPAIdFxCs6LHuS6sYxABHxMqqWlt+jahUOVSV7y09MYb/7Kt9gGR9Wk7mRM6mGCxHx4xHx3yLib0oDo68AryiVvN1uugD8XVbzIbTscSOGvW8gdTLRjaKfBI5sa1DxPkolcreGUZn5ZaqhQ/4YeCoiromIg8vyH+sQY/1G0UxvBnVyBHAM1Q2Ev27bf0f7+L+0TLZxSstEN/eO7LCsHsu+GrTVbaI6xk1UNw5eMcF6MIUbZ1ZcN9vt7H1Xpd2/j4hDo5rY7CKqbhx7ycwfUd2h+mhEHAFVZVFEnFGevyUijomIoBqjZld57KFMAjE+idh/jKrLyN8BO8tEEYOa3GWUqgvU8wPav+Yny+9uB5W4dkQ16WKrIowS83VUXSXPpxqj7PIu2/oXVHeUpX4Y5nI8VZ8HjouId0XEAeXxTyPiZ8ryg6h+vDwXEadQte6crlE8L6sZhrqMl1ZVt1D98D8M2FBbPKMyW24in9y2TWlQhrasZuYWqmvXT5T4DoiIny+LPw38ekScVMrch4B7M3O8tFj9HvBrEbFfRPwGVXf9yXqKaniFbryuHk6TuZEzWRcDP0015OPBQOuzGXS/6QJ73xzZ40YMe99A2nsDE98oeoJq+J56g4qDMvPNJWvXhlGZ+fHMPJlqqJTjgH9PddPqhQ4xfq9bjDOVmTdR3XS6nur37pMR8ScR8c/L90wn3f4v0zXRzb0tHZZVO5tEg7by/bQ0Im4EHqcaWuQ/Uw0hOOGkukzhxpkV1812PfDmKDMcT+BWqvF7vk41/tC1Xda9hOrOxz3lrs2XqAoDVOPifYlqYPe7gU9k5liHbRwN/L/7Crx0Z3gP1cD/z1BdLK/fV74eeSdVKxOpnyy/u/27so3tVD8U6j8k3kN15/w/ZGZSTf7w6xHxz9s3ElV3tddQjbct9cPQluOpKuX+dKoumE9StThpjVsI1XihH4iI7VTjDa6bwe48L6sp5kIZ/zTVmJb/vVRkt8y0zP4y1URKXSsdpD4Z9rL6LqoKs29TjW/7XoDMvAv4D1QVQ1uoKqaX1fL9JlWF29NUFXD/c5L7A/gj4K0R8UxEfLx9YUT8U+DvM/O+KWxTzTCZGzmTdRBV69wfRMRhVL1ggX3edOnkRuD9EfGqiHgl1bnnz7rtvMuNovuAH0bEJRGxoFSOnlA+t624J2oY9U+jGi7rAKqeC88Bu8rwH+uAVRFxUET8JNUEpV1jnCDu0YiYTK8GADLzucy8MTNPp5rEfJzqO2rTBFkm/L/MwEQ399YB74mIRRFxKFAfHqlrg7Zy828zVUX1PVRzA/xqZv6PtmuSPUTV4/qwkmffsgGDy/voOmD5hyiDzHdYlvRwMrYJ9vlF4GcG/b5MId4TgbsHHYeP+fmw/M56/FcA/3rQcfiYXw/L8azH73nZR6MelvEJ47gXOGHQcfjw0XpYVmc9/s8Abx50HD6m9b97ZaksXDDB8j3KA/BV4Lza6w8CnyzPj6Sa6G8H1YR9v8WeE/MdRjXW81NUDZo+W9JHgc1t+30p8HGqmzBbyvOX7uNY/i1VJe7fl2P6D7VlR1JVhv9t2fc9wC+WZT9PdSNoB/CXVBMCfrUsO42qVfoOqlbWNwAvL8sOpaqo/juqVt3/EXhJWXZeaxsTvZe19HcB/3MW/pf/d+35deyenHFf/5cxapOvlv/pdbXXv8iek7cmVYOx71LdCLsC2K8s2x/4aEl/DLiwbV8Xlv//D4BPATfV4nw58NppHPe/Bz4y2fWjZNIQKnd4js3Mie7SSGooy680/CzH0txmGZeGg2VV801EfAjYmpkfG3Qs81FEfJKqp9Odg45l2JThR/4a+PnM3DqZPPv3NiRJkiRJkiRJsyEz3zfoGOazzPxXg45hWGU1x80/mUoeW1xLkiRJkiRJmlUR8T6qyRPb/WVm/lK/49HwcXJGSZIkSZImISLGI2JjRHw9Ih4oaYdFxIaIeLT8PbS2/sqI2BQRj0TEGbX0k8t2NkXEx8vkZETEgRFxc0m/NyIW1/IsL/t4NCKW9/GwJWlaMvNDmfnyDg8rrTUpVlxLkiRJkjR5v5CZJ2XmkvJ6BXBXZh4L3FVeExGvAZYBxwNLgU9ExH4lz9XABcCx5bG0pJ8PPJOZx1BNmPXhsq3DgEuB1wOnAJfWK8glSZqL5twY16985Stz8eLFfdnX3//93/Oyl72sL/uaTcbdX72I+8EHH/x+Zr5qVjfaIP0qx36m+m9YY7ccT53luDvj7q9exT3fy3FTPg9NiKMJMTQljibEMJU4ZqkcnwWMludrgTHgkpJ+UxnT87GI2AScEhHjwMGZeTdARFwPnA3cUfJcVrZ1C3BVaY19BrAhM7eVPBuoKrtv7BbYsJRjaFYs0Kx4jKWzVixz+Xw8mWvqpvxPjKNZMQxTHN3K8JyruF68eDEPPPBAX/Y1NjbG6OhoX/Y1m4y7v3oRd0T8zaxusGH6VY79TPXfsMZuOZ46y3F3xt1fvYp7vpfjpnwemhBHE2JoShxNiGEqcUyjHCfwxYhI4L9l5jXASGZuAcjMLRFxRFn3KOCeWt7NJe2F8rw9vZXnibKtnRHxLHB4Pb1DnvZjuoCqNTcjIyP84R/+4YQHs2PHDl7+8pfv65j7okmxQLPiMZbOWrH8wi/8wqTLcUS8FPgKcCBVndgtmXlp6dVwM7AYGAfenpnPlDwrqXpD7ALek5l3lvSTgeuABcDtwEWZmRFxIHA9cDLwNHBOZo6XPMuB95dwPpiZa7vFO5lr6mH73p0PcTQhhmGKo9u5eM5VXEuSJEmS1CNvyMwnS+X0hoj4dpd1o0Nadkmfbp49E6vK9GsAlixZkt0qC5pSqQHNigWaFY+xdDbNWJ4H3piZOyLiAOCrEXEH8KtUQ/6sjogVVEP+XNI25M+RwJci4rjM3MXuIX/uoaq4XkrVc+LFIX8iYhnVkD/n1Ib8WUJVfh+MiPWtCnJJe3OMa0mSJEmSJiEznyx/twKfoxpv+qmIWAhQ/m4tq28Gjq5lXwQ8WdIXdUjfI09E7A8cAmzrsi1JU5CVHeXlAeWRVMP0tFo/r6UavgdqQ/5k5mNAa8ifhZQhfzIzqVpY1/O0tnULcFr7kD+lsro15I+kCVhxLUmSJEnSPkTEyyLioNZz4HTgm8B6YHlZbTlwa3m+HlgWEQdGxKupJmG8rwwrsj0iTi2VWee25Wlt663Al0ul2J3A6RFxaJmU8fSSJmmKImK/iPg61U2mDZl5L21D/gD1IX86DdNzFJMc8geY8pA/kioOFSJJkiRJ0r6NAJ+r6prZH/h0Zn4hIu4H1kXE+cDjwNsAMvOhiFgHfAvYCVxYhhcAeDe7x8a9ozwArgU+VSZy3EY1RAGZuS0iLgfuL+t9oDVRo6SpKeXwpIh4BVWZPqHL6n0f8qd9nPqxsbEu4VVjfe9rnX4wjmbFMFfisOJakiRJkqR9yMzvAq/tkP40cNoEeVYBqzqkPwDsVVmWmc9RKr47LFsDrJla1JImkpk/iIgxquE6noqIhWWC1dka8mdzhyF/RtvyjHWIa9Lj1ENzxh03jmbFMFficKgQSZIkSZIkzXkR8arS0pqIWAD8IvBtHPJHaiRbXEuSNEdFxDiwHdgF7MzMJWU285uBxcA48PbWTOYRsZJqFvRdwHsy886SfjK7uzPfDlxULr4lSZKkYbIQWBsR+1E15lyXmZ+PiLtxyB+pcay41oQWr7htj9fjq88cUCSSmqr+PeF3RGP9QmZ+v/Z6BXBXZq6OiBXl9SUR8Rqqi+rjgSOBL0XEceXC/Gqqcfbuoaq4XsruC3MNAcuqemHj957lPD9b0lCrl2PLsOaDzPwG8LoO6UM75I/lWE1U//1x3dKXTXs7VlzPUVY6S5ImcBa7x9ZbSzWu3iUl/abMfB54rLQQOaW02j44M+8GiIjrgbOx4lqSJEmS1EOOcS1J0tyVwBcj4sEyOznASBmTj/L3iJJ+FPBELe/mknZUed6eLkmSJElSz9jieh6yW6ckzRtvyMwnI+IIYENEfLvLutEhLbuk772BqnL8AoCRkRHGxsamGO7U7dixoy/7mW39jvviE3e++Hwm+/X9liRJktQvVlxrWhwrU5KaLzOfLH+3RsTngFOApyJiYWZuiYiFwNay+mbg6Fr2RcCTJX1Rh/RO+7sGuAZgyZIlOTo6OotH09nY2Bj92M9s63fce9ywfuf09+v7LUmSJKlfHCpEktTV4hW3vfjQ8IiIl0XEQa3nwOnAN4H1wPKy2nLg1vJ8PbAsIg6MiFcDxwL3leFEtkfEqRERwLm1PJIkSZIk9YQV15IkzU0jwFcj4q+B+4DbMvMLwGrgTRHxKPCm8prMfAhYB3wL+AJwYWbuKtt6N/BJYBPwHZyYUZpVEXF0RPxFRDwcEQ9FxEUl/bCI2BARj5a/h9byrIyITRHxSEScUUs/OSI2lmUfLzecKDelbi7p90bE4r4fqCRJkjQFDhUiSdIclJnfBV7bIf1p4LQJ8qwCVnVIfwA4YbZjVO/YQ2Lo7AQuzsyvlZ4SD0bEBuA84K7MXB0RK4AVwCUR8RpgGXA8cCTwpYg4rtxsuppqrPl7gNuBpVQ3m84HnsnMYyJiGfBh4Jy+HqUkSZI0Bba4liRJkgYoM7dk5tfK8+3Aw8BRwFnA2rLaWuDs8vws4KbMfD4zH6PqDXFKGbf+4My8OzMTuL4tT2tbtwCntVpjS5IkSU1ki2tJkqR5zAmXm6UM4fE64F5gpIwzT5lQ9Yiy2lFULapbNpe0F8rz9vRWnifKtnZGxLPA4cD3e3MkkiRJ0sxYcS1JkiQ1QES8HPgM8N7M/GGXBtGdFmSX9G552mO4gGqoEUZGRhgbG5sw3pEFcPGJO1983W3dXtqxY8fA9t2kGJoSRxNiaFIckiRp+qy4liRJkgYsIg6gqrS+ITM/W5KfioiFpbX1QmBrSd8MHF3Lvgh4sqQv6pBez7M5IvYHDgG2tceRmdcA1wAsWbIkR0dHJ4z5yhtu5YqNu39OjL9z4nV7aWxsjG5xzpcYmhJHE2JoUhySJGn6rLiWJEmaR5y4sXnKWNPXAg9n5kdqi9YDy4HV5e+ttfRPR8RHqCZnPBa4LzN3RcT2iDiVaqiRc4Er27Z1N/BW4MtlHGxJkiSpkay4liRJGlJWQs8ZbwDeBWyMiK+XtPdRVVivi4jzgceBtwFk5kMRsQ74FrATuDAzd5V87wauAxYAd5QHVBXjn4qITVQtrZf1+JgkSZKkGbHiWpIkSRqgzPwqncegBjhtgjyrgFUd0h8ATuiQ/hyl4luSJEkaBi8ZdACSJEmSJEmSJNVZcS1JkiRJkiRJahSHClFPtY+9Ob76zAFFImk2OJ6uJEmSJEnqB1tcS5IkSZIkSZIaxYprSROKiP0i4q8i4vPl9WERsSEiHi1/D62tuzIiNkXEIxFxRi395IjYWJZ9PCImmnxKkiRJkiRJAqy4ltTdRcDDtdcrgLsy81jgrvKaiHgNsAw4HlgKfCIi9it5rgYuAI4tj6X9CV3zyeIVt734kCRJkiRJw69nFde21JSGW0QsAs4EPllLPgtYW56vBc6upd+Umc9n5mPAJuCUiFgIHJyZd2dmAtfX8kiSJEmSJEkd9XJyxlZLzYPL61ZLzdURsaK8vqStpeaRwJci4rjM3MXulpr3ALdTtdS8o4cxS9rtY8DvAQfV0kYycwtAZm6JiCNK+lFU5bRlc0l7oTxvT99LRFxAVd4ZGRlhbGxs5kewDzt27OjLfmZbv+O++MSdk1rvyhtunXDZiUcdAvQu9nqMvdj+sH5WJEmSJO0WEUdTNaj6CeBHwDWZ+UcRcRnwm8DflVXfl5m3lzwrgfOBXcB7MvPOkn4ycB2wgKrO6qLMzIg4sOzjZOBp4JzMHC95lgPvL/v4YGa2GoZJ6qAnFde1lpqrgN8tyWcBo+X5WmAMuIRaS03gsYhotdQcp7TULNtstdS04lrqsYh4C7A1Mx+MiNHJZOmQll3S907MvAa4BmDJkiU5OjqZ3c7M2NgY/djPbOt33OfNwvAb4+8cBXoXez3G1r5m07B+ViRJkiTtYSdwcWZ+LSIOAh6MiA1l2Ucz8w/rK0+zseX5wDOZeUxELAM+DJwTEYcBlwJLqH4XPxgR6zPzmR4fszS0ejVUyMeoWmr+qJa2R0tNoN5S84naeq0WmUcxyZaakmbdG4BfLjeQbgLeGBF/BjxVhv+g/N1a1t8MHF3Lvwh4sqQv6pAuSZIkSVJfZeaWzPxaeb6daqSAbnVN0xkWsz7E5i3AaWXo2zOADZm5rVRWb8A5oKSuZr3F9SBaag5iiAFodtfx9q799ThHFkyuW323bUy2W363bUxVk9/vboYx7sxcCawEKOX432Xmr0XEHwDLgdXlb2tsiPXApyPiI1R3oY8F7svMXRGxPSJOBe4FzgWu7OexaHi1T7Q4vvrMAUUiSZK0W5mE/AHge5n5ltKK8mZgMTAOvL3VgtIhBqTmiojFwOuofqu+AfidiDiXqnxfXMrxdIbFfLGBZmbujIhngcOZuOFme1xTquOq1/EMsu6hKXUfTYijCTEMOo56feBM4ujFUCGtlppvBl4KHFxvqVnGxZ3VlpqDGGIAmt11vL1rf73r/JU33MoVG/fvuKzbNtj497UX+86/rzimqsnvdzfDGvcEVgPrIuJ84HHgbQCZ+VBErAO+RdX16sLSdQrg3ey+KL8Dh/uRJGkPEbEGaDX+OKGk3Qz8dFnlFcAPMvOk8iP7YeCRsuyezPztkmfKFWGSpqWX8zk5xIDUBxHxcuAzwHsz84cRcTVwOVX5uhy4AvgNptfYckYNNKdax1Wv4+nFsImT1ZS6jybE0YQYBh1HvT7wuqUvm3Ycs15xbUtNaW7JzDGqMenJzKeB0yZYbxXVuPbt6Q8AJ/QuQg27esvq2W5VbattaWp6WR7V1XXAVVSVywBk5jmt5xFxBfBsbf3vZOZJHbYzpYqw2T0EaX7ow3xOZwGXlW3dAlzVPsRAydMaYuDG3hypNHdFxAFUldY3ZOZnATLzqdryPwE+X15Op7FlK8/miNgfOATYVtJH2/KMzcYxSXNVTyZnnIAtNYdAeyWPJEmSeiszv1JaUu+lVFi9HXhjt23Ux9osr/dZEVbG5JQ0NR+jms/poFraHvM5RUR9Pqe+DjEAUxtmoClDDEBzuta3NCkeY+lsOrGU8+q1wMOZ+ZFa+sJWOQZ+BfhmeT6dxpbrqRps3g28Ffhy6QF1J/ChiDi0rHc6peGnpM56WnFtS835wRaNkiRJPfPPgacy89Fa2qsj4q+AHwLvz8y/pPvE5hNVhH2/18FLc0mf5nOa8RxQUxlmoClDDEBzuta3NCkeY+lsmrG8AXgXsDEivl7S3ge8IyJOoipX48BvwbQbW14LfKr0sthGNWQQmbktIi4H7i/rfaDVi0JSZ/1scS1JmqdaN7guPnHnHn3jJEn79A72HApgC/CPMvPpMqb1n0fE8UyvImwv022pCYNrrdmE1n9NiKEpcTQhhh7G0Y/5nBxiQOqhzPwqnc+Lt3fJM6XGlpn5HGWUgQ7L1gBrJhuvNN9ZcT3kHItSkiRpbiqVVr9KNakiAGWs3OfL8wcj4jvAcUyvImwv022pCYNrrdmE1n9NiKEpcTQhhl7F0af5nBxiQJKkwoprzTrHyZYkqTc8x847vwh8OzNfHAIkIl4FbCsVXz9FVRH23dL9eEoVYf08EGmOm835nBxiQJKkwoprDYytxaXmsnJs7oiI/YAHgO9l5lsi4jDgZmAx1fh9b8/MZ8q6K4HzgV3AezLzzpJ+Mrt/XN8OXGSllzR7IuJGqiEAXhkRm4FLM/NaqgqrG9tW/3ngAxGxk6qs/nat8mpKFWGSpq9X8zk5xIAkSbtZcS1J0tx2EfAwcHB5vQK4KzNXR8SK8vqSiHgNVWXW8VTdmb8UEceVlmFXU415ew9VxfVSdleISZqhzHzHBOnndUj7DPCZCdafckWYJEmS1FQvGXQAkqTBWLzithcfmpsiYhFwJvDJWvJZwNryfC1wdi39psx8PjMfAzYBp5RJpg7OzLtLK+vra3kkSZIkSeoJW1zPIVY+SZLafAz4PeCgWtpIZm4ByMwtEXFEST+KqkV1y+aS9kJ53p4uSZIkSVLPWHEtK7wlaQ6KiLcAWzPzwYgYnUyWDmnZJb3TPi+gGlKEkZERxsbGJhXrTOzYsaMv+5lt04374hN3zn4wE+gU33x7vyVJkiQNjhXXkiTNTW8Afjki3gy8FDg4Iv4MeCoiFpbW1guBrWX9zcDRtfyLgCdL+qIO6XvJzGuAawCWLFmSo6Ojs3g4nY2NjdGP/cy2qcS95w3m/l26jb9zdK+0+fB+S5IkSWoGK66HjK2jJUmTkZkrgZUApcX1v8vMX4uIPwCWA6vL31tLlvXApyPiI1STMx4L3JeZuyJie0ScCtwLnAtc2c9j0WC0X3OMrz5zQJFIkiRJmo+suJYkaX5ZDayLiPOBx4G3AWTmQxGxDvgWsBO4MDN3lTzvBq4DFgB3lIckSZIkST1jxbUkyd4cc1xmjgFj5fnTwGkTrLcKWNUh/QHghN5FKEmSJEnSnqy4nifqlVIXnzjAQCRJkiRJkiRpH14y6AAkSZIkSZIkSaqz4lqSJEmSJEmS1CgOFaK+chxdSZIkSZIkSftii2tJkiRJkiRJUqNYcS1JkiQNUESsiYitEfHNWtplEfG9iPh6eby5tmxlRGyKiEci4oxa+skRsbEs+3hEREk/MCJuLun3RsTivh6gJEmSNA1WXEuSJEmDdR2wtEP6RzPzpPK4HSAiXgMsA44veT4REfuV9a8GLgCOLY/WNs8HnsnMY4CPAh/u1YFIkiRJs8UxriVJQ8fx8iXNJZn5lSm0gj4LuCkznwcei4hNwCkRMQ4cnJl3A0TE9cDZwB0lz2Ul/y3AVRERmZmzdhCSJEnSLLPiWpIkSfu0eMVtXHziTs5bcRvjq88cdDjzxe9ExLnAA8DFmfkMcBRwT22dzSXthfK8PZ3y9wmAzNwZEc8ChwPfb99hRFxA1WqbkZERxsbGJgxuZAFcfOLOF193W7eXduzYMbB9NymGpsTRhBiaFIckSZo+K64lSZKk5rkauBzI8vcK4DeA6LBudklnH8v2TMy8BrgGYMmSJTk6OjphgFfecCtXbNz9c2L8nROv20tjY2N0i3O+xNCUOJoQQ5PikNQsEXE0cD3wE8CPgGsy848i4jDgZmAxMA68vdwwJiJWUg27tQt4T2beWdJPphruawFwO3BRZmZEHFj2cTLwNHBOZo6XPMuB95dwPpiZa3t8yNJQc4xrSZIkqWEy86nM3JWZPwL+BDilLNoMHF1bdRHwZElf1CF9jzwRsT9wCLCtd9FLktRYO6l6Mf0McCpwYZk/YgVwV2YeC9xVXs/q3BKlcvxS4PVU5/VLI+LQ3h6uNNysuJYkSZIaJiIW1l7+CvDN8nw9sCwiDoyIV1P9UL4vM7cA2yPi1IgI4Fzg1lqe5eX5W4EvO761JGk+yswtmfm18nw78DDVkFpnAa3Wz2up5omA2twSmfkY0JpbYiFlbolyTr2+LU9rW7cAp5Vz8xnAhszcVlpzb6Dz5MySCocKkSRJkgYoIm4ERoFXRsRmqtZYoxFxEtWQHuPAbwFk5kMRsQ74FlWrsQszc1fZ1LvZ3WX5jvIAuBb4VJnIcRtVyzFJkua1MjHy64B7gZFyE5jM3BIRR5TVZnNuiRfTO+SR1IEV15IkSdIAZeY7OiRf22X9VcCqDukPACd0SH8OeNtMYpQkaS6JiJcDnwHem5k/rBpEd161Q9p055aY1JwTU5koGfacLHmQk9I2ZVLcJsTRhBgGHUd9Au+ZxGHFtaS9RMRLga8AB1J9T9ySmZfO5oQV/TweSZIkSZIAIuIAqkrrGzLzsyX5qYhYWFpbLwS2lvSZzC2xuW1uic1UPazqecba45vKRMmw52TJg5ooGZozKW4T4mhCDIOO47wVt734/LqlL5t2HI5xPQQWr7jtxYfUJ88Db8zM1wInAUsj4lRmd8IKSZIkSZL6pow1fS3wcGZ+pLaoPh/EcvacJ2K25pa4Ezg9Ig4tkzKeXtIkTaAnLa5trSkNt1LGdpSXB5RHUk0yMVrS11LdHb6E2oQVwGNlDM1TImKcMmEFQES0JqxojbkpSerAm9WSJEk98QbgXcDGiPh6SXsfsBpYFxHnA49ThtiazbklMnNbRFwO3F/W+0BmbuvRcUpzQq+GCmm11txRumB8NSLuAH6VqrXm6ohYQdVa85K21ppHAl+KiOPKl0GrteY9VBXXS7HSS+q50mL6QeAY4I8z896ImM0JKzRPWSEnSZIkaRAy86t0Hmsa4LQJ8sza3BKZuQZYM9l4pfmuJxXXttaUhl+5cXRSRLwC+FxE7HVCrpnR5BMw9QkoZkNTJkyYqqnEvfF7z774/MSjDtljWX2yhH6pTxzSSf242tfrtmw626i/N7D3+1M3rJ8VSZIkSZKGVc8mZ7S1pjQ3ZOYPImKMqrfDbE5Y0b6fKU1AMRuaMmHCVE0l7vqECO0TdZw3gJbPF5+488WJQzqpx9geX7dls7GNbhOZDOtnRZIkzZ5+DIkZEQcC1wMnA08D52TmeMmzHHh/CeeDmbm2x4csSdJA9aziup+tNQfRUhP61wJvtltF7qvF4yBM5n0c1haPwxh3RLwKeKFUWi8AfhH4MLsnmVjN3hNWfDoiPkI13E9rwopdEbG9TOx4L9WEFVf292gkSZKkWdGPITHPB57JzGMiYhnVNfg5pXL8UmAJ1W/iByNifauCXJKkuahnFdct/WitOYiWmtC7Fnh7j/86u/+mfbV4HIRuLR1bhrXF45DGvRBYW3pOvARYl5mfj4i7mb0JK9Rnwz629LDHL0mShlufhsQ8C7isbOsW4KqICOAMYENrIreI2ED1G/vGnhysJEkN0JPaS1trSsMtM78BvK5D+tPM0oQVkiRJ0rDpw5CYRwFPlG3tjIhngcPr6R3ytMc46R7J9d64g+4l2rSeqk2Kx1g6a1IsknqjV81uba05RbYklCRJkqRm68OQmDOe9HwqPZKvvOHWF3vjTqYXbC81radqk+Ixls6aFIuk3uhJxbWtNTVV7RX346vPHFAkkiRJktRdD4fEbOXZHBH7A4cA20r6aFuesVk8JEmSGqdZAx1LkqRZEREvBb4CHEh1vr8lMy8tkzvdDCwGxoG3tyZ2ioiVVJNC7QLek5l3lvST2d376XbgojLOp2bJMPe88ubzzEXEGuAtwNbMPKGk/QHwL4H/A3wH+PVSUbYYeBh4pGS/JzN/u+TpWFYj4kDgeuBk4GngnMwc78/RSXNHn4bEbG3rbuCtwJdLOb4T+FBEHFrWOx1Y2dsjliRpsKy4liQ1xjBX3jXQ88AbM3NHRBwAfDUi7gB+FbgrM1dHxApgBXBJRLwGWAYcT/Xj+ksRcVzpEn011ViZ91BVhi1ljg7dJQ3IdcBVVJXLLRuAlWWM2w9TVVBdUpZ9JzNP6rCdicrq+cAzmXlMRCyjqmg7pwfHIc11/RgS81rgU2Uix21U52Yyc1tEXA7cX9b7QGuiRkmS5iorriVJmoNKi+gd5eUB5ZHAWezuaryWqpvxJSX9psx8Hnis/GA+JSLGgYMz826AiLgeOBsrrmfMGzVqycyvlJbU9bQv1l7eQ9XyckJleIKJyupZwGVl1VuAqyIi7DkhTU0/hsTMzOcoFd8dlq0B1kwtakmShpcV12qk+o95uxxL0vSUFmEPAscAf5yZ90bESGZuAShjcR5RVj+KqnKsZXNJe6E8b0/XPGale9/9BtUQPy2vjoi/An4IvD8z/5KqXE5UVo8CngAoLbifBQ4Hvt/rwCVJkqTpsuJajdf6cXzxiTs5b8VtVmRL0iSV7sgnRcQrgM9FRLfJjqPTJrqk772BiAuohilgZGSEsbGxKcU7HTt27OjLfmbbjh07uPjEXftesWFGFlTn426a+P8Y1s8JQET8PtUQAzeUpC3AP8rMp8uY1n8eEcfTvaz2pBy3fx4G9R434f/bhBiaEkcTYmhSHJIkafqsuJYkaY4rk0iNUY13+1RELCytrRcCW8tqm4Gja9kWAU+W9EUd0jvt5xrgGoAlS5bk6OjobB5GR2NjY/RjP7NtbGyMK77694MOY8ouPnEnV2zsfvk4/s7R/gQzBcP6OYmI5VSTNp7WGtajDOfzfHn+YER8BziO7mW1Vb43R8T+wCFUY+fuZSrl+Mobbt3j8zCo/30T/r9NiKEpcTQhhibFIUmSpu8lgw5AkiTNvoh4VWlpTUQsAH4R+DawHlheVlsO3FqerweWRcSBEfFq4FjgvjKsyPaIODUiAji3lkdSj0TEUqrx5385M/+hlv6qMgwQEfFTVGX1u/soq/Vy/1bgy45vLUmSpKazxbUkSXPTQmBtqeB6CbAuMz8fEXcD6yLifOBxygRQmflQRKwDvkU1LMGFZagRgHcD1wELqCZ6c2JGaRZFxI1Uk6a+MiI2A5cCK4EDgQ1VPTT3ZOZvAz8PfCAidgK7gN/OzFbr6YnK6rXAp8qkq9uAZX04LEmSJGlGrLiWJGkOysxvAK/rkP40cNoEeVYBqzqkPwB0Gx9b0gxk5js6JF87wbqfAT4zwbKOZTUzn6PcpJIkSZKGhRXXkjTHtCY0lSRJkiRJGlZWXGuo1SvoxlefOcBIJEnam+cpSZIkSZoeJ2eUJEmSJEmSJDWKLa4lqc9sgSlJkiRJktSdLa4laQgtXnHbiw9JkiRJ0r5FxJqI2BoR36ylXRYR34uIr5fHm2vLVkbEpoh4JCLOqKWfHBEby7KPR0SU9AMj4uaSfm9ELK7lWR4Rj5bH8j4dsjTUbHEtSQ3VXilt62xJkiRJmpHrgKuA69vSP5qZf1hPiIjXAMuA44EjgS9FxHGZuQu4GrgAuAe4HVgK3AGcDzyTmcdExDLgw8A5EXEYcCmwBEjgwYhYn5nP9OYwpbnBimtJkqRZNFFPiHr6xSfuxMswSZKk/srMr9RbQe/DWcBNmfk88FhEbAJOiYhx4ODMvBsgIq4HzqaquD4LuKzkvwW4qrTGPgPYkJnbSp4NVJXdN87CYUlzlr+YJKlBHPpDkiRJkvrudyLiXOAB4OLSEvooqhbVLZtL2gvleXs65e8TAJm5MyKeBQ6vp3fII2kCVlxLkiSpJxzySJIkDYGrgcuphvC4HLgC+A0gOqybXdKZZp49RMQFVMOQMDIywtjYWJfQYWRBqzcf+1y3l3bs2DHQ/TcpjibEMOg4Wp/JmcZhxbUkSZIkSZLmpcx8qvU8Iv4E+Hx5uRk4urbqIuDJkr6oQ3o9z+aI2B84BNhW0kfb8oxNEM81wDUAS5YsydHR0U6rvejKG27lio1V9d74O7uv20tjY2PsK9b5EkcTYhh0HOfVGrBct/Rl047jJbMUjyRJ0ryxeMVtLz4kSZI0vCJiYe3lrwDfLM/XA8si4sCIeDVwLHBfZm4BtkfEqWX86nOBW2t5lpfnbwW+nJkJ3AmcHhGHRsShwOklTVIXtriWpAGy0kuSFBFrgLcAWzPzhJJ2GHAzsBgYB95extskIlYC5wO7gPdk5p0l/WTgOmABcDtwUWZmRBwIXA+cDDwNnJOZ4306PEmSGiMibqRq+fzKiNgMXAqMRsRJVEN3jAO/BZCZD0XEOuBbwE7gwszcVTb1bnafc+8oD4BrgU+ViRy3AcvKtrZFxOXA/WW9D7QmapQ0MSuuJWnIWfktDZZlULPgOuAqqsrllhXAXZm5OiJWlNeXRMRrqH4EHw8cCXwpIo4rP6SvphoT8x6qiuulVD+kzweeycxjImIZ8GHgnL4cmSRJDZKZ7+iQfG2X9VcBqzqkPwCc0CH9OeBtE2xrDbBm0sFKsuJac1e9IsHJoDRoVmz1j++1esHPlXopM78SEYvbks9i91iYa6nGwbykpN+Umc8Dj5UWXadExDhwcGbeDRAR1wNnU1VcnwVcVrZ1C3BVRETpuixJkiQ1khXXkiRJmjVW8s+akTKGJpm5JSKOKOlHUbWobtlc0l4oz9vTW3meKNvaGRHPAocD3+9d+JIkSdLMWHEtSZIkDY/okJZd0rvl2XvjERdQDTfCyMgIY2NjEwYysgAuPnHni6+7rdtLO3bsGNi+mxRDU+JoQgxNikOSJE2fFdeSJElS8zwVEQtLa+uFwNaSvhk4urbeIuDJkr6oQ3o9z+aI2B84hGrCqL1k5jXANQBLlizJ0dHRCQO88oZbuWLj7p8T4++ceN1eGhsbo1uc8yWGpsTRhBiaFIckSZq+lww6AEmSJEl7WQ8sL8+XA7fW0pdFxIER8WrgWOC+MqzI9og4NSICOLctT2tbbwW+7PjWkiRJajpbXEvaS0QcDVwP/ATwI+CazPyjiDgMuBlYDIwDb8/MZ0qelcD5wC7gPZl5Z0k/GbgOWADcDlw0H34s92KM19Y2q27Zfn1L0lwRETdSTcT4yojYDFwKrAbWRcT5wOPA2wAy86GIWAd8C9gJXJiZu8qm3s3uc+4d5QFwLfCpMpHjNmBZHw5LkiRJmpGe1HxY6SUNvZ3AxZn5tYg4CHgwIjYA5wF3ZebqiFgBrAAuiYjXUP0IPh44EvhSRBxXfkhfTTVW5j1UZXgpu39IS1JjOcmg+iUz3zHBotMmWH8VsKpD+gPACR3Sn6NUfEuSJEnDoldDhbQqvX4GOBW4sFRsraCq9DoWuKu8pq3SaynwiYjYr2yrVel1bHks7VHMkorM3JKZXyvPtwMPA0cBZwFry2prgbPL87OAmzLz+cx8DNgEnFLG5Dw4M+8uN5yur+WRJEmSJEmSOupJi+syxt6W8nx7RNQrvUbLamuBMeASapVewGOlG+MpETFOqfQCiIhWpZetNaU+iYjFwOuAe4GRUr4pk0UdUVY7iqpFdcvmkvZCed6e3mk/F1DdpGJkZKQvs8D3crb5ajiP3hhZ0Nvt99KgY6//v9vj6PZZ6OVnRdLerdvHV585oEgkaWL96FkcEQeWfZwMPA2ck5njJc9y4P0lnA9mZqtBiSRJc1LPB0ntV6WXpNkXES8HPgO8NzN/WM311HnVDmnZJX3vxMxrgGsAlixZkv2YBb6Xs82f18MhBi4+cSdXbBzOMa4HHfv4O0dffN7+P6ova9fLz4okSRoa/RhO73zgmcw8JiKWAR8GzimV45cCS6iupx+MiPWtCnJJkuaintYe9KvSaxAtNWHqLfA2fu/ZPV6feNQhLz7vZwvEQbd4nK5W3BO1mGz/X3RbVlf/v9T/J7NlWFtqRsQBVOX3hsz8bEl+KiIWlhtPC4GtJX0zcHQt+yLgyZK+qEO6JDWS41pLkibSp57FZwGXlW3dAlwV1Q/pM4ANmbmt5NlAVdl9Y88OWJKkAetZxXU/K70G0VITpt4Cr1vrvl62zmw36BaP09WKe6L3rb21ZLdl01lvuoaxpWa5OL4WeDgzP1JbtB5YDqwuf2+tpX86Ij5C1ZrkWOC+zNwVEdsj4lSqXhfnAlf26TD6zgovSZKk+aGHPYuPAp4o29oZEc8Ch9fTO+Rpj23SDbvqjZoG3dimaQ1+mhSPsXTWpFgk9UZPai+t9JocK7mmx/etL94AvAvYGBFfL2nvoyq76yLifOBx4G0AmflQRKwDvkXVhfLC0gUS4N3sHr/vDhyjXuqLfozD2c/jkSSpKXrcs7ivQ/BdecOtLzZq6kUjnqloWoOfJsVjLJ01KRZJvdGrZrdWeklDLDO/SueLY4DTJsizCljVIf0B4ITZi07SJPVjHE5JkuaVPvQsbuXZHBH7A4cA20r6aFuesVk6LEmSGqknFddWekmSNFh9GodTmpJ6r6nx1WcOMBJJmro+9Sxubetu4K3AlzMzI+JO4EMRcWhZ73RgZY8OVZKkRhi+gY4lSdKU9HAcTkmS5pN+9Cy+FvhUuYG8jao3FJm5LSIuB+4v632gNVGjJElzlRXXkiTNYT0eh7N9X5OeDGq2zPakPK0JqnqtPhnWMJnNuK+84dYXn1984p7LZvuz4+RNkmZDP3oWZ+ZzlIrvDsvWAGsmG68kScPOimtJkuaoPozDuYepTAY1W2Z7Up7z+jQB8MUn7nxxMqxh0q+4Z3uCMCdv6g+HgpEkSdJsesmgA5AkSbNvEuNwwt7jcC6LiAMj4tXsHodzC7A9Ik4t2zy3lkeSJEmSpJ4YvqY+Uh/ZckjSEOvHOJxSzy1uawU/n87HEfHTwM21pJ8C/iPwCuA3gb8r6e/LzNtLnpXA+cAu4D2ZeWdJP5nd5fh24KLM7DjsjyRJktQEVlxLkjQH9WMcTkm9lZmPACcBRMR+wPeAzwG/Dnw0M/+wvn5EvIZqIrfjgSOBL0XEceUm1NVUY9DfQ1VxvRRvQkmSJKnBHCpEkiRJar7TgO9k5t90Wecs4KbMfD4zHwM2AaeU8ewPzsy7Syvr64Gzex6xJEkNExFrImJrRHyzlnZYRGyIiEfL30Nry1ZGxKaIeCQizqilnxwRG8uyj5ch9SjD7t1c0u+NiMW1PMvLPh6NiNbQfZK6sMW1JEmS1HzLgBtrr38nIs4FHgAuzsxngKOoWlS3bC5pL5Tn7el7iYgLqFpmMzIywtjY2IQBjSyoJuzspFu+2bZjx46+7q+pMTQljibE0KQ4JDXOdcBVVDdxW1YAd2Xm6ohYUV5fMs2eTOcDz2TmMRGxDPgwcE5EHAZcCiwBEngwItaX87ekCVhxLUmSJDVYRPwY8MvAypJ0NXA51Q/fy4ErgN+g8/BA2SV978TMa4BrAJYsWZKjo6MTxnXlDbdyxcbOPyfG3zlxvtk2NjZGtzjnSwxNiaMJMTQpDknNkplfqbeCLs4CRsvztcAYcAm1nkzAYxHR6sk0TunJBBARrZ5Md5Q8l5Vt3QJcVVpjnwFsyMxtJc8Gqsru+k1pSW2suJYkSZKa7ZeAr2XmUwCtvwAR8SfA58vLzcDRtXyLgCdL+qIO6ZIkCUYycwtAZm6JiCNK+nR6Mh0FPFG2tTMingUOr6d3yLOHqfR+gj17QA2yp0lTero0IY4mxDDoOOq98mYShxXXkiRJUrO9g1qLrIhY2PqBDfwK0Bqncz3w6Yj4CFWX5mOB+zJzV0Rsj4hTgXuBc4Er+xa9JEnDaTo9mfra+wn27AHVzx5P7ZrS06UJcTQhhkHHcd6K2158ft3Sl007DidnlCRJkhoqIn4ceBPw2VryfykTQn0D+AXg3wJk5kPAOuBbwBeAC8s4nADvBj5JNWHjd6i6M0uSJHiqTGRM+bu1pE+nJ9OLeSJif+AQYFuXbUnqwoprSZIkqaEy8x8y8/DMfLaW9q7MPDEzfzYzf7nW+prMXJWZ/zgzfzoz76ilP5CZJ5Rlv5OZHVt5SZI0D60Hlpfny4Fba+nLIuLAiHg1u3sybQG2R8SpZfzqc9vytLb1VuDL5Zx7J3B6RBwaEYcCp5c0SV04VIjmjMW1bgiSJEmSJEl1EXEj1USMr4yIzcClwGpgXUScDzwOvA2qnkwR0erJtJO9ezJdByyg6sXUull8LfCpMpHjNmBZ2da2iLgcuL+s94HWRI2SJmbFteYdK7glSZIkSZp/MvMdEyw6bYL1VwGrOqQ/AJzQIf05SsV3h2VrgDWTDlaSFdeaH6ysliRJkiRJkoaHY1xLkiRJkiRJkhrFFteSpHnD3hdq52dCkiRJkprJFteSJEmSJEmSpEax4lqSJEmSJEmS1CgOFSJNQ3vX8vHVZw4oEkmSJEmSJGnuscW1JEmSJEmSJKlRbHEtSTPgxG7S8LHczh31/6W9nyRJkqS5xRbXkiRJkiRJkqRGscW1JEmS1FARMQ5sB3YBOzNzSUQcBtwMLAbGgbdn5jNl/ZXA+WX992TmnSX9ZOA6YAFwO3BRZmav4rY1vCRJkmbKimtJmgKHGJCk3vO7di+/kJnfr71eAdyVmasjYkV5fUlEvAZYBhwPHAl8KSKOy8xdwNXABcA9VBXXS4E7+nkQkiRJ0lQ4VIgkSZI0XM4C1pbna4Gza+k3ZebzmfkYsAk4JSIWAgdn5t2llfX1tTySJElSI1lxLamjiFgTEVsj4pu1tMMiYkNEPFr+HlpbtjIiNkXEIxFxRi395IjYWJZ9PCKi38ciSdIQS+CLEfFgRFxQ0kYycwtA+XtEST8KeKKWd3NJO6o8b0+XJEmSGqsnQ4VExBrgLcDWzDyhpDV+LD5Je7gOuIqqVVaLXZMlSeqvN2TmkxFxBLAhIr7dZd1ON4ezS/reG6gqxy8AGBkZYWxsbMKdjSyAi0/c2SWcSrdtzIYdO3b0fB/DEENT4mhCDE2KQ5IkTV+vxri+Diu8AMdo1PDKzK9ExOK25LOA0fJ8LTAGXEKtazLwWES0uiaPU7omA0REq2vyUJVjSZIGJTOfLH+3RsTngFOApyJiYWZuKcOAbC2rbwaOrmVfBDxZ0hd1SO+0v2uAawCWLFmSo6OjE8Z25Q23csXGff+cGH/nxNuYDWNjY3SLsx+aEENT4mhCDL2Mo9eNtCLiQKrf0ScDTwPnZOZ4ybMceH8J5YOZ2RoySJKkOaknQ4Vk5leAbW3JjsUnDT+7JkuS1CcR8bKIOKj1HDgd+CawHlheVlsO3FqerweWRcSBEfFq4FjgvnLO3h4Rp5Yhu86t5ZE0NddRNaiqazXSOha4q7ymrZHWUuATEbFfydNqpHVsebS2eT7wTGYeA3wU+HDZ1mHApcDrqW5gXVoftk+SpLmoVy2uO9mjwqt0d4SqEuue2nqtiq0XmGSF11S6NM6myXQ/m0z3yX6bbLfOpulH3O3/z/r+6sva4+j2OZgn3RT72jV5tkznf9OEsjOsZRiaHftcLMcO3SUNvRHgc2V6iP2BT2fmFyLifmBdRJwPPA68DSAzH4qIdcC3gJ3AhaUXI8C72V2O78DeT9K09KFX4lnAZWVbtwBXlRtOZwAbMnNbybOBqrL7xtk+RkmSmqKfFdcTmXGF11S6NM6myXQ/O6+BQ4VcfOLOSXXrbJp+xN3elbX+/6sva/+/dusC25TukrOkEV2TZ8t0/jdNKNPDWoah2bHP0XJ8HQ7dJQ2tzPwu8NoO6U8Dp02QZxWwqkP6A8AJsx2jJGB2G2m92JMxM3dGxLPA4Uzcw3Ev0x2rftA36ZvWUKBJ8RhLZ02KRVJv9LP2oGcVXpL6ptU1eTV7d03+dER8hKrCq9U1eVdEbI+IU4F7qbomX9n/sKX5ybHqJUkaqOk00uprw676WPW9Ho9+X5rWUKBJ8RhLZ02KRVJv9LPi2govaYhExI1UlVuvjIjNVGPqrcauydKwm1NDdznkT381Ie6Jhu6ai8P9SBoas9lIq5Vnc0TsDxxCNX/UZnbfeG7lGZvdw5Ak6f9r7+7Dbavq+9B/f+EoIaC8SDwlQAJJiKlK1HCKWG/aQ/HlxBgxt5gcSyIm9FKN5knuJY2QPE1Nc0m0rbH1NaVCQcUgMRq4viR6ISe5aREFiwIi8ahEjyBEQQSjJsf+7h9rblxs995nn/069zmfz/OsZ8855hhz/tZcc+y19m+PNea4rEriWsILNr7ufv48m3w1GfZNG3LqLlP+rK0xxD3f1F376HQ/wMawkoO0ZvZ1bZIzklzT3V1Vf5rkd6ZuyPiMJOev/lOD/cfwTcP7M7nfy+7u3rKS94epqgMzmcLvpCRfSvIz3X37Gj092JBW5S8PCS8AGC1TdwHAEq3BIK2LkrxlmLLrnkzuP5HuvqeqfjvJh4d6/27mRo3Aijq1u784tb6S94c5O8m93f2DVbU9ySuT/MxaPTHYiDbeUB9YRceNYBQesP6mfxfc/oqfWMdIVoWpu4A1Nfvz1T74e5X9yGoP0urur2dIfM+x7eIkFy86WGAlrOT9YU5P8vJhX+9I8rqqqu6e89uMQPId6x0AALA6hlFh1yZ5TFXtGkaCvSLJ06vqk0mePqynu29JMjMq7E/y7aPC3pRkZ5JPxdRdAADsezrJ+6vqhuHeLcms+8Mkmb4/zOem2s7cB+bozH9/mAfbdPfuJPcledQqPA/YZxhxvQqM2gVgDEzdBQAAi/bU7r5juHn5B6rqEwvUXcr9YRZ175i9veH59M2z1/Nm1GO5GfYY4hhDDOsdx/RN1JcTh8Q1AAAAAPu17r5j+Hl3Vb0ryclZ2fvDzLTZVVWbkhyayVz2s+PYqxuev/ayKx+8efZCN6tebWO5GfYY4hhDDOsdx/RN1C/ZdvCS4zBVCAAAAAD7rao6uKoeMbOc5BlJbs637g+TfPv9YbZX1YFVdXy+dX+YO5PcX1WnVFVlcn+Y6TYz+zojyTXmt4aFGXENAOzzTOMFAMACNid51yTXnE1J3tbdf1JVH05yxXCvmM9muHlqd99SVTP3h9mdb78/zCVJDsrk3jAz94e5KMlbhhs53pNk+1o8MdjIJK4BAAAA2G9196eTPGGO8i9lhe4P091fz5D4BhbHVCEAADBCVXVsVf1ZVd1aVbdU1S8P5S+vqs9X1Y3D41lTbc6vqp1VdVtVPXOq/KSqumnY9prh68sAADBaRlwD7IEpBgDGYz/7nbw7ybnd/ZFh3s0bquoDw7ZXd/d/nK5cVY/N5GvHj0vyPUn+36r6oeGry29Mck6SDyZ5b5Jt+dZXl9fU9Gt4+yt+Yj1CAABgAzDiGgAARqi77+zujwzL9ye5NcnRCzQ5Pcnl3f2N7v5Mkp1JTq6qo5I8sruvHW4C9eYkz13d6AEAYHkkrgEAYOSq6rgkT0py3VD00qr6WFVdXFWHD2VHJ/ncVLNdQ9nRw/LscgAAGC1ThQAAwIhV1SFJ/ijJr3T3V6rqjUl+O0kPP1+V5BeSzDVvdS9QPtexzslkSpFs3rw5O3bsmDeuzQcl5564e/FPZA4L7X+xHnjggRXZz0aPYSxxjCGGMcWxrzLlDwBrQeJ6Bexncy0CALBGquphmSStL+vudyZJd981tf2/Jnn3sLorybFTzY9JcsdQfswc5d+muy9McmGSbNmypbdu3TpvbK+97Mq86qbl/Tlx+5nz73+xduzYkYXiXAtjiGEscYwhhjHFAQAsncQ1LJJ/UMC+TR8HxqaqKslFSW7t7t+bKj+qu+8cVn8qyc3D8lVJ3lZVv5fJzRlPSPKh7v5mVd1fVadkMtXIC5K8dq2eBwAALIXENQAAjNNTk/xckpuq6sah7NeTPL+qnpjJdB+3J/lXSdLdt1TVFUk+nmR3kpd09zeHdi9OckmSg5K8b3isu9n/NDTlAAAAMySuAYB9jhH07Au6+y8z9/zU712gzQVJLpij/Pokj1+56AAAYHVJXAPMIuEFAAAAsL6+Y70DAAAAAACAaUZcwwqbHq1rnkYAgMXzOQoAgBkS1wAA7HOmE6CXbDt4HSMBAACWwlQhS3Tcee/JTZ+/z1y4AAAAAAArzIhrAABgdEwbAgCwf5O4BgD2Cb4FBQDjNft92j+kANgTiWuASHgBAAAAjInENQAAG55/QO7bjNQEANj/SFzDCvDHMgDA2pn+7HXJtoPXMRIAAFaLxDUAALBh3fT5+/JCN3IEANjnSFwvkhG1sO+Z/YcusLEcd957cu6Ju/VjANiHHecfUwD7rQ2RuK6qbUn+c5IDkrypu1+xziEBe0k/ho1NH4aNb3/px/MNOJHwYl+wv/Rj2Jfpx7B4o09cV9UBSV6f5OlJdiX5cFVd1d0fX+1jG2UNK2M9+/G02X363BPX8uiwcY2lDyfem2GpxtSP18tCvz8ktdkI9OPFc0NXxko/hr0z+sR1kpOT7OzuTydJVV2e5PQkq9Kp/UEMq2JN+/E0fRpWxLr14UQ/hhWyrv147Jb7e+bcE3dn68qEAgvRj1eZG7+yBvRj2AvV3esdw4Kq6owk27r7Xw7rP5fkyd390qk65yQ5Z1h9TJLb1ii8I5N8cY2OtZLEvbZWI+7v6+7vXuF9rpoR92PX1NrbqLHv1/14MX14KNePF0/ca2u14t7f+/FYrocxxDGGGJJxxDGGGJLFx6Efj+P1SsYVSzKueMQyt5lY9ql+vITP1GN5TcQxrhiSjRPHvH14I4y4rjnKHpJt7+4Lk1y4NuF8S1Vd391b1vq4yyXutbVR415ho+zHG/W12ahxJxs39o0a9wraYx9O9OO9Ie61tVHjXmEr3o/Hcl7HEMcYYhhLHGOIYUxxrLB9th8n44olGVc8YpnbmGLZCyv+t/FYzoM4xhXDvhLHd6x0MKtgV5Jjp9aPSXLHOsUCLI1+DBubPgwbn34MG59+DBuffgx7YSMkrj+c5ISqOr6qHp5ke5Kr1jkmYO/ox7Cx6cOw8enHsPHpx7Dx6cewF0Y/VUh3766qlyb50yQHJLm4u29Z57BmrPn0JCtE3Gtro8a9Ykbcjzfqa7NR4042buwbNe4VMeI+nGzc10bca2ujxr1iVqkfj+W8jiGOMcSQjCOOMcSQjCeOFbOP9+NkXLEk44pHLHMbUyyLso/3Y3F8yxhiSPaBOEZ/c0YAAAAAAPYvG2GqEAAAAAAA9iMS1wAAAAAAjMp+nbiuqmOr6s+q6taquqWqfnkoP6KqPlBVnxx+Hj7V5vyq2llVt1XVM6fKT6qqm4Ztr6mqGsoPrKq3D+XXVdVxKxj/AVX1P6vq3Rss7sOq6h1V9Ynh3D9lI8ReVf/ncJ3cXFV/UFXfuRHi3tfpx+vSF/RhfXhF6cf68V7ErR+vkaraNpy3nVV13hzbazh3O6vqY1X1o4ttu4IxnDkc+2NV9T+q6glT224fXuMbq+r6pcawyDi2VtV9w7FurKrfXGzbFYzhX08d/+aq+mZVHTFsW5FzUVUXV9XdVXXzPNtX/ZpYZBxrcl1sNMvp0yscx5zv+bPqzNunViGeBa+JtTovw7EeM/Wcb6yqr1TVr8yqs2rnZq6+VQu8x85qu2J9fIFY/kNNPrd8rKreVVWHzdN2n+3nY+jHi4hh3t/BaxnHVL1/VJP3xDPWK46h3944/M778/WIo6oOrar/p6o+OsTx86sQw5I/Jyyou/fbR5KjkvzosPyIJH+V5LFJ/n2S84by85K8clh+bJKPJjkwyfFJPpXkgGHbh5I8JUkleV+SHx/KfzHJ7w/L25O8fQXj/7+SvC3Ju4f1jRL3pUn+5bD88CSHjT32JEcn+UySg4b1K5K8cOxx7w8P/Xjt49aH9eGVfujH+vEiY9aP1+iRyc2iPpXk+4fr46NJHjurzrOGc1dJTkly3WLbrmAM/zjJ4cPyj8/EMKzfnuTINToXWzP0/71tu1IxzKr/k0muWYVz8U+S/GiSm+fZvqrXxF7EserXxUZ7LKdPr0Isc77nz6ozZ59apXgWvCbW6rzM85p9Icn3rdW5matvZZ732L29vlYolmck2TQsv3KuWBbzmm7Uxxj68SJjmPd38FrGMVXvmiTvTXLGOr0mhyX5eJLvHdYfvU5x/Hq+9Rn5u5Pck+ThKxzHkj4n7OmxX4+47u47u/sjw/L9SW7N5I+i0zP5gy7Dz+cOy6cnuby7v9Hdn0myM8nJVXVUkkd297U9eTXePKvNzL7ekeS0qsmonuWoqmOS/ESSN00Vb4S4H5nJxXxRknT333X3lzdC7Ek2JTmoqjYl+a4kd2yQuPdp+vHaxq0P68OrQT/Wj/eCfrw2Tk6ys7s/3d1/l+TyTM7NtNOTvLknPpjksOHcLqbtisTQ3f+ju+8dVj+Y5JglHGfZcaxS2+Xs5/lJ/mAJx1lQd/9FJn/ozme1r4lFxbFG18VGs5w+vaIWeM8fqzU5L3M4Lcmnuvuv1+BYSebtW/O9x05b0T4+Xyzd/f7u3j2s7o99ewz9eKO9N/9Skj9KcvcqxLDYOP5Fknd292eTpLtXI5bFxNFJHjF8rj0kk/61OytoGZ8TFrRfJ66n1eSroE9Kcl2Szd19ZzJ5Y03y6KHa0Uk+N9Vs11B29LA8u/whbYZfsvcledQKhPyfkvxakv81VbYR4v7+JH+T5L/V5GvVb6qqg8cee3d/Psl/TPLZJHcmua+73z/2uPc3+vGaxK0P68OrSj9ek7j1Y/14T+Y7d4ups5i2KxXDtLMzGcUzo5O8v6puqKpzlnD8vY3jKcPXb99XVY/by7YrFUOq6ruSbMvkD/UZK3Uu9mS1r4mlWK3rYqNZTp9eNbPe82ebq0+thj1dE+t1/W7P/P+AWqtzk8z/HjttPc7RL+ShfXvavtrPx9CPl/vevGZxVNXRSX4qye+vwvEXHUeSH0pyeFXtGK7JF6xTHK9L8g8zGfRxU5Jf7u7/lbW1pOtT4jpJVR2SyYe7X+nuryxUdY6yXqB8oTZLVlXPTnJ3d9+w2CbzxLCmcQ82ZfLVgTd295OSfDWTrxzNZxSx12Qur9Mz+arx9yQ5uKp+dqEm88SwHud8v6Aff1v5Qm2WQx+ePzZ9eJn0428rX6jNcujH88emH08s5jws5fyudAyTilWnZvLH8cumip/a3T+aydeUX1JV/2QJMSw2jo9k8nX+JyR5bZI/3ou2KxXDjJ9M8t+7e3rE00qdiz1Z7Wtir6zydbHRLKdPr4o9vOfP16dWw56uiTW/fqvq4Umek+QP59i8ludmsdb62vmNTEaKXjZPlX21n4+hHy/3vXkt4/hPSV7W3d9chePvTRybkpyUyTc0n5nk31TVD61DHM9McmMmn6GfmOR1w7cw19KSrs/9PnFdVQ/L5A3zsu5+51B818xw9eHnzFD+XUmOnWp+TCb/rdiVh379Yab8IW2Gr7UemoWHzi/GU5M8p6puz+QrAP+sqt66AeKe2e+u7p75r/o7MvnjeeyxPy3JZ7r7b7r775O8M5O5m8Ye935BP17TuPVhfXhV6Mf68SLox2tnvnO3mDqLabtSMaSqfiSTqXpO7+4vzZR39x3Dz7uTvCuTr9EuxR7j6O6vdPcDw/J7kzysqo5c7HNYiRimfNsozRU8F3uy2tfEoq3BdbHRLKdPr7h53vMftECfWnGLuCbW/PrNJOH6ke6+a/aGtTw3g/neY6et5bVzVpJnJzmzu+dMeO3D/XwM/XhZ781rHMeWJJcPn9PPSPKGqnruOsSxK8mfdPdXu/uLSf4iyRPWIY6fz2TKku7unZncN+aHVziOPVnS9blfJ66HuV0uSnJrd//e1Karkpw1LJ+V5Mqp8u01ueP88UlOSPKh4Ssz91fVKcM+XzCrzcy+zsjkRinL+o9Xd5/f3cd093GZfDi9prt/duxxD7F/IcnnquoxQ9FpmUxUP/bYP5vklKr6ruF4p2UyH9vY497n6cdrHrc+rA+vOP1YP14k/XjtfDjJCVV1fE1G/m3P5NxMuyrJC2rilEymbrlzkW1XJIaq+t5M/oHxc939V1PlB1fVI2aWM7mh181LiGGxcfyD4VpKVZ2cyd9YX1pM25WKYTj2oUn+ab51Pa/0udiT1b4mFmWNrouNZjl9ekUt8J4/XWe+PrXSsSzmmliT8zLLvPPUr9W5mTLfe+y0NenjVbUtk9G7z+nuv52nzr7cz8fQj5f83rzC9hhHdx/f3ccNn9PfkeQXu/uP1zqOTPrMj1XVpppM5/XkTD6/rnUcn83ks3OqanOSxyT59ArHsSdLuz57he9muZEeSf63TIalfyyTIfM3ZnKXy0cluTrJJ4efR0y1+Y1M7tZ5W4Y70A/lWzL5hfipTOaOqaH8OzP5is/OTO5g//0r/By2Zrir8EaJO5OvJVw/nPc/TnL4Rog9yW8l+cRwzLckOXAjxL2vP/TjdekL+rA+vKIP/Vg/3ou49eM1egx98K+Gc/QbQ9mLkrxoWK4krx+235Rky0JtVymGNyW5N9/6vXH9UP79ST46PG5ZTgyLjOOlw3E+msmNqP7xWp+LYf2FmdyQdLrdip2LTBJpdyb5+0xGTZ291tfEIuNYk+tioz2W06dXOI753vMX1adWOJY5r4n1OC9TMX1XJonoQ6fK1uTczNO35nyPzWS6gfcudH2tQiw7M5kfd+a6+f3Zsezr/XwM/XgRMcz5O3it45hV95IkZ6xXHEn+dSaDRG7OZHqk9bg2vifJ+4fr4uYkP7sKMSz5c8JCj5kP6QAAAAAAMAr79VQhAAAAAACMj8Q1AAAAAACjInENAAAAAMCoSFwDAAAAADAqEtcAAABsKFV1cVXdXVU3L6Lu91XV1VX1saraUVXHrEWMAMDySFwDAACw0VySZNsi6/7HJG/u7h9J8u+S/O5qBQUArByJawAAADaU7v6LJPdMl1XVD1TVn1TVDVX1/1XVDw+bHpvk6mH5z5KcvoahAgBLJHENAADAvuDCJL/U3Scl+dUkbxjKP5rknw/LP5XkEVX1qHWIDwDYC5vWOwAAAABYjqo6JMk/TvKHVTVTfODw81eTvK6qXpjkL5J8PsnutY4RANg7EtcAAABsdN+R5Mvd/cTZG7r7jiT/e/Jggvufd/d9axseALC3TBUCAADAhtbdX0nymap6XpLUxBOG5SOrauZv3/OTXLxOYQIAe0HiGgAAgA2lqv4gybVJHlNVu6rq7CRnJjm7qj6a5JZ86yaMW5PcVlV/lWRzkgvWIWQAYC9Vd693DAAAAAAA8CAjrgEAAAAAGBWJawAAAAAARkXiGgAAAACAUZG4BgAAAABgVCSuAQAAAAAYFYlrAAAAAABGReIaAAAAAIBRkbgGAAAAAGBUJK4BAAAAABgVietlqqrfrapfWe84ZquqS6rq/17H499eVU9bhf1urapde1H/hVX1lysdx9T+f6Sq/sdq7R8AAAAA9kcS18tQVd+d5AVJ/suwvrWqdqxDHHudnK2qXmS946rq9iUFto+qqpdX1cuTpLs/luTLVfWT6xsVAAAAAOw7JK6X54VJ3tvdX1vvQMagqjavdwzLUVVHVNXDltD0siT/aqXjAQAAAID9lcT18vx4kj+fb2NVdVX9YlV9sqrur6rfrqofqKprq+orVXVFVT18qv7/UVU7q+qeqrqqqr5n1r5eNOzr3qp6fU38wyS/n+QpVfVAVX15KoTDq+o9w7Gvq6ofWOkTUFWHVdWLq+pDSS6Zp87Jw3P+clXdWVWvm/W89+o8DW1+vaq+OExJcuZU+aOGc/eVIaYfmNXuP1fV54btN1TVj01tfnqSXVX1qqp6/F6chh1JTquqA/eiDQAAAAAwD4nr5TkxyW0zK929o7u3zqqzLclJSU5J8mtJLkxyZpJjkzw+yfOTpKr+WZLfTfLTSY5K8tdJLp+1r2cn+UdJnjDUe2Z335rkRUmu7e5DuvuwqfrPT/JbSQ5PsjPJBVOx1mKeYHff3t3HTZdV1XdU1dOr6m1DnM9I8jtJnjPPbr6Z5P9McmSSpyQ5LckvzqqzqPM0+AfDvo5OclaSC6vqMcO21yf5eibn8BeGx7QPJ3likiOSvC3JH1bVdw7P9e1DbP8ryfur6sNDQv3wWefk5d398qn1zyf5+ySPCQAAAACwbBLXy3NYkvv3UOeV3f2V7r4lyc1J3t/dn+7u+5K8L8mThnpnJrm4uz/S3d9Icn4mo6iPm9rXK7r7y9392SR/lkkCdiHv7O4PdffuTKaz2FP9Paqqlya5Pckrk3wwyQ9090919x9399/P1aa7b+juD3b37u6+PZM5wf/prGqLPU8z/k13f6O7/zzJe5L8dFUdkOSfJ/nN7v5qd9+c5NJZsby1u780xPKqJAdmKuHc3Td397/OJGH+b5NsTfKZqrq8qh65wKm5P5PrAQAAAABYJonr5bk3ySP2UOeuqeWvzbF+yLD8PZmMXk6SdPcDSb6UyajiGV+YWv7bqbbz2dv6i3F8JiO4b0zysUxiXFBV/VBVvbuqvlBVX8lkdPaRs6ot9jwlyb3d/dWp9b/O5Px9d5JNST43a9t0LOdW1a1Vdd8wrcqhc8SS7v5mJgn0jya5J5NR3wvNf/2IJF9eYDsAAAAAsEgS18vzsSQ/tEL7uiPJ982sVNXBSR6V5POLaNsrFMOeD9R9bpLvT3JTktdkMhr5t6vqhAWavTHJJ5Kc0N2PTPLrSRY1Vck8Dh/Oz4zvzeT8/U2S3ZmMlp7eliQZ5rN+WSbTrBw+TKty33QsVXVIVb2wqq5J8pFM/nHwM939+O6eM0k/zEX+8ExNGwMAAAAALJ3E9fK8N98+5cVSvS3Jz1fVE4eb/P1OkuuGqTX25K4kx8y+geFiVdXLq2rHYut3999096u7+0cymZrjsCTXVtXF8zR5RJKvJHmgqn44yYuXEucsv1VVDx+S0c9O8ofDKOl3Jnl5VX1XVT02kzmwp+PYnUmCe1NV/WaSB6f/qKptmSTAfyaT6UyO7u5f7O4P7yGWrUmuGaZ4AQAAAACWSeJ6ed6c5FlVddByd9TdVyf5N0n+KMmdSX4gyfZFNr8myS1JvlBVX1zC4Y9N8t+X0G5m/upfymSqjt+fp9qvJvkXmcwD/V+TvH0px5ryhUymabkjk7m7X9Tdnxi2vTSTaUW+kOSSJP9tqt2fZjJf9l9lMoXI1/PQaUVuS/LD3f3j3f32vUhEn5n5nzsAAAAAsJeqe81mmdgnVdXvJLm7u//TeseyVFV1Y5LT5psKg/lV1YlJLuzup6x3LAAAAACwr5C4BgAAAABgVEwVAgAAAADAqEhcAwAAAAAwKhLXAAAAAACMisQ1AAAAAACjsmm9A1hpRx55ZB933HHrdvyvfvWrOfjgg9ft+PMZa1zJeGMba1xJcsMNN3yxu797veMAAAAAgNWwzyWujzvuuFx//fXrdvwdO3Zk69at63b8+Yw1rmS8sY01riSpqr9e7xgAAAAAYLWYKgQAAAAAgFGRuAYAAAAAYFQkrgEAAAAAGJVlJa6r6vaquqmqbqyq64eyI6rqA1X1yeHn4VP1z6+qnVV1W1U9c6r8pGE/O6vqNVVVQ/mBVfX2ofy6qjpuOfECAAAAADB+KzHi+tTufmJ3bxnWz0tydXefkOTqYT1V9dgk25M8Lsm2JG+oqgOGNm9Mck6SE4bHtqH87CT3dvcPJnl1kleuQLwAAAAAAIzYakwVcnqSS4flS5M8d6r88u7+Rnd/JsnOJCdX1VFJHtnd13Z3J3nzrDYz+3pHktNmRmMDAAAAALBv2rTM9p3k/VXVSf5Ld1+YZHN335kk3X1nVT16qHt0kg9Otd01lP39sDy7fKbN54Z97a6q+5I8KskXlxk3a+S4897zkPXbX/ET6xQJAAAAALBRLDdx/dTuvmNITn+gqj6xQN25Rkr3AuULtXnojqvOyWSqkWzevDk7duxYMOjV9MADD6zr8eezXnGde+Luh6zPFYNzBgAAAABMW1biurvvGH7eXVXvSnJykruq6qhhtPVRSe4equ9KcuxU82OS3DGUHzNH+XSbXVW1KcmhSe6ZI44Lk1yYJFu2bOmtW7cu52kty44dOzLf8ddz9PFrL7syr/rLr675cV84+zmfufXb6ix0zpZj+nwv5TkvNa7lHhcAAAAA9ndLnuO6qg6uqkfMLCd5RpKbk1yV5Kyh2llJrhyWr0qyvaoOrKrjM7kJ44eGaUXur6pThvmrXzCrzcy+zkhyzTAPNgAAAAAA+6jljLjenORdw70SNyV5W3f/SVV9OMkVVXV2ks8meV6SdPctVXVFko8n2Z3kJd39zWFfL05ySZKDkrxveCTJRUneUlU7MxlpvX0Z8QIAAAAAsAEsOXHd3Z9O8oQ5yr+U5LR52lyQ5II5yq9P8vg5yr+eIfENAAAAAMD+YclThQAAAAAAwGpY1s0ZN6ql3DxvPW+suF43+9vfn/O5J+7O1jU7MgAAAAAww4hrAAAAAABGReIaAAAAAIBR2S+nCoExWq/pUQAAAABgbIy4BgAAAABgVCSuAQAAAAAYFYlrAAAAAABGReIaAAAAAIBRkbgGAAAAAGBUJK4BAAAAABgViWsAAAAAAEZF4hoAAAAAgFGRuAYAAAAAYFQkrgEAAAAAGBWJawAAAAAARkXiGgAAAACAUZG4BgAAAABgVCSuAQAAAAAYFYlrAAAAAABGReIaAAAAAIBRkbgGAAAAAGBUJK4BAAAAABgViWsAAAAAAEZF4hoAAAAAgFGRuAYAAAAAYFSWnbiuqgOq6n9W1buH9SOq6gNV9cnh5+FTdc+vqp1VdVtVPXOq/KSqumnY9pqqqqH8wKp6+1B+XVUdt9x4AQAAAAAYt5UYcf3LSW6dWj8vydXdfUKSq4f1VNVjk2xP8rgk25K8oaoOGNq8Mck5SU4YHtuG8rOT3NvdP5jk1UleuQLxAgAAAAAwYstKXFfVMUl+IsmbpopPT3LpsHxpkudOlV/e3d/o7s8k2Znk5Ko6Kskju/va7u4kb57VZmZf70hy2sxobAAAAAAA9k01yRUvsXHVO5L8bpJHJPnV7n52VX25uw+bqnNvdx9eVa9L8sHufutQflGS9yW5PckruvtpQ/mPJXnZsK+bk2zr7l3Dtk8leXJ3f3FWHOdkMmI7mzdvPunyyy9fMO6bPn/fg8snHn3oop7rdJuF2j3wwAM55JBDlrWPhdotts1sd99zX+762tKPu9RjL2Yfc52zlXjOy32dNx+UPPqI5T3nvYl9b9qdeuqpN3T3lr0ODgAAAAA2gE1LbVhVz05yd3ffUFVbF9NkjrJeoHyhNg8t6L4wyYVJsmXLlt66deFwXnjeex5cvv3MhevO1Wahdjt27Mh8x1/sPhZqt9g2s732sivzqps2Lfm4Sz32YvYx1zlbiee83Nf53BN356f3cC2t1HGX0w4AAAAA9jVLTlwneWqS51TVs5J8Z5JHVtVbk9xVVUd1953DNCB3D/V3JTl2qv0xSe4Yyo+Zo3y6za6q2pTk0CT3LCNmAAAAAABGbslzXHf3+d19THcfl8lNF6/p7p9NclWSs4ZqZyW5cli+Ksn2qjqwqo7P5CaMH+ruO5PcX1WnDPNXv2BWm5l9nTEcY+lzmwAAAAAAMHrLGXE9n1ckuaKqzk7y2STPS5LuvqWqrkjy8SS7k7yku785tHlxkkuSHJTJvNfvG8ovSvKWqtqZyUjr7asQLwAAAAAAI7Iiievu3pFkx7D8pSSnzVPvgiQXzFF+fZLHz1H+9QyJbwAAAAAA9g9LnioEAAAAAABWg8Q1AAAAAACjInENAAAAAMCoSFwDAAAAADAqEtcAAAAAAIyKxDUAAAAAAKMicQ0AAAAAwKhIXAMAAAAAMCoS1wAAAAAAjIrENQAAAAAAoyJxDQAAAADAqEhcAwAAAAAwKhLXAAAAAACMisQ1AAAAAACjInENAAAAAMCoSFwDAAAAADAqEtcAAAAAAIyKxDUAAAAAAKMicQ0AAAAAwKhIXAMAAAAAMCoS1wAAAAAAjIrENQAAAAAAoyJxDQAAAADAqEhcAwAAAAAwKhLXAAAAAACMypIT11X1nVX1oar6aFXdUlW/NZQfUVUfqKpPDj8Pn2pzflXtrKrbquqZU+UnVdVNw7bXVFUN5QdW1duH8uuq6rhlPFcAAAAAADaA5Yy4/kaSf9bdT0jyxCTbquqUJOclubq7T0hy9bCeqnpsku1JHpdkW5I3VNUBw77emOScJCcMj21D+dlJ7u3uH0zy6iSvXEa8AAAAAABsAEtOXPfEA8Pqw4ZHJzk9yaVD+aVJnjssn57k8u7+Rnd/JsnOJCdX1VFJHtnd13Z3J3nzrDYz+3pHktNmRmMDAAAAALBvWtYc11V1QFXdmOTuJB/o7uuSbO7uO5Nk+PnoofrRST431XzXUHb0sDy7/CFtunt3kvuSPGo5MQMAAAAAMG41GeS8zJ1UHZbkXUl+KclfdvdhU9vu7e7Dq+r1Sa7t7rcO5RcleW+Szyb53e5+2lD+Y0l+rbt/sqpuSfLM7t41bPtUkpO7+0uzjn9OJlONZPPmzSddfvnlC8Z70+fve3D5xKMPXdRznG6zULsHHngghxxyyLL2sVC7xbaZ7e577stdX1v6cZd67MXsY65zthLPebmv8+aDkkcfsbznvDex7027U0899Ybu3rLXwQEAAADABrBpJXbS3V+uqh2ZzE19V1Ud1d13DtOA3D1U25Xk2KlmxyS5Yyg/Zo7y6Ta7qmpTkkOT3DPH8S9McmGSbNmypbdu3bpgvC887z0PLt9+5sJ152qzULsdO3ZkvuMvdh8LtVtsm9lee9mVedVNm5Z83KUeezH7mOucrcRzXu7rfO6Ju/PTe7iWVuq4y2kHAAAAAPuaJU8VUlXfPYy0TlUdlORpST6R5KokZw3Vzkpy5bB8VZLtVXVgVR2fyU0YPzRMJ3J/VZ0yzF/9glltZvZ1RpJreiWGiAMAAAAAMFrLGXF9VJJLq+qATBLgV3T3u6vq2iRXVNXZmUwD8rwk6e5bquqKJB9PsjvJS7r7m8O+XpzkkiQHJXnf8EiSi5K8pap2ZjLSevsy4gUAAAAAYANYcuK6uz+W5ElzlH8pyWnztLkgyQVzlF+f5PFzlH89Q+IbAAAAAID9w5KnCgEAAAAAgNUgcQ0AAAAAwKhIXAMAAAAAMCoS1wAAAAAAjIrENQAAAAAAoyJxDQAAAADAqEhcAwAAAAAwKhLXAAAAAACMisQ1AAAAAACjInENAAAAAMCoSFwDAAAAADAqEtcAAAAAAIyKxDUAAAAAAKMicQ0AAAAAwKhIXAMAAAAAMCoS1wAAAAAAjIrENQAAAAAAoyJxDQAAAADAqEhcAwAAAAAwKhLXAAAAAACMisQ1AAAAAACjInENAAAAAMCoSFwDAAAAADAqEtcAAAAAAIyKxDUAAAAAAKOy5MR1VR1bVX9WVbdW1S1V9ctD+RFV9YGq+uTw8/CpNudX1c6quq2qnjlVflJV3TRse01V1VB+YFW9fSi/rqqOW8ZzBQAAAABgA1jOiOvdSc7t7n+Y5JQkL6mqxyY5L8nV3X1CkquH9Qzbtid5XJJtSd5QVQcM+3pjknOSnDA8tg3lZye5t7t/MMmrk7xyGfECAAAAALABLDlx3d13dvdHhuX7k9ya5Ogkpye5dKh2aZLnDsunJ7m8u7/R3Z9JsjPJyVV1VJJHdve13d1J3jyrzcy+3pHktJnR2AAAAAAA7JtWZI7rYQqPJyW5Lsnm7r4zmSS3kzx6qHZ0ks9NNds1lB09LM8uf0ib7t6d5L4kj1qJmAEAAAAAGKeaDHJexg6qDkny50ku6O53VtWXu/uwqe33dvfhVfX6JNd291uH8ouSvDfJZ5P8bnc/bSj/sSS/1t0/WVW3JHlmd+8atn0qycnd/aVZMZyTyVQj2bx580mXX375gjHf9Pn7Hlw+8ehDF/U8p9ss1O6BBx7IIYccsqx9LNRusW1mu/ue+3LX15Z+3KUeezH7mOucrcRzXu7rvPmg5NFHLO85703se9Pu1FNPvaG7t+x1cAAAAACwAWxaTuOqeliSP0pyWXe/cyi+q6qO6u47h2lA7h7KdyU5dqr5MUnuGMqPmaN8us2uqtqU5NAk98yOo7svTHJhkmzZsqW3bt26YNwvPO89Dy7ffubCdedqs1C7HTt2ZL7jL3YfC7VbbJvZXnvZlXnVTZuWfNylHnsx+5jrnK3Ec17u63zuibvz03u4llbquMtpBwAAAAD7miVPFTLMNX1Rklu7+/emNl2V5Kxh+awkV06Vb6+qA6vq+ExuwvihYTqR+6vqlGGfL5jVZmZfZyS5ppc7RBwAAAAAgFFbzojrpyb5uSQ3VdWNQ9mvJ3lFkiuq6uxMpgF5XpJ09y1VdUWSjyfZneQl3f3Nod2Lk1yS5KAk7xseySQx/paq2pnJSOvty4gXAAAAAIANYMmJ6+7+yyQ1z+bT5mlzQZIL5ii/Psnj5yj/eobENwAAAAAA+4clTxUCAAAAAACrQeIaAAAAAIBRkbgGAAAAAGBUJK4BAAAAABgViWsAAAAAAEZF4hoAAAAAgFGRuAYAAAAAYFQkrgEAAAAAGBWJawAAAAAARkXiGgAAAACAUZG4BgAAAABgVCSuAQAAAAAYFYlrAAAAAABGReIaAAAAAIBRkbgGAAAAAGBUJK4BAAAAABgViWsAAAAAAEZF4hoAAAAAgFGRuAYAAAAAYFQkrgEAAAAAGBWJawAAAAAARkXiGgAAAACAUZG4BgAAAABgVCSuAQAAAAAYFYlrAAAAAABGReIaAAAAAIBRWVbiuqourqq7q+rmqbIjquoDVfXJ4efhU9vOr6qdVXVbVT1zqvykqrpp2Paaqqqh/MCqevtQfl1VHbeceAEAAAAAGL/ljri+JMm2WWXnJbm6u09IcvWwnqp6bJLtSR43tHlDVR0wtHljknOSnDA8ZvZ5dpJ7u/sHk7w6ySuXGS8AAAAAACO3rMR1d/9FkntmFZ+e5NJh+dIkz50qv7y7v9Hdn0myM8nJVXVUkkd297Xd3UnePKvNzL7ekeS0mdHYAAAAAADsm2qSK17GDibTd7y7ux8/rH+5uw+b2n5vdx9eVa9L8sHufutQflGS9yW5PckruvtpQ/mPJXlZdz97mIJkW3fvGrZ9KsmTu/uLs2I4J5MR29m8efNJl19++YIx3/T5+x5cPvHoQxf1PKfbLNTugQceyCGHHLKsfSzUbrFtZrv7nvty19eWftylHnsx+5jrnK3Ec17u67z5oOTRRyzvOe9N7HvT7tRTT72hu7fsdXAAAAAAsAFsWsNjzTVSuhcoX6jNQwu6L0xyYZJs2bKlt27dumAgLzzvPQ8u337mwnXnarNQux07dmS+4y92Hwu1W2yb2V572ZV51U2blnzcpR57MfuY65ytxHNe7ut87om789N7uJZW6rjLaQcAAAAA+5rlznE9l7uG6T8y/Lx7KN+V5NipesckuWMoP2aO8oe0qapNSQ7Nt09NAgAAAADAPmQ1EtdXJTlrWD4ryZVT5dur6sCqOj6TmzB+qLvvTHJ/VZ0yzF/9glltZvZ1RpJrerlzmwAAAAAAMGrLmiqkqv4gydYkR1bVriT/NskrklxRVWcn+WyS5yVJd99SVVck+XiS3Ule0t3fHHb14iSXJDkok3mv3zeUX5TkLVW1M5OR1tuXEy8AAAAAAOO3rMR1dz9/nk2nzVP/giQXzFF+fZLHz1H+9QyJbwAAAAAA9g+rMVUIAAAAAAAsmcQ1AAAAAACjInENAAAAAMCoSFwDAAAAADAqEtcAAAAAAIyKxDUAAAAAAKMicQ0AAAAAwKhIXAMAAAAAMCoS1wAAAAAAjIrENQAAAAAAoyJxDQAAAADAqEhcAwAAAAAwKhLXAAAAAACMisQ1AAAAAACjInENAAAAAMCoSFwDAAAAADAqEtcAAAAAAIyKxDUAAAAAAKMicQ0AAAAAwKhIXAMAAAAAMCoS1wAAAAAAjIrENQAAAAAAoyJxDQAAAADAqEhcAwAAAAAwKhLXAAAAAACMyoZIXFfVtqq6rap2VtV56x0PAAAAAACrZ/SJ66o6IMnrk/x4kscmeX5VPXZ9owIAAAAAYLWMPnGd5OQkO7v70939d0kuT3L6OscEAAAAAMAq2QiJ66OTfG5qfddQBgAAAADAPqi6e71jWFBVPS/JM7v7Xw7rP5fk5O7+pak65yQ5Z1h9TJLb1jzQbzkyyRfX8fjzGWtcyXhjG2tcSfKY7n7EegcBAAAAAKth03oHsAi7khw7tX5MkjumK3T3hUkuXMug5lNV13f3lvWOY7axxpWMN7axxpVMYlvvGAAAAABgtWyEqUI+nOSEqjq+qh6eZHuSq9Y5JgAAAAAAVsnoR1x39+6qemmSP01yQJKLu/uWdQ4LAAAAAIBVMvrEdZJ093uTvHe941ikUUxZMoexxpWMN7axxpWMOzYAAAAAWJbR35wRAAAAAID9y0aY4xoAAAAAgP2IxPUSVNWxVfVnVXVrVd1SVb88R52tVXVfVd04PH5zjWK7vapuGo55/Rzbq6peU1U7q+pjVfWjaxTXY6bOxY1V9ZWq+pVZddbknFXVxVV1d1XdPFV2RFV9oKo+Ofw8fJ6226rqtuH8nbdGsf2HqvrE8Hq9q6oOm6ftgq89AAAAAGwUpgpZgqo6KslR3f2RqnpEkhuSPLe7Pz5VZ2uSX+3uZ69xbLcn2dLdX5xn+7OS/FKSZyV5cpL/3N1PXrsIk6o6IMnnkzy5u/96qnxr1uCcVdU/SfJAkjd39+OHsn+f5J7ufsWQkD68u182R9x/leTpSXYl+XCS50+/7qsU2zOSXDPcqPSVSTI7tqHe7VngtQcAAACAjcKI6yXo7ju7+yPD8v1Jbk1y9PpGtWinZ5IU7e7+YJLDhkT8Wjotyaemk9Zrqbv/Isk9s4pPT3LpsHxpkufO0fTkJDu7+9Pd/XdJLh/arWps3f3+7t49rH4wyTEreUwAAAAAGBuJ62WqquOSPCnJdXNsfkpVfbSq3ldVj1ujkDrJ+6vqhqo6Z47tRyf53NT6rqx90n17kj+YZ9t6nLMk2dzddyaTf0wkefQcdcZw7n4hyfvm2ban1x4AAAAANoRN6x3ARlZVhyT5oyS/0t1fmbX5I0m+r7sfGKbn+OMkJ6xBWE/t7juq6tFJPlBVnxhG8T4Y9hxt1my+mKp6eJLnJDl/js3rdc4Wa73P3W8k2Z3ksnmq7Om1BwAAAIANwYjrJaqqh2WStL6su985e3t3f6W7HxiW35vkYVV15GrH1d13DD/vTvKuTKa3mLYrybFT68ckuWO145ry40k+0t13zd6wXudscNfMlCnDz7vnqLNu566qzkry7CRn9jwT0y/itQcAAACADUHiegmqqpJclOTW7v69eer8g6FequrkTM71l1Y5roOHm0Wmqg5O8owkN8+qdlWSF9TEKUnum5kiY408P/NME7Ie52zKVUnOGpbPSnLlHHU+nOSEqjp+GDm+fWi3qqpqW5KXJXlOd//tPHUW89oDAAAAwIZgqpCleWqSn0tyU1XdOJT9epLvTZLu/v0kZyR5cVXtTvK1JNvnGym7gjYnedeQ+92U5G3d/SdV9aKpuN6b5FlJdib52yQ/v8oxPaiqvivJ05P8q6my6djW5JxV1R8k2ZrkyKraleTfJnlFkiuq6uwkn03yvKHu9yR5U3c/q7t3V9VLk/xpkgOSXNzdt6xBbOcnOTCT6T+S5IPd/aLp2DLPa7+SsQEAAADAWqnVz6UCAAAAAMDimSoEAAAAAIBRkbgGAAAAAGBUJK4BAAAAABgViWsAAAAAAEZF4hoAAAAAgFGRuAYAAAAAYFQkrgEAAAAAGBWJawAAAAAARuX/Bz/3h1y1PHzwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1800x1800 with 36 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "threeday_features_full.hist(bins = 50, figsize = (25, 25))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-Day Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 352107 entries, 0 to 352106\n",
      "Data columns (total 31 columns):\n",
      " #   Column                        Non-Null Count   Dtype  \n",
      "---  ------                        --------------   -----  \n",
      " 0   ('cwat', 'amin')              352107 non-null  float64\n",
      " 1   ('cwat', 'amax')              352107 non-null  float64\n",
      " 2   ('cwat', 'mean')              352107 non-null  float64\n",
      " 3   ('cwat', 'var')               352107 non-null  float64\n",
      " 4   ('r', 'amin')                 352107 non-null  float64\n",
      " 5   ('r', 'amax')                 352107 non-null  float64\n",
      " 6   ('r', 'mean')                 352107 non-null  float64\n",
      " 7   ('r', 'var')                  352107 non-null  float64\n",
      " 8   ('tozne', 'amin')             352107 non-null  float64\n",
      " 9   ('tozne', 'amax')             352107 non-null  float64\n",
      " 10  ('tozne', 'mean')             352107 non-null  float64\n",
      " 11  ('tozne', 'var')              352107 non-null  float64\n",
      " 12  ('gh', 'amin')                352107 non-null  float64\n",
      " 13  ('gh', 'amax')                352107 non-null  float64\n",
      " 14  ('gh', 'mean')                352107 non-null  float64\n",
      " 15  ('gh', 'var')                 352107 non-null  float64\n",
      " 16  ('pwat', 'amin')              352107 non-null  float64\n",
      " 17  ('pwat', 'amax')              352107 non-null  float64\n",
      " 18  ('pwat', 'mean')              352107 non-null  float64\n",
      " 19  ('pwat', 'var')               352107 non-null  float64\n",
      " 20  ('paramId_0', 'amin')         352107 non-null  float64\n",
      " 21  ('paramId_0', 'amax')         352107 non-null  float64\n",
      " 22  ('paramId_0', 'mean')         352107 non-null  float64\n",
      " 23  ('paramId_0', 'var')          352107 non-null  float64\n",
      " 24  ('pres', 'amin')              90895 non-null   float64\n",
      " 25  ('pres', 'amax')              90895 non-null   float64\n",
      " 26  ('pres', 'mean')              90895 non-null   float64\n",
      " 27  ('pres', 'var')               49916 non-null   float64\n",
      " 28  ('pres', 'count')             352107 non-null  int64  \n",
      " 29  ('macro_season', '<lambda>')  352107 non-null  int64  \n",
      " 30  ('month', '<lambda>')         352107 non-null  int64  \n",
      "dtypes: float64(28), int64(3)\n",
      "memory usage: 83.3 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 352107 entries, 0 to 352106\n",
      "Data columns (total 27 columns):\n",
      " #   Column                        Non-Null Count   Dtype  \n",
      "---  ------                        --------------   -----  \n",
      " 0   ('cwat', 'amin')              352107 non-null  float64\n",
      " 1   ('cwat', 'amax')              352107 non-null  float64\n",
      " 2   ('cwat', 'mean')              352107 non-null  float64\n",
      " 3   ('cwat', 'var')               352107 non-null  float64\n",
      " 4   ('r', 'amin')                 352107 non-null  float64\n",
      " 5   ('r', 'amax')                 352107 non-null  float64\n",
      " 6   ('r', 'mean')                 352107 non-null  float64\n",
      " 7   ('r', 'var')                  352107 non-null  float64\n",
      " 8   ('tozne', 'amin')             352107 non-null  float64\n",
      " 9   ('tozne', 'amax')             352107 non-null  float64\n",
      " 10  ('tozne', 'mean')             352107 non-null  float64\n",
      " 11  ('tozne', 'var')              352107 non-null  float64\n",
      " 12  ('gh', 'amin')                352107 non-null  float64\n",
      " 13  ('gh', 'amax')                352107 non-null  float64\n",
      " 14  ('gh', 'mean')                352107 non-null  float64\n",
      " 15  ('gh', 'var')                 352107 non-null  float64\n",
      " 16  ('pwat', 'amin')              352107 non-null  float64\n",
      " 17  ('pwat', 'amax')              352107 non-null  float64\n",
      " 18  ('pwat', 'mean')              352107 non-null  float64\n",
      " 19  ('pwat', 'var')               352107 non-null  float64\n",
      " 20  ('paramId_0', 'amin')         352107 non-null  float64\n",
      " 21  ('paramId_0', 'amax')         352107 non-null  float64\n",
      " 22  ('paramId_0', 'mean')         352107 non-null  float64\n",
      " 23  ('paramId_0', 'var')          352107 non-null  float64\n",
      " 24  ('pres', 'count')             352107 non-null  int64  \n",
      " 25  ('macro_season', '<lambda>')  352107 non-null  int64  \n",
      " 26  ('month', '<lambda>')         352107 non-null  int64  \n",
      "dtypes: float64(24), int64(3)\n",
      "memory usage: 72.5 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    312927\n",
       "1     39180\n",
       "Name: fire, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fiveday_features_full = fiveday_df.iloc[:, 9:40]\n",
    "fiveday_features_no_null = fiveday_df.iloc[:,np.r_[9:33, 37:40]]\n",
    "fiveday_features_full.info()\n",
    "fiveday_features_no_null.info()\n",
    "fiveday_target = fiveday_df[\"fire\"]\n",
    "fiveday_target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x1800 with 36 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fiveday_features_full.hist(bins = 50, figsize = (25, 25))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10-Day Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 176295 entries, 0 to 176294\n",
      "Data columns (total 31 columns):\n",
      " #   Column                        Non-Null Count   Dtype  \n",
      "---  ------                        --------------   -----  \n",
      " 0   ('cwat', 'amin')              176295 non-null  float64\n",
      " 1   ('cwat', 'amax')              176295 non-null  float64\n",
      " 2   ('cwat', 'mean')              176295 non-null  float64\n",
      " 3   ('cwat', 'var')               176295 non-null  float64\n",
      " 4   ('r', 'amin')                 176295 non-null  float64\n",
      " 5   ('r', 'amax')                 176295 non-null  float64\n",
      " 6   ('r', 'mean')                 176295 non-null  float64\n",
      " 7   ('r', 'var')                  176295 non-null  float64\n",
      " 8   ('tozne', 'amin')             176295 non-null  float64\n",
      " 9   ('tozne', 'amax')             176295 non-null  float64\n",
      " 10  ('tozne', 'mean')             176295 non-null  float64\n",
      " 11  ('tozne', 'var')              176295 non-null  float64\n",
      " 12  ('gh', 'amin')                176295 non-null  float64\n",
      " 13  ('gh', 'amax')                176295 non-null  float64\n",
      " 14  ('gh', 'mean')                176295 non-null  float64\n",
      " 15  ('gh', 'var')                 176295 non-null  float64\n",
      " 16  ('pwat', 'amin')              176295 non-null  float64\n",
      " 17  ('pwat', 'amax')              176295 non-null  float64\n",
      " 18  ('pwat', 'mean')              176295 non-null  float64\n",
      " 19  ('pwat', 'var')               176295 non-null  float64\n",
      " 20  ('paramId_0', 'amin')         176295 non-null  float64\n",
      " 21  ('paramId_0', 'amax')         176295 non-null  float64\n",
      " 22  ('paramId_0', 'mean')         176295 non-null  float64\n",
      " 23  ('paramId_0', 'var')          176295 non-null  float64\n",
      " 24  ('pres', 'amin')              70527 non-null   float64\n",
      " 25  ('pres', 'amax')              70527 non-null   float64\n",
      " 26  ('pres', 'mean')              70527 non-null   float64\n",
      " 27  ('pres', 'var')               44410 non-null   float64\n",
      " 28  ('pres', 'count')             176295 non-null  int64  \n",
      " 29  ('macro_season', '<lambda>')  176295 non-null  int64  \n",
      " 30  ('month', '<lambda>')         176295 non-null  int64  \n",
      "dtypes: float64(28), int64(3)\n",
      "memory usage: 41.7 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 176295 entries, 0 to 176294\n",
      "Data columns (total 27 columns):\n",
      " #   Column                        Non-Null Count   Dtype  \n",
      "---  ------                        --------------   -----  \n",
      " 0   ('cwat', 'amin')              176295 non-null  float64\n",
      " 1   ('cwat', 'amax')              176295 non-null  float64\n",
      " 2   ('cwat', 'mean')              176295 non-null  float64\n",
      " 3   ('cwat', 'var')               176295 non-null  float64\n",
      " 4   ('r', 'amin')                 176295 non-null  float64\n",
      " 5   ('r', 'amax')                 176295 non-null  float64\n",
      " 6   ('r', 'mean')                 176295 non-null  float64\n",
      " 7   ('r', 'var')                  176295 non-null  float64\n",
      " 8   ('tozne', 'amin')             176295 non-null  float64\n",
      " 9   ('tozne', 'amax')             176295 non-null  float64\n",
      " 10  ('tozne', 'mean')             176295 non-null  float64\n",
      " 11  ('tozne', 'var')              176295 non-null  float64\n",
      " 12  ('gh', 'amin')                176295 non-null  float64\n",
      " 13  ('gh', 'amax')                176295 non-null  float64\n",
      " 14  ('gh', 'mean')                176295 non-null  float64\n",
      " 15  ('gh', 'var')                 176295 non-null  float64\n",
      " 16  ('pwat', 'amin')              176295 non-null  float64\n",
      " 17  ('pwat', 'amax')              176295 non-null  float64\n",
      " 18  ('pwat', 'mean')              176295 non-null  float64\n",
      " 19  ('pwat', 'var')               176295 non-null  float64\n",
      " 20  ('paramId_0', 'amin')         176295 non-null  float64\n",
      " 21  ('paramId_0', 'amax')         176295 non-null  float64\n",
      " 22  ('paramId_0', 'mean')         176295 non-null  float64\n",
      " 23  ('paramId_0', 'var')          176295 non-null  float64\n",
      " 24  ('pres', 'count')             176295 non-null  int64  \n",
      " 25  ('macro_season', '<lambda>')  176295 non-null  int64  \n",
      " 26  ('month', '<lambda>')         176295 non-null  int64  \n",
      "dtypes: float64(24), int64(3)\n",
      "memory usage: 36.3 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    147611\n",
       "1     28684\n",
       "Name: fire, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tenday_features_full = tenday_df.iloc[:, 9:40]\n",
    "tenday_features_no_null = tenday_df.iloc[:,np.r_[9:33, 37:40]]\n",
    "tenday_features_full.info()\n",
    "tenday_features_no_null.info()\n",
    "tenday_target = tenday_df[\"fire\"]\n",
    "tenday_target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x1800 with 36 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tenday_features_full.hist(bins = 50, figsize = (25, 25))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now Split Feature and Target into Training and Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-Day Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(469476, 31)\n",
      "(116886, 31)\n",
      "(469476, 27)\n",
      "(116886, 27)\n",
      "(469476,)\n",
      "(116886,)\n"
     ]
    }
   ],
   "source": [
    "threeday_full_train = threeday_features_full[threeday_df.iloc[:,7] < 2009]\n",
    "threeday_full_test = threeday_features_full[threeday_df.iloc[:,7] >= 2009]\n",
    "threeday_no_null_train = threeday_features_no_null[threeday_df.iloc[:,7] < 2009]\n",
    "threeday_no_null_test = threeday_features_no_null[threeday_df.iloc[:,7] >= 2009]\n",
    "threeday_target_train = threeday_target[threeday_df.iloc[:,7] < 2009]\n",
    "threeday_target_test = threeday_target[threeday_df.iloc[:,7] >= 2009]\n",
    "print(threeday_full_train.shape)\n",
    "print(threeday_full_test.shape)\n",
    "print(threeday_no_null_train.shape)\n",
    "print(threeday_no_null_test.shape)\n",
    "print(threeday_target_train.shape)\n",
    "print(threeday_target_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-Day Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(281589, 31)\n",
      "(70518, 31)\n",
      "(281589, 27)\n",
      "(70518, 27)\n",
      "(281589,)\n",
      "(70518,)\n"
     ]
    }
   ],
   "source": [
    "fiveday_full_train = fiveday_features_full[fiveday_df.iloc[:,7] < 2009]\n",
    "fiveday_full_test = fiveday_features_full[fiveday_df.iloc[:,7] >= 2009]\n",
    "fiveday_no_null_train = fiveday_features_no_null[fiveday_df.iloc[:,7] < 2009]\n",
    "fiveday_no_null_test = fiveday_features_no_null[fiveday_df.iloc[:,7] >= 2009]\n",
    "fiveday_target_train = fiveday_target[fiveday_df.iloc[:,7] < 2009]\n",
    "fiveday_target_test = fiveday_target[fiveday_df.iloc[:,7] >= 2009]\n",
    "print(fiveday_full_train.shape)\n",
    "print(fiveday_full_test.shape)\n",
    "print(fiveday_no_null_train.shape)\n",
    "print(fiveday_no_null_test.shape)\n",
    "print(fiveday_target_train.shape)\n",
    "print(fiveday_target_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10-Day Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(141036, 31)\n",
      "(35259, 31)\n",
      "(141036, 27)\n",
      "(35259, 27)\n",
      "(141036,)\n",
      "(35259,)\n"
     ]
    }
   ],
   "source": [
    "tenday_full_train = tenday_features_full[tenday_df.iloc[:,7] < 2009]\n",
    "tenday_full_test = tenday_features_full[tenday_df.iloc[:,7] >= 2009]\n",
    "tenday_no_null_train = tenday_features_no_null[tenday_df.iloc[:,7] < 2009]\n",
    "tenday_no_null_test = tenday_features_no_null[tenday_df.iloc[:,7] >= 2009]\n",
    "tenday_target_train = tenday_target[tenday_df.iloc[:,7] < 2009]\n",
    "tenday_target_test = tenday_target[tenday_df.iloc[:,7] >= 2009]\n",
    "print(tenday_full_train.shape)\n",
    "print(tenday_full_test.shape)\n",
    "print(tenday_no_null_train.shape)\n",
    "print(tenday_no_null_test.shape)\n",
    "print(tenday_target_train.shape)\n",
    "print(tenday_target_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputing Missing Values Using Median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-Day Average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 469476 entries, 0 to 586119\n",
      "Data columns (total 31 columns):\n",
      " #   Column                        Non-Null Count   Dtype  \n",
      "---  ------                        --------------   -----  \n",
      " 0   ('cwat', 'amin')              469476 non-null  float64\n",
      " 1   ('cwat', 'amax')              469476 non-null  float64\n",
      " 2   ('cwat', 'mean')              469476 non-null  float64\n",
      " 3   ('cwat', 'var')               469476 non-null  float64\n",
      " 4   ('r', 'amin')                 469476 non-null  float64\n",
      " 5   ('r', 'amax')                 469476 non-null  float64\n",
      " 6   ('r', 'mean')                 469476 non-null  float64\n",
      " 7   ('r', 'var')                  469476 non-null  float64\n",
      " 8   ('tozne', 'amin')             469476 non-null  float64\n",
      " 9   ('tozne', 'amax')             469476 non-null  float64\n",
      " 10  ('tozne', 'mean')             469476 non-null  float64\n",
      " 11  ('tozne', 'var')              469476 non-null  float64\n",
      " 12  ('gh', 'amin')                469476 non-null  float64\n",
      " 13  ('gh', 'amax')                469476 non-null  float64\n",
      " 14  ('gh', 'mean')                469476 non-null  float64\n",
      " 15  ('gh', 'var')                 469476 non-null  float64\n",
      " 16  ('pwat', 'amin')              469476 non-null  float64\n",
      " 17  ('pwat', 'amax')              469476 non-null  float64\n",
      " 18  ('pwat', 'mean')              469476 non-null  float64\n",
      " 19  ('pwat', 'var')               469476 non-null  float64\n",
      " 20  ('paramId_0', 'amin')         469476 non-null  float64\n",
      " 21  ('paramId_0', 'amax')         469476 non-null  float64\n",
      " 22  ('paramId_0', 'mean')         469476 non-null  float64\n",
      " 23  ('paramId_0', 'var')          469476 non-null  float64\n",
      " 24  ('pres', 'amin')              83604 non-null   float64\n",
      " 25  ('pres', 'amax')              83604 non-null   float64\n",
      " 26  ('pres', 'mean')              83604 non-null   float64\n",
      " 27  ('pres', 'var')               40400 non-null   float64\n",
      " 28  ('pres', 'count')             469476 non-null  int64  \n",
      " 29  ('macro_season', '<lambda>')  469476 non-null  int64  \n",
      " 30  ('month', '<lambda>')         469476 non-null  int64  \n",
      "dtypes: float64(28), int64(3)\n",
      "memory usage: 114.6 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 469476 entries, 0 to 469475\n",
      "Data columns (total 31 columns):\n",
      " #   Column                        Non-Null Count   Dtype  \n",
      "---  ------                        --------------   -----  \n",
      " 0   ('cwat', 'amin')              469476 non-null  float64\n",
      " 1   ('cwat', 'amax')              469476 non-null  float64\n",
      " 2   ('cwat', 'mean')              469476 non-null  float64\n",
      " 3   ('cwat', 'var')               469476 non-null  float64\n",
      " 4   ('r', 'amin')                 469476 non-null  float64\n",
      " 5   ('r', 'amax')                 469476 non-null  float64\n",
      " 6   ('r', 'mean')                 469476 non-null  float64\n",
      " 7   ('r', 'var')                  469476 non-null  float64\n",
      " 8   ('tozne', 'amin')             469476 non-null  float64\n",
      " 9   ('tozne', 'amax')             469476 non-null  float64\n",
      " 10  ('tozne', 'mean')             469476 non-null  float64\n",
      " 11  ('tozne', 'var')              469476 non-null  float64\n",
      " 12  ('gh', 'amin')                469476 non-null  float64\n",
      " 13  ('gh', 'amax')                469476 non-null  float64\n",
      " 14  ('gh', 'mean')                469476 non-null  float64\n",
      " 15  ('gh', 'var')                 469476 non-null  float64\n",
      " 16  ('pwat', 'amin')              469476 non-null  float64\n",
      " 17  ('pwat', 'amax')              469476 non-null  float64\n",
      " 18  ('pwat', 'mean')              469476 non-null  float64\n",
      " 19  ('pwat', 'var')               469476 non-null  float64\n",
      " 20  ('paramId_0', 'amin')         469476 non-null  float64\n",
      " 21  ('paramId_0', 'amax')         469476 non-null  float64\n",
      " 22  ('paramId_0', 'mean')         469476 non-null  float64\n",
      " 23  ('paramId_0', 'var')          469476 non-null  float64\n",
      " 24  ('pres', 'amin')              469476 non-null  float64\n",
      " 25  ('pres', 'amax')              469476 non-null  float64\n",
      " 26  ('pres', 'mean')              469476 non-null  float64\n",
      " 27  ('pres', 'var')               469476 non-null  float64\n",
      " 28  ('pres', 'count')             469476 non-null  float64\n",
      " 29  ('macro_season', '<lambda>')  469476 non-null  float64\n",
      " 30  ('month', '<lambda>')         469476 non-null  float64\n",
      "dtypes: float64(31)\n",
      "memory usage: 111.0 MB\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp = SimpleImputer(strategy = \"median\")\n",
    "\n",
    "threeday_full_train.info()\n",
    "threeday_imp_train = pd.DataFrame(imp.fit_transform(threeday_full_train))\n",
    "threeday_imp_train.columns = threeday_full_train.columns\n",
    "threeday_imp_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 116886 entries, 972 to 586361\n",
      "Data columns (total 31 columns):\n",
      " #   Column                        Non-Null Count   Dtype  \n",
      "---  ------                        --------------   -----  \n",
      " 0   ('cwat', 'amin')              116886 non-null  float64\n",
      " 1   ('cwat', 'amax')              116886 non-null  float64\n",
      " 2   ('cwat', 'mean')              116886 non-null  float64\n",
      " 3   ('cwat', 'var')               116886 non-null  float64\n",
      " 4   ('r', 'amin')                 116886 non-null  float64\n",
      " 5   ('r', 'amax')                 116886 non-null  float64\n",
      " 6   ('r', 'mean')                 116886 non-null  float64\n",
      " 7   ('r', 'var')                  116886 non-null  float64\n",
      " 8   ('tozne', 'amin')             116886 non-null  float64\n",
      " 9   ('tozne', 'amax')             116886 non-null  float64\n",
      " 10  ('tozne', 'mean')             116886 non-null  float64\n",
      " 11  ('tozne', 'var')              116886 non-null  float64\n",
      " 12  ('gh', 'amin')                116886 non-null  float64\n",
      " 13  ('gh', 'amax')                116886 non-null  float64\n",
      " 14  ('gh', 'mean')                116886 non-null  float64\n",
      " 15  ('gh', 'var')                 116886 non-null  float64\n",
      " 16  ('pwat', 'amin')              116886 non-null  float64\n",
      " 17  ('pwat', 'amax')              116886 non-null  float64\n",
      " 18  ('pwat', 'mean')              116886 non-null  float64\n",
      " 19  ('pwat', 'var')               116886 non-null  float64\n",
      " 20  ('paramId_0', 'amin')         116886 non-null  float64\n",
      " 21  ('paramId_0', 'amax')         116886 non-null  float64\n",
      " 22  ('paramId_0', 'mean')         116886 non-null  float64\n",
      " 23  ('paramId_0', 'var')          116886 non-null  float64\n",
      " 24  ('pres', 'amin')              22730 non-null   float64\n",
      " 25  ('pres', 'amax')              22730 non-null   float64\n",
      " 26  ('pres', 'mean')              22730 non-null   float64\n",
      " 27  ('pres', 'var')               10833 non-null   float64\n",
      " 28  ('pres', 'count')             116886 non-null  int64  \n",
      " 29  ('macro_season', '<lambda>')  116886 non-null  int64  \n",
      " 30  ('month', '<lambda>')         116886 non-null  int64  \n",
      "dtypes: float64(28), int64(3)\n",
      "memory usage: 28.5 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 116886 entries, 0 to 116885\n",
      "Data columns (total 31 columns):\n",
      " #   Column                        Non-Null Count   Dtype  \n",
      "---  ------                        --------------   -----  \n",
      " 0   ('cwat', 'amin')              116886 non-null  float64\n",
      " 1   ('cwat', 'amax')              116886 non-null  float64\n",
      " 2   ('cwat', 'mean')              116886 non-null  float64\n",
      " 3   ('cwat', 'var')               116886 non-null  float64\n",
      " 4   ('r', 'amin')                 116886 non-null  float64\n",
      " 5   ('r', 'amax')                 116886 non-null  float64\n",
      " 6   ('r', 'mean')                 116886 non-null  float64\n",
      " 7   ('r', 'var')                  116886 non-null  float64\n",
      " 8   ('tozne', 'amin')             116886 non-null  float64\n",
      " 9   ('tozne', 'amax')             116886 non-null  float64\n",
      " 10  ('tozne', 'mean')             116886 non-null  float64\n",
      " 11  ('tozne', 'var')              116886 non-null  float64\n",
      " 12  ('gh', 'amin')                116886 non-null  float64\n",
      " 13  ('gh', 'amax')                116886 non-null  float64\n",
      " 14  ('gh', 'mean')                116886 non-null  float64\n",
      " 15  ('gh', 'var')                 116886 non-null  float64\n",
      " 16  ('pwat', 'amin')              116886 non-null  float64\n",
      " 17  ('pwat', 'amax')              116886 non-null  float64\n",
      " 18  ('pwat', 'mean')              116886 non-null  float64\n",
      " 19  ('pwat', 'var')               116886 non-null  float64\n",
      " 20  ('paramId_0', 'amin')         116886 non-null  float64\n",
      " 21  ('paramId_0', 'amax')         116886 non-null  float64\n",
      " 22  ('paramId_0', 'mean')         116886 non-null  float64\n",
      " 23  ('paramId_0', 'var')          116886 non-null  float64\n",
      " 24  ('pres', 'amin')              116886 non-null  float64\n",
      " 25  ('pres', 'amax')              116886 non-null  float64\n",
      " 26  ('pres', 'mean')              116886 non-null  float64\n",
      " 27  ('pres', 'var')               116886 non-null  float64\n",
      " 28  ('pres', 'count')             116886 non-null  float64\n",
      " 29  ('macro_season', '<lambda>')  116886 non-null  float64\n",
      " 30  ('month', '<lambda>')         116886 non-null  float64\n",
      "dtypes: float64(31)\n",
      "memory usage: 27.6 MB\n"
     ]
    }
   ],
   "source": [
    "threeday_full_test.info()\n",
    "threeday_imp_test = pd.DataFrame(imp.fit_transform(threeday_full_test))\n",
    "threeday_imp_test.columns = threeday_full_test.columns\n",
    "threeday_imp_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-Day Average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 281589 entries, 0 to 351960\n",
      "Data columns (total 31 columns):\n",
      " #   Column                        Non-Null Count   Dtype  \n",
      "---  ------                        --------------   -----  \n",
      " 0   ('cwat', 'amin')              281589 non-null  float64\n",
      " 1   ('cwat', 'amax')              281589 non-null  float64\n",
      " 2   ('cwat', 'mean')              281589 non-null  float64\n",
      " 3   ('cwat', 'var')               281589 non-null  float64\n",
      " 4   ('r', 'amin')                 281589 non-null  float64\n",
      " 5   ('r', 'amax')                 281589 non-null  float64\n",
      " 6   ('r', 'mean')                 281589 non-null  float64\n",
      " 7   ('r', 'var')                  281589 non-null  float64\n",
      " 8   ('tozne', 'amin')             281589 non-null  float64\n",
      " 9   ('tozne', 'amax')             281589 non-null  float64\n",
      " 10  ('tozne', 'mean')             281589 non-null  float64\n",
      " 11  ('tozne', 'var')              281589 non-null  float64\n",
      " 12  ('gh', 'amin')                281589 non-null  float64\n",
      " 13  ('gh', 'amax')                281589 non-null  float64\n",
      " 14  ('gh', 'mean')                281589 non-null  float64\n",
      " 15  ('gh', 'var')                 281589 non-null  float64\n",
      " 16  ('pwat', 'amin')              281589 non-null  float64\n",
      " 17  ('pwat', 'amax')              281589 non-null  float64\n",
      " 18  ('pwat', 'mean')              281589 non-null  float64\n",
      " 19  ('pwat', 'var')               281589 non-null  float64\n",
      " 20  ('paramId_0', 'amin')         281589 non-null  float64\n",
      " 21  ('paramId_0', 'amax')         281589 non-null  float64\n",
      " 22  ('paramId_0', 'mean')         281589 non-null  float64\n",
      " 23  ('paramId_0', 'var')          281589 non-null  float64\n",
      " 24  ('pres', 'amin')              71153 non-null   float64\n",
      " 25  ('pres', 'amax')              71153 non-null   float64\n",
      " 26  ('pres', 'mean')              71153 non-null   float64\n",
      " 27  ('pres', 'var')               39239 non-null   float64\n",
      " 28  ('pres', 'count')             281589 non-null  int64  \n",
      " 29  ('macro_season', '<lambda>')  281589 non-null  int64  \n",
      " 30  ('month', '<lambda>')         281589 non-null  int64  \n",
      "dtypes: float64(28), int64(3)\n",
      "memory usage: 68.7 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 281589 entries, 0 to 281588\n",
      "Data columns (total 31 columns):\n",
      " #   Column                        Non-Null Count   Dtype  \n",
      "---  ------                        --------------   -----  \n",
      " 0   ('cwat', 'amin')              281589 non-null  float64\n",
      " 1   ('cwat', 'amax')              281589 non-null  float64\n",
      " 2   ('cwat', 'mean')              281589 non-null  float64\n",
      " 3   ('cwat', 'var')               281589 non-null  float64\n",
      " 4   ('r', 'amin')                 281589 non-null  float64\n",
      " 5   ('r', 'amax')                 281589 non-null  float64\n",
      " 6   ('r', 'mean')                 281589 non-null  float64\n",
      " 7   ('r', 'var')                  281589 non-null  float64\n",
      " 8   ('tozne', 'amin')             281589 non-null  float64\n",
      " 9   ('tozne', 'amax')             281589 non-null  float64\n",
      " 10  ('tozne', 'mean')             281589 non-null  float64\n",
      " 11  ('tozne', 'var')              281589 non-null  float64\n",
      " 12  ('gh', 'amin')                281589 non-null  float64\n",
      " 13  ('gh', 'amax')                281589 non-null  float64\n",
      " 14  ('gh', 'mean')                281589 non-null  float64\n",
      " 15  ('gh', 'var')                 281589 non-null  float64\n",
      " 16  ('pwat', 'amin')              281589 non-null  float64\n",
      " 17  ('pwat', 'amax')              281589 non-null  float64\n",
      " 18  ('pwat', 'mean')              281589 non-null  float64\n",
      " 19  ('pwat', 'var')               281589 non-null  float64\n",
      " 20  ('paramId_0', 'amin')         281589 non-null  float64\n",
      " 21  ('paramId_0', 'amax')         281589 non-null  float64\n",
      " 22  ('paramId_0', 'mean')         281589 non-null  float64\n",
      " 23  ('paramId_0', 'var')          281589 non-null  float64\n",
      " 24  ('pres', 'amin')              281589 non-null  float64\n",
      " 25  ('pres', 'amax')              281589 non-null  float64\n",
      " 26  ('pres', 'mean')              281589 non-null  float64\n",
      " 27  ('pres', 'var')               281589 non-null  float64\n",
      " 28  ('pres', 'count')             281589 non-null  float64\n",
      " 29  ('macro_season', '<lambda>')  281589 non-null  float64\n",
      " 30  ('month', '<lambda>')         281589 non-null  float64\n",
      "dtypes: float64(31)\n",
      "memory usage: 66.6 MB\n"
     ]
    }
   ],
   "source": [
    "fiveday_full_train.info()\n",
    "fiveday_imp_train = pd.DataFrame(imp.fit_transform(fiveday_full_train))\n",
    "fiveday_imp_train.columns = fiveday_full_train.columns\n",
    "fiveday_imp_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 70518 entries, 583 to 352106\n",
      "Data columns (total 31 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   ('cwat', 'amin')              70518 non-null  float64\n",
      " 1   ('cwat', 'amax')              70518 non-null  float64\n",
      " 2   ('cwat', 'mean')              70518 non-null  float64\n",
      " 3   ('cwat', 'var')               70518 non-null  float64\n",
      " 4   ('r', 'amin')                 70518 non-null  float64\n",
      " 5   ('r', 'amax')                 70518 non-null  float64\n",
      " 6   ('r', 'mean')                 70518 non-null  float64\n",
      " 7   ('r', 'var')                  70518 non-null  float64\n",
      " 8   ('tozne', 'amin')             70518 non-null  float64\n",
      " 9   ('tozne', 'amax')             70518 non-null  float64\n",
      " 10  ('tozne', 'mean')             70518 non-null  float64\n",
      " 11  ('tozne', 'var')              70518 non-null  float64\n",
      " 12  ('gh', 'amin')                70518 non-null  float64\n",
      " 13  ('gh', 'amax')                70518 non-null  float64\n",
      " 14  ('gh', 'mean')                70518 non-null  float64\n",
      " 15  ('gh', 'var')                 70518 non-null  float64\n",
      " 16  ('pwat', 'amin')              70518 non-null  float64\n",
      " 17  ('pwat', 'amax')              70518 non-null  float64\n",
      " 18  ('pwat', 'mean')              70518 non-null  float64\n",
      " 19  ('pwat', 'var')               70518 non-null  float64\n",
      " 20  ('paramId_0', 'amin')         70518 non-null  float64\n",
      " 21  ('paramId_0', 'amax')         70518 non-null  float64\n",
      " 22  ('paramId_0', 'mean')         70518 non-null  float64\n",
      " 23  ('paramId_0', 'var')          70518 non-null  float64\n",
      " 24  ('pres', 'amin')              19742 non-null  float64\n",
      " 25  ('pres', 'amax')              19742 non-null  float64\n",
      " 26  ('pres', 'mean')              19742 non-null  float64\n",
      " 27  ('pres', 'var')               10677 non-null  float64\n",
      " 28  ('pres', 'count')             70518 non-null  int64  \n",
      " 29  ('macro_season', '<lambda>')  70518 non-null  int64  \n",
      " 30  ('month', '<lambda>')         70518 non-null  int64  \n",
      "dtypes: float64(28), int64(3)\n",
      "memory usage: 17.2 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 70518 entries, 0 to 70517\n",
      "Data columns (total 31 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   ('cwat', 'amin')              70518 non-null  float64\n",
      " 1   ('cwat', 'amax')              70518 non-null  float64\n",
      " 2   ('cwat', 'mean')              70518 non-null  float64\n",
      " 3   ('cwat', 'var')               70518 non-null  float64\n",
      " 4   ('r', 'amin')                 70518 non-null  float64\n",
      " 5   ('r', 'amax')                 70518 non-null  float64\n",
      " 6   ('r', 'mean')                 70518 non-null  float64\n",
      " 7   ('r', 'var')                  70518 non-null  float64\n",
      " 8   ('tozne', 'amin')             70518 non-null  float64\n",
      " 9   ('tozne', 'amax')             70518 non-null  float64\n",
      " 10  ('tozne', 'mean')             70518 non-null  float64\n",
      " 11  ('tozne', 'var')              70518 non-null  float64\n",
      " 12  ('gh', 'amin')                70518 non-null  float64\n",
      " 13  ('gh', 'amax')                70518 non-null  float64\n",
      " 14  ('gh', 'mean')                70518 non-null  float64\n",
      " 15  ('gh', 'var')                 70518 non-null  float64\n",
      " 16  ('pwat', 'amin')              70518 non-null  float64\n",
      " 17  ('pwat', 'amax')              70518 non-null  float64\n",
      " 18  ('pwat', 'mean')              70518 non-null  float64\n",
      " 19  ('pwat', 'var')               70518 non-null  float64\n",
      " 20  ('paramId_0', 'amin')         70518 non-null  float64\n",
      " 21  ('paramId_0', 'amax')         70518 non-null  float64\n",
      " 22  ('paramId_0', 'mean')         70518 non-null  float64\n",
      " 23  ('paramId_0', 'var')          70518 non-null  float64\n",
      " 24  ('pres', 'amin')              70518 non-null  float64\n",
      " 25  ('pres', 'amax')              70518 non-null  float64\n",
      " 26  ('pres', 'mean')              70518 non-null  float64\n",
      " 27  ('pres', 'var')               70518 non-null  float64\n",
      " 28  ('pres', 'count')             70518 non-null  float64\n",
      " 29  ('macro_season', '<lambda>')  70518 non-null  float64\n",
      " 30  ('month', '<lambda>')         70518 non-null  float64\n",
      "dtypes: float64(31)\n",
      "memory usage: 16.7 MB\n"
     ]
    }
   ],
   "source": [
    "fiveday_full_test.info()\n",
    "fiveday_imp_test = pd.DataFrame(imp.fit_transform(fiveday_full_test))\n",
    "fiveday_imp_test.columns = fiveday_full_test.columns\n",
    "fiveday_imp_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10-Day Average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 141036 entries, 0 to 176221\n",
      "Data columns (total 31 columns):\n",
      " #   Column                        Non-Null Count   Dtype  \n",
      "---  ------                        --------------   -----  \n",
      " 0   ('cwat', 'amin')              141036 non-null  float64\n",
      " 1   ('cwat', 'amax')              141036 non-null  float64\n",
      " 2   ('cwat', 'mean')              141036 non-null  float64\n",
      " 3   ('cwat', 'var')               141036 non-null  float64\n",
      " 4   ('r', 'amin')                 141036 non-null  float64\n",
      " 5   ('r', 'amax')                 141036 non-null  float64\n",
      " 6   ('r', 'mean')                 141036 non-null  float64\n",
      " 7   ('r', 'var')                  141036 non-null  float64\n",
      " 8   ('tozne', 'amin')             141036 non-null  float64\n",
      " 9   ('tozne', 'amax')             141036 non-null  float64\n",
      " 10  ('tozne', 'mean')             141036 non-null  float64\n",
      " 11  ('tozne', 'var')              141036 non-null  float64\n",
      " 12  ('gh', 'amin')                141036 non-null  float64\n",
      " 13  ('gh', 'amax')                141036 non-null  float64\n",
      " 14  ('gh', 'mean')                141036 non-null  float64\n",
      " 15  ('gh', 'var')                 141036 non-null  float64\n",
      " 16  ('pwat', 'amin')              141036 non-null  float64\n",
      " 17  ('pwat', 'amax')              141036 non-null  float64\n",
      " 18  ('pwat', 'mean')              141036 non-null  float64\n",
      " 19  ('pwat', 'var')               141036 non-null  float64\n",
      " 20  ('paramId_0', 'amin')         141036 non-null  float64\n",
      " 21  ('paramId_0', 'amax')         141036 non-null  float64\n",
      " 22  ('paramId_0', 'mean')         141036 non-null  float64\n",
      " 23  ('paramId_0', 'var')          141036 non-null  float64\n",
      " 24  ('pres', 'amin')              55458 non-null   float64\n",
      " 25  ('pres', 'amax')              55458 non-null   float64\n",
      " 26  ('pres', 'mean')              55458 non-null   float64\n",
      " 27  ('pres', 'var')               34949 non-null   float64\n",
      " 28  ('pres', 'count')             141036 non-null  int64  \n",
      " 29  ('macro_season', '<lambda>')  141036 non-null  int64  \n",
      " 30  ('month', '<lambda>')         141036 non-null  int64  \n",
      "dtypes: float64(28), int64(3)\n",
      "memory usage: 34.4 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 141036 entries, 0 to 141035\n",
      "Data columns (total 31 columns):\n",
      " #   Column                        Non-Null Count   Dtype  \n",
      "---  ------                        --------------   -----  \n",
      " 0   ('cwat', 'amin')              141036 non-null  float64\n",
      " 1   ('cwat', 'amax')              141036 non-null  float64\n",
      " 2   ('cwat', 'mean')              141036 non-null  float64\n",
      " 3   ('cwat', 'var')               141036 non-null  float64\n",
      " 4   ('r', 'amin')                 141036 non-null  float64\n",
      " 5   ('r', 'amax')                 141036 non-null  float64\n",
      " 6   ('r', 'mean')                 141036 non-null  float64\n",
      " 7   ('r', 'var')                  141036 non-null  float64\n",
      " 8   ('tozne', 'amin')             141036 non-null  float64\n",
      " 9   ('tozne', 'amax')             141036 non-null  float64\n",
      " 10  ('tozne', 'mean')             141036 non-null  float64\n",
      " 11  ('tozne', 'var')              141036 non-null  float64\n",
      " 12  ('gh', 'amin')                141036 non-null  float64\n",
      " 13  ('gh', 'amax')                141036 non-null  float64\n",
      " 14  ('gh', 'mean')                141036 non-null  float64\n",
      " 15  ('gh', 'var')                 141036 non-null  float64\n",
      " 16  ('pwat', 'amin')              141036 non-null  float64\n",
      " 17  ('pwat', 'amax')              141036 non-null  float64\n",
      " 18  ('pwat', 'mean')              141036 non-null  float64\n",
      " 19  ('pwat', 'var')               141036 non-null  float64\n",
      " 20  ('paramId_0', 'amin')         141036 non-null  float64\n",
      " 21  ('paramId_0', 'amax')         141036 non-null  float64\n",
      " 22  ('paramId_0', 'mean')         141036 non-null  float64\n",
      " 23  ('paramId_0', 'var')          141036 non-null  float64\n",
      " 24  ('pres', 'amin')              141036 non-null  float64\n",
      " 25  ('pres', 'amax')              141036 non-null  float64\n",
      " 26  ('pres', 'mean')              141036 non-null  float64\n",
      " 27  ('pres', 'var')               141036 non-null  float64\n",
      " 28  ('pres', 'count')             141036 non-null  float64\n",
      " 29  ('macro_season', '<lambda>')  141036 non-null  float64\n",
      " 30  ('month', '<lambda>')         141036 non-null  float64\n",
      "dtypes: float64(31)\n",
      "memory usage: 33.4 MB\n"
     ]
    }
   ],
   "source": [
    "tenday_full_train.info()\n",
    "tenday_imp_train = pd.DataFrame(imp.fit_transform(tenday_full_train))\n",
    "tenday_imp_train.columns = tenday_full_train.columns\n",
    "tenday_imp_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 35259 entries, 292 to 176294\n",
      "Data columns (total 31 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   ('cwat', 'amin')              35259 non-null  float64\n",
      " 1   ('cwat', 'amax')              35259 non-null  float64\n",
      " 2   ('cwat', 'mean')              35259 non-null  float64\n",
      " 3   ('cwat', 'var')               35259 non-null  float64\n",
      " 4   ('r', 'amin')                 35259 non-null  float64\n",
      " 5   ('r', 'amax')                 35259 non-null  float64\n",
      " 6   ('r', 'mean')                 35259 non-null  float64\n",
      " 7   ('r', 'var')                  35259 non-null  float64\n",
      " 8   ('tozne', 'amin')             35259 non-null  float64\n",
      " 9   ('tozne', 'amax')             35259 non-null  float64\n",
      " 10  ('tozne', 'mean')             35259 non-null  float64\n",
      " 11  ('tozne', 'var')              35259 non-null  float64\n",
      " 12  ('gh', 'amin')                35259 non-null  float64\n",
      " 13  ('gh', 'amax')                35259 non-null  float64\n",
      " 14  ('gh', 'mean')                35259 non-null  float64\n",
      " 15  ('gh', 'var')                 35259 non-null  float64\n",
      " 16  ('pwat', 'amin')              35259 non-null  float64\n",
      " 17  ('pwat', 'amax')              35259 non-null  float64\n",
      " 18  ('pwat', 'mean')              35259 non-null  float64\n",
      " 19  ('pwat', 'var')               35259 non-null  float64\n",
      " 20  ('paramId_0', 'amin')         35259 non-null  float64\n",
      " 21  ('paramId_0', 'amax')         35259 non-null  float64\n",
      " 22  ('paramId_0', 'mean')         35259 non-null  float64\n",
      " 23  ('paramId_0', 'var')          35259 non-null  float64\n",
      " 24  ('pres', 'amin')              15069 non-null  float64\n",
      " 25  ('pres', 'amax')              15069 non-null  float64\n",
      " 26  ('pres', 'mean')              15069 non-null  float64\n",
      " 27  ('pres', 'var')               9461 non-null   float64\n",
      " 28  ('pres', 'count')             35259 non-null  int64  \n",
      " 29  ('macro_season', '<lambda>')  35259 non-null  int64  \n",
      " 30  ('month', '<lambda>')         35259 non-null  int64  \n",
      "dtypes: float64(28), int64(3)\n",
      "memory usage: 8.6 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 35259 entries, 0 to 35258\n",
      "Data columns (total 31 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   ('cwat', 'amin')              35259 non-null  float64\n",
      " 1   ('cwat', 'amax')              35259 non-null  float64\n",
      " 2   ('cwat', 'mean')              35259 non-null  float64\n",
      " 3   ('cwat', 'var')               35259 non-null  float64\n",
      " 4   ('r', 'amin')                 35259 non-null  float64\n",
      " 5   ('r', 'amax')                 35259 non-null  float64\n",
      " 6   ('r', 'mean')                 35259 non-null  float64\n",
      " 7   ('r', 'var')                  35259 non-null  float64\n",
      " 8   ('tozne', 'amin')             35259 non-null  float64\n",
      " 9   ('tozne', 'amax')             35259 non-null  float64\n",
      " 10  ('tozne', 'mean')             35259 non-null  float64\n",
      " 11  ('tozne', 'var')              35259 non-null  float64\n",
      " 12  ('gh', 'amin')                35259 non-null  float64\n",
      " 13  ('gh', 'amax')                35259 non-null  float64\n",
      " 14  ('gh', 'mean')                35259 non-null  float64\n",
      " 15  ('gh', 'var')                 35259 non-null  float64\n",
      " 16  ('pwat', 'amin')              35259 non-null  float64\n",
      " 17  ('pwat', 'amax')              35259 non-null  float64\n",
      " 18  ('pwat', 'mean')              35259 non-null  float64\n",
      " 19  ('pwat', 'var')               35259 non-null  float64\n",
      " 20  ('paramId_0', 'amin')         35259 non-null  float64\n",
      " 21  ('paramId_0', 'amax')         35259 non-null  float64\n",
      " 22  ('paramId_0', 'mean')         35259 non-null  float64\n",
      " 23  ('paramId_0', 'var')          35259 non-null  float64\n",
      " 24  ('pres', 'amin')              35259 non-null  float64\n",
      " 25  ('pres', 'amax')              35259 non-null  float64\n",
      " 26  ('pres', 'mean')              35259 non-null  float64\n",
      " 27  ('pres', 'var')               35259 non-null  float64\n",
      " 28  ('pres', 'count')             35259 non-null  float64\n",
      " 29  ('macro_season', '<lambda>')  35259 non-null  float64\n",
      " 30  ('month', '<lambda>')         35259 non-null  float64\n",
      "dtypes: float64(31)\n",
      "memory usage: 8.3 MB\n"
     ]
    }
   ],
   "source": [
    "tenday_full_test.info()\n",
    "tenday_imp_test = pd.DataFrame(imp.fit_transform(tenday_full_test))\n",
    "tenday_imp_test.columns = tenday_full_test.columns\n",
    "tenday_imp_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling Using Standard Scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-Day Average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Full Feature Training and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
      "0              0.03              0.18          0.109167         0.002408   \n",
      "1              0.02              0.42          0.129167         0.013354   \n",
      "2              0.00              0.19          0.034167         0.003172   \n",
      "3              0.00              0.33          0.110833         0.015172   \n",
      "4              0.00              0.22          0.061667         0.005233   \n",
      "\n",
      "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
      "0           10.0           15.0      11.916667      4.446970   \n",
      "1            8.0           24.0      16.666667     17.333333   \n",
      "2           21.0           46.0      29.250000     52.022727   \n",
      "3           25.0           48.0      33.916667     63.537879   \n",
      "4           17.0           33.0      23.000000     21.272727   \n",
      "\n",
      "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('paramId_0', 'amax')  \\\n",
      "0         263.500000         284.799988  ...                    0.0   \n",
      "1         269.899994         286.399994  ...                    0.0   \n",
      "2         267.500000         283.500000  ...                    0.0   \n",
      "3         272.399994         360.100006  ...                   59.0   \n",
      "4         284.100006         310.299988  ...                    0.0   \n",
      "\n",
      "   ('paramId_0', 'mean')  ('paramId_0', 'var')  ('pres', 'amin')  \\\n",
      "0               0.000000              0.000000           46580.0   \n",
      "1               0.000000              0.000000           46580.0   \n",
      "2               0.000000              0.000000           46580.0   \n",
      "3              15.583333            600.810606           34430.0   \n",
      "4               0.000000              0.000000           46580.0   \n",
      "\n",
      "   ('pres', 'amax')  ('pres', 'mean')  ('pres', 'var')  ('pres', 'count')  \\\n",
      "0           54590.0           50990.0       36865830.0                0.0   \n",
      "1           54590.0           50990.0       36865830.0                0.0   \n",
      "2           54590.0           50990.0       36865830.0                0.0   \n",
      "3           62960.0           55440.0      196700200.0                4.0   \n",
      "4           54590.0           50990.0       36865830.0                0.0   \n",
      "\n",
      "   ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
      "0                           0.0                    1.0  \n",
      "1                           0.0                    1.0  \n",
      "2                           0.0                    1.0  \n",
      "3                           0.0                    1.0  \n",
      "4                           0.0                    1.0  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>('cwat', 'amin')</th>\n",
       "      <th>('cwat', 'amax')</th>\n",
       "      <th>('cwat', 'mean')</th>\n",
       "      <th>('cwat', 'var')</th>\n",
       "      <th>('r', 'amin')</th>\n",
       "      <th>('r', 'amax')</th>\n",
       "      <th>('r', 'mean')</th>\n",
       "      <th>('r', 'var')</th>\n",
       "      <th>('tozne', 'amin')</th>\n",
       "      <th>('tozne', 'amax')</th>\n",
       "      <th>...</th>\n",
       "      <th>('paramId_0', 'amax')</th>\n",
       "      <th>('paramId_0', 'mean')</th>\n",
       "      <th>('paramId_0', 'var')</th>\n",
       "      <th>('pres', 'amin')</th>\n",
       "      <th>('pres', 'amax')</th>\n",
       "      <th>('pres', 'mean')</th>\n",
       "      <th>('pres', 'var')</th>\n",
       "      <th>('pres', 'count')</th>\n",
       "      <th>('macro_season', '&lt;lambda&gt;')</th>\n",
       "      <th>('month', '&lt;lambda&gt;')</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.897020</td>\n",
       "      <td>-0.205501</td>\n",
       "      <td>0.578897</td>\n",
       "      <td>-0.295475</td>\n",
       "      <td>-0.380295</td>\n",
       "      <td>-1.086629</td>\n",
       "      <td>-0.893432</td>\n",
       "      <td>-0.692110</td>\n",
       "      <td>-1.083547</td>\n",
       "      <td>-1.113394</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.453470</td>\n",
       "      <td>-0.351478</td>\n",
       "      <td>-0.347880</td>\n",
       "      <td>-0.023514</td>\n",
       "      <td>0.030865</td>\n",
       "      <td>0.017832</td>\n",
       "      <td>-0.101443</td>\n",
       "      <td>-0.367394</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.595566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.881970</td>\n",
       "      <td>0.410284</td>\n",
       "      <td>0.799291</td>\n",
       "      <td>-0.120191</td>\n",
       "      <td>-0.723584</td>\n",
       "      <td>-0.277542</td>\n",
       "      <td>-0.282145</td>\n",
       "      <td>-0.352465</td>\n",
       "      <td>-0.843844</td>\n",
       "      <td>-1.069683</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.453470</td>\n",
       "      <td>-0.351478</td>\n",
       "      <td>-0.347880</td>\n",
       "      <td>-0.023514</td>\n",
       "      <td>0.030865</td>\n",
       "      <td>0.017832</td>\n",
       "      <td>-0.101443</td>\n",
       "      <td>-0.367394</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.595566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.148129</td>\n",
       "      <td>-0.179844</td>\n",
       "      <td>-0.247581</td>\n",
       "      <td>-0.283246</td>\n",
       "      <td>1.507794</td>\n",
       "      <td>1.700226</td>\n",
       "      <td>1.337230</td>\n",
       "      <td>0.561843</td>\n",
       "      <td>-0.933733</td>\n",
       "      <td>-1.148910</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.453470</td>\n",
       "      <td>-0.351478</td>\n",
       "      <td>-0.347880</td>\n",
       "      <td>-0.023514</td>\n",
       "      <td>0.030865</td>\n",
       "      <td>0.017832</td>\n",
       "      <td>-0.101443</td>\n",
       "      <td>-0.367394</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.595566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.148129</td>\n",
       "      <td>0.179365</td>\n",
       "      <td>0.597263</td>\n",
       "      <td>-0.091074</td>\n",
       "      <td>2.194372</td>\n",
       "      <td>1.880023</td>\n",
       "      <td>1.937792</td>\n",
       "      <td>0.865348</td>\n",
       "      <td>-0.750210</td>\n",
       "      <td>0.943773</td>\n",
       "      <td>...</td>\n",
       "      <td>3.092190</td>\n",
       "      <td>4.271061</td>\n",
       "      <td>5.151983</td>\n",
       "      <td>-2.214938</td>\n",
       "      <td>1.452011</td>\n",
       "      <td>0.844899</td>\n",
       "      <td>4.692663</td>\n",
       "      <td>3.812911</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.595566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.148129</td>\n",
       "      <td>-0.102871</td>\n",
       "      <td>0.055461</td>\n",
       "      <td>-0.250235</td>\n",
       "      <td>0.821216</td>\n",
       "      <td>0.531545</td>\n",
       "      <td>0.532905</td>\n",
       "      <td>-0.248634</td>\n",
       "      <td>-0.312003</td>\n",
       "      <td>-0.416744</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.453470</td>\n",
       "      <td>-0.351478</td>\n",
       "      <td>-0.347880</td>\n",
       "      <td>-0.023514</td>\n",
       "      <td>0.030865</td>\n",
       "      <td>0.017832</td>\n",
       "      <td>-0.101443</td>\n",
       "      <td>-0.367394</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.595566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469471</th>\n",
       "      <td>-0.148129</td>\n",
       "      <td>2.488561</td>\n",
       "      <td>1.928811</td>\n",
       "      <td>1.435642</td>\n",
       "      <td>-0.723584</td>\n",
       "      <td>0.082052</td>\n",
       "      <td>-0.378664</td>\n",
       "      <td>-0.078711</td>\n",
       "      <td>0.496996</td>\n",
       "      <td>0.430164</td>\n",
       "      <td>...</td>\n",
       "      <td>0.628257</td>\n",
       "      <td>0.093473</td>\n",
       "      <td>-0.100720</td>\n",
       "      <td>-0.984855</td>\n",
       "      <td>-2.234139</td>\n",
       "      <td>-1.792423</td>\n",
       "      <td>-0.101443</td>\n",
       "      <td>0.677682</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.600348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469472</th>\n",
       "      <td>-0.148129</td>\n",
       "      <td>0.872123</td>\n",
       "      <td>1.800248</td>\n",
       "      <td>0.269844</td>\n",
       "      <td>0.134639</td>\n",
       "      <td>0.981038</td>\n",
       "      <td>0.961878</td>\n",
       "      <td>1.233747</td>\n",
       "      <td>0.590630</td>\n",
       "      <td>1.181453</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.393374</td>\n",
       "      <td>-0.326758</td>\n",
       "      <td>-0.347117</td>\n",
       "      <td>-0.023514</td>\n",
       "      <td>0.030865</td>\n",
       "      <td>0.017832</td>\n",
       "      <td>-0.101443</td>\n",
       "      <td>-0.367394</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.600348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469473</th>\n",
       "      <td>-0.148129</td>\n",
       "      <td>1.410936</td>\n",
       "      <td>2.002276</td>\n",
       "      <td>0.514379</td>\n",
       "      <td>-0.380295</td>\n",
       "      <td>0.801241</td>\n",
       "      <td>0.264796</td>\n",
       "      <td>1.996302</td>\n",
       "      <td>0.695500</td>\n",
       "      <td>2.730476</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.453470</td>\n",
       "      <td>-0.351478</td>\n",
       "      <td>-0.347880</td>\n",
       "      <td>-0.023514</td>\n",
       "      <td>0.030865</td>\n",
       "      <td>0.017832</td>\n",
       "      <td>-0.101443</td>\n",
       "      <td>-0.367394</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.600348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469474</th>\n",
       "      <td>-0.148129</td>\n",
       "      <td>2.385930</td>\n",
       "      <td>1.561488</td>\n",
       "      <td>1.333441</td>\n",
       "      <td>0.477928</td>\n",
       "      <td>1.610328</td>\n",
       "      <td>1.819825</td>\n",
       "      <td>1.323201</td>\n",
       "      <td>0.388379</td>\n",
       "      <td>1.389082</td>\n",
       "      <td>...</td>\n",
       "      <td>2.070559</td>\n",
       "      <td>0.686740</td>\n",
       "      <td>0.997769</td>\n",
       "      <td>-1.872246</td>\n",
       "      <td>-3.069508</td>\n",
       "      <td>-2.706843</td>\n",
       "      <td>-0.101443</td>\n",
       "      <td>0.677682</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.600348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469475</th>\n",
       "      <td>0.866920</td>\n",
       "      <td>0.128049</td>\n",
       "      <td>0.799291</td>\n",
       "      <td>-0.092529</td>\n",
       "      <td>0.477928</td>\n",
       "      <td>0.351748</td>\n",
       "      <td>0.693770</td>\n",
       "      <td>-0.329502</td>\n",
       "      <td>0.141186</td>\n",
       "      <td>0.075009</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.453470</td>\n",
       "      <td>-0.351478</td>\n",
       "      <td>-0.347880</td>\n",
       "      <td>-0.023514</td>\n",
       "      <td>0.030865</td>\n",
       "      <td>0.017832</td>\n",
       "      <td>-0.101443</td>\n",
       "      <td>-0.367394</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.595566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>469476 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
       "0               2.897020         -0.205501          0.578897        -0.295475   \n",
       "1               1.881970          0.410284          0.799291        -0.120191   \n",
       "2              -0.148129         -0.179844         -0.247581        -0.283246   \n",
       "3              -0.148129          0.179365          0.597263        -0.091074   \n",
       "4              -0.148129         -0.102871          0.055461        -0.250235   \n",
       "...                  ...               ...               ...              ...   \n",
       "469471         -0.148129          2.488561          1.928811         1.435642   \n",
       "469472         -0.148129          0.872123          1.800248         0.269844   \n",
       "469473         -0.148129          1.410936          2.002276         0.514379   \n",
       "469474         -0.148129          2.385930          1.561488         1.333441   \n",
       "469475          0.866920          0.128049          0.799291        -0.092529   \n",
       "\n",
       "        ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
       "0           -0.380295      -1.086629      -0.893432     -0.692110   \n",
       "1           -0.723584      -0.277542      -0.282145     -0.352465   \n",
       "2            1.507794       1.700226       1.337230      0.561843   \n",
       "3            2.194372       1.880023       1.937792      0.865348   \n",
       "4            0.821216       0.531545       0.532905     -0.248634   \n",
       "...               ...            ...            ...           ...   \n",
       "469471      -0.723584       0.082052      -0.378664     -0.078711   \n",
       "469472       0.134639       0.981038       0.961878      1.233747   \n",
       "469473      -0.380295       0.801241       0.264796      1.996302   \n",
       "469474       0.477928       1.610328       1.819825      1.323201   \n",
       "469475       0.477928       0.351748       0.693770     -0.329502   \n",
       "\n",
       "        ('tozne', 'amin')  ('tozne', 'amax')  ...  ('paramId_0', 'amax')  \\\n",
       "0               -1.083547          -1.113394  ...              -0.453470   \n",
       "1               -0.843844          -1.069683  ...              -0.453470   \n",
       "2               -0.933733          -1.148910  ...              -0.453470   \n",
       "3               -0.750210           0.943773  ...               3.092190   \n",
       "4               -0.312003          -0.416744  ...              -0.453470   \n",
       "...                   ...                ...  ...                    ...   \n",
       "469471           0.496996           0.430164  ...               0.628257   \n",
       "469472           0.590630           1.181453  ...              -0.393374   \n",
       "469473           0.695500           2.730476  ...              -0.453470   \n",
       "469474           0.388379           1.389082  ...               2.070559   \n",
       "469475           0.141186           0.075009  ...              -0.453470   \n",
       "\n",
       "        ('paramId_0', 'mean')  ('paramId_0', 'var')  ('pres', 'amin')  \\\n",
       "0                   -0.351478             -0.347880         -0.023514   \n",
       "1                   -0.351478             -0.347880         -0.023514   \n",
       "2                   -0.351478             -0.347880         -0.023514   \n",
       "3                    4.271061              5.151983         -2.214938   \n",
       "4                   -0.351478             -0.347880         -0.023514   \n",
       "...                       ...                   ...               ...   \n",
       "469471               0.093473             -0.100720         -0.984855   \n",
       "469472              -0.326758             -0.347117         -0.023514   \n",
       "469473              -0.351478             -0.347880         -0.023514   \n",
       "469474               0.686740              0.997769         -1.872246   \n",
       "469475              -0.351478             -0.347880         -0.023514   \n",
       "\n",
       "        ('pres', 'amax')  ('pres', 'mean')  ('pres', 'var')  \\\n",
       "0               0.030865          0.017832        -0.101443   \n",
       "1               0.030865          0.017832        -0.101443   \n",
       "2               0.030865          0.017832        -0.101443   \n",
       "3               1.452011          0.844899         4.692663   \n",
       "4               0.030865          0.017832        -0.101443   \n",
       "...                  ...               ...              ...   \n",
       "469471         -2.234139         -1.792423        -0.101443   \n",
       "469472          0.030865          0.017832        -0.101443   \n",
       "469473          0.030865          0.017832        -0.101443   \n",
       "469474         -3.069508         -2.706843        -0.101443   \n",
       "469475          0.030865          0.017832        -0.101443   \n",
       "\n",
       "        ('pres', 'count')  ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
       "0               -0.367394                          -1.0              -1.595566  \n",
       "1               -0.367394                          -1.0              -1.595566  \n",
       "2               -0.367394                          -1.0              -1.595566  \n",
       "3                3.812911                          -1.0              -1.595566  \n",
       "4               -0.367394                          -1.0              -1.595566  \n",
       "...                   ...                           ...                    ...  \n",
       "469471           0.677682                           1.0               1.600348  \n",
       "469472          -0.367394                           1.0               1.600348  \n",
       "469473          -0.367394                           1.0               1.600348  \n",
       "469474           0.677682                           1.0               1.600348  \n",
       "469475          -0.367394                          -1.0              -1.595566  \n",
       "\n",
       "[469476 rows x 31 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "print(threeday_imp_train.head())\n",
    "scaler.fit(threeday_imp_train)\n",
    "threeday_full_features_train = pd.DataFrame(scaler.transform(threeday_imp_train))\n",
    "threeday_full_features_train.columns = threeday_imp_train.columns\n",
    "threeday_full_features_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>('cwat', 'amin')</th>\n",
       "      <th>('cwat', 'amax')</th>\n",
       "      <th>('cwat', 'mean')</th>\n",
       "      <th>('cwat', 'var')</th>\n",
       "      <th>('r', 'amin')</th>\n",
       "      <th>('r', 'amax')</th>\n",
       "      <th>('r', 'mean')</th>\n",
       "      <th>('r', 'var')</th>\n",
       "      <th>('tozne', 'amin')</th>\n",
       "      <th>('tozne', 'amax')</th>\n",
       "      <th>...</th>\n",
       "      <th>('paramId_0', 'amax')</th>\n",
       "      <th>('paramId_0', 'mean')</th>\n",
       "      <th>('paramId_0', 'var')</th>\n",
       "      <th>('pres', 'amin')</th>\n",
       "      <th>('pres', 'amax')</th>\n",
       "      <th>('pres', 'mean')</th>\n",
       "      <th>('pres', 'var')</th>\n",
       "      <th>('pres', 'count')</th>\n",
       "      <th>('macro_season', '&lt;lambda&gt;')</th>\n",
       "      <th>('month', '&lt;lambda&gt;')</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.881970</td>\n",
       "      <td>0.384626</td>\n",
       "      <td>1.157432</td>\n",
       "      <td>-0.099408</td>\n",
       "      <td>0.649572</td>\n",
       "      <td>1.430530</td>\n",
       "      <td>1.101294</td>\n",
       "      <td>1.723148</td>\n",
       "      <td>0.032570</td>\n",
       "      <td>0.771659</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.453470</td>\n",
       "      <td>-0.351478</td>\n",
       "      <td>-0.347880</td>\n",
       "      <td>-0.108285</td>\n",
       "      <td>-0.054030</td>\n",
       "      <td>-0.052794</td>\n",
       "      <td>-0.080322</td>\n",
       "      <td>-0.367394</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.595566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.881970</td>\n",
       "      <td>-0.308132</td>\n",
       "      <td>0.404418</td>\n",
       "      <td>-0.308614</td>\n",
       "      <td>0.992861</td>\n",
       "      <td>0.801241</td>\n",
       "      <td>0.801013</td>\n",
       "      <td>0.165690</td>\n",
       "      <td>-1.124747</td>\n",
       "      <td>-0.881178</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.453470</td>\n",
       "      <td>-0.351478</td>\n",
       "      <td>-0.347880</td>\n",
       "      <td>-0.108285</td>\n",
       "      <td>-0.054030</td>\n",
       "      <td>-0.052794</td>\n",
       "      <td>-0.080322</td>\n",
       "      <td>-0.367394</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.595566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.148129</td>\n",
       "      <td>-0.410763</td>\n",
       "      <td>-0.532257</td>\n",
       "      <td>-0.320698</td>\n",
       "      <td>-1.238517</td>\n",
       "      <td>-0.727035</td>\n",
       "      <td>-1.300956</td>\n",
       "      <td>-0.454099</td>\n",
       "      <td>-1.252089</td>\n",
       "      <td>-1.351075</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.453470</td>\n",
       "      <td>-0.351478</td>\n",
       "      <td>-0.347880</td>\n",
       "      <td>-0.108285</td>\n",
       "      <td>-0.054030</td>\n",
       "      <td>-0.052794</td>\n",
       "      <td>-0.080322</td>\n",
       "      <td>-0.367394</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.595566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.148129</td>\n",
       "      <td>-0.667341</td>\n",
       "      <td>-0.624088</td>\n",
       "      <td>-0.334043</td>\n",
       "      <td>-1.066872</td>\n",
       "      <td>-1.086629</td>\n",
       "      <td>-1.322405</td>\n",
       "      <td>-0.591474</td>\n",
       "      <td>-1.709023</td>\n",
       "      <td>-1.091538</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.453470</td>\n",
       "      <td>-0.351478</td>\n",
       "      <td>-0.347880</td>\n",
       "      <td>-0.108285</td>\n",
       "      <td>-0.054030</td>\n",
       "      <td>-0.052794</td>\n",
       "      <td>-0.080322</td>\n",
       "      <td>-0.367394</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.595566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.148129</td>\n",
       "      <td>-0.282475</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>-0.275130</td>\n",
       "      <td>-1.066872</td>\n",
       "      <td>-1.446223</td>\n",
       "      <td>-1.333129</td>\n",
       "      <td>-0.725456</td>\n",
       "      <td>-1.053585</td>\n",
       "      <td>-0.982260</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.453470</td>\n",
       "      <td>-0.351478</td>\n",
       "      <td>-0.347880</td>\n",
       "      <td>-0.108285</td>\n",
       "      <td>-0.054030</td>\n",
       "      <td>-0.052794</td>\n",
       "      <td>-0.080322</td>\n",
       "      <td>-0.367394</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.595566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116881</th>\n",
       "      <td>-0.148129</td>\n",
       "      <td>2.668164</td>\n",
       "      <td>1.350277</td>\n",
       "      <td>1.971868</td>\n",
       "      <td>-0.208650</td>\n",
       "      <td>0.171951</td>\n",
       "      <td>0.039585</td>\n",
       "      <td>0.277707</td>\n",
       "      <td>0.358417</td>\n",
       "      <td>-0.151744</td>\n",
       "      <td>...</td>\n",
       "      <td>0.808545</td>\n",
       "      <td>0.167631</td>\n",
       "      <td>-0.011467</td>\n",
       "      <td>0.757463</td>\n",
       "      <td>-0.593963</td>\n",
       "      <td>0.002963</td>\n",
       "      <td>-0.080322</td>\n",
       "      <td>0.677682</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.600348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116882</th>\n",
       "      <td>3.912069</td>\n",
       "      <td>1.308305</td>\n",
       "      <td>2.149205</td>\n",
       "      <td>0.505644</td>\n",
       "      <td>0.134639</td>\n",
       "      <td>0.621444</td>\n",
       "      <td>0.865359</td>\n",
       "      <td>0.031509</td>\n",
       "      <td>0.186130</td>\n",
       "      <td>0.211607</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.453470</td>\n",
       "      <td>-0.351478</td>\n",
       "      <td>-0.347880</td>\n",
       "      <td>-0.108285</td>\n",
       "      <td>-0.054030</td>\n",
       "      <td>-0.052794</td>\n",
       "      <td>-0.080322</td>\n",
       "      <td>-0.367394</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.600348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116883</th>\n",
       "      <td>-0.148129</td>\n",
       "      <td>0.102391</td>\n",
       "      <td>0.229940</td>\n",
       "      <td>-0.199777</td>\n",
       "      <td>0.649572</td>\n",
       "      <td>0.981038</td>\n",
       "      <td>1.079846</td>\n",
       "      <td>0.461207</td>\n",
       "      <td>-0.997404</td>\n",
       "      <td>0.006710</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.453470</td>\n",
       "      <td>-0.351478</td>\n",
       "      <td>-0.347880</td>\n",
       "      <td>-0.108285</td>\n",
       "      <td>-0.054030</td>\n",
       "      <td>-0.052794</td>\n",
       "      <td>-0.080322</td>\n",
       "      <td>-0.367394</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.600348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116884</th>\n",
       "      <td>-0.148129</td>\n",
       "      <td>-0.077213</td>\n",
       "      <td>-0.027187</td>\n",
       "      <td>-0.261408</td>\n",
       "      <td>-0.208650</td>\n",
       "      <td>0.082052</td>\n",
       "      <td>-0.014036</td>\n",
       "      <td>0.197638</td>\n",
       "      <td>-1.165946</td>\n",
       "      <td>-0.244630</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.453470</td>\n",
       "      <td>-0.351478</td>\n",
       "      <td>-0.347880</td>\n",
       "      <td>-0.108285</td>\n",
       "      <td>-0.054030</td>\n",
       "      <td>-0.052794</td>\n",
       "      <td>-0.080322</td>\n",
       "      <td>-0.367394</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.600348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116885</th>\n",
       "      <td>-0.148129</td>\n",
       "      <td>1.718829</td>\n",
       "      <td>2.029825</td>\n",
       "      <td>1.264033</td>\n",
       "      <td>-0.208650</td>\n",
       "      <td>0.981038</td>\n",
       "      <td>0.704494</td>\n",
       "      <td>1.751302</td>\n",
       "      <td>-0.120989</td>\n",
       "      <td>1.700526</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.453470</td>\n",
       "      <td>-0.351478</td>\n",
       "      <td>-0.347880</td>\n",
       "      <td>-0.108285</td>\n",
       "      <td>-0.054030</td>\n",
       "      <td>-0.052794</td>\n",
       "      <td>-0.080322</td>\n",
       "      <td>-0.367394</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.600348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>116886 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
       "0               1.881970          0.384626          1.157432        -0.099408   \n",
       "1               1.881970         -0.308132          0.404418        -0.308614   \n",
       "2              -0.148129         -0.410763         -0.532257        -0.320698   \n",
       "3              -0.148129         -0.667341         -0.624088        -0.334043   \n",
       "4              -0.148129         -0.282475          0.000362        -0.275130   \n",
       "...                  ...               ...               ...              ...   \n",
       "116881         -0.148129          2.668164          1.350277         1.971868   \n",
       "116882          3.912069          1.308305          2.149205         0.505644   \n",
       "116883         -0.148129          0.102391          0.229940        -0.199777   \n",
       "116884         -0.148129         -0.077213         -0.027187        -0.261408   \n",
       "116885         -0.148129          1.718829          2.029825         1.264033   \n",
       "\n",
       "        ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
       "0            0.649572       1.430530       1.101294      1.723148   \n",
       "1            0.992861       0.801241       0.801013      0.165690   \n",
       "2           -1.238517      -0.727035      -1.300956     -0.454099   \n",
       "3           -1.066872      -1.086629      -1.322405     -0.591474   \n",
       "4           -1.066872      -1.446223      -1.333129     -0.725456   \n",
       "...               ...            ...            ...           ...   \n",
       "116881      -0.208650       0.171951       0.039585      0.277707   \n",
       "116882       0.134639       0.621444       0.865359      0.031509   \n",
       "116883       0.649572       0.981038       1.079846      0.461207   \n",
       "116884      -0.208650       0.082052      -0.014036      0.197638   \n",
       "116885      -0.208650       0.981038       0.704494      1.751302   \n",
       "\n",
       "        ('tozne', 'amin')  ('tozne', 'amax')  ...  ('paramId_0', 'amax')  \\\n",
       "0                0.032570           0.771659  ...              -0.453470   \n",
       "1               -1.124747          -0.881178  ...              -0.453470   \n",
       "2               -1.252089          -1.351075  ...              -0.453470   \n",
       "3               -1.709023          -1.091538  ...              -0.453470   \n",
       "4               -1.053585          -0.982260  ...              -0.453470   \n",
       "...                   ...                ...  ...                    ...   \n",
       "116881           0.358417          -0.151744  ...               0.808545   \n",
       "116882           0.186130           0.211607  ...              -0.453470   \n",
       "116883          -0.997404           0.006710  ...              -0.453470   \n",
       "116884          -1.165946          -0.244630  ...              -0.453470   \n",
       "116885          -0.120989           1.700526  ...              -0.453470   \n",
       "\n",
       "        ('paramId_0', 'mean')  ('paramId_0', 'var')  ('pres', 'amin')  \\\n",
       "0                   -0.351478             -0.347880         -0.108285   \n",
       "1                   -0.351478             -0.347880         -0.108285   \n",
       "2                   -0.351478             -0.347880         -0.108285   \n",
       "3                   -0.351478             -0.347880         -0.108285   \n",
       "4                   -0.351478             -0.347880         -0.108285   \n",
       "...                       ...                   ...               ...   \n",
       "116881               0.167631             -0.011467          0.757463   \n",
       "116882              -0.351478             -0.347880         -0.108285   \n",
       "116883              -0.351478             -0.347880         -0.108285   \n",
       "116884              -0.351478             -0.347880         -0.108285   \n",
       "116885              -0.351478             -0.347880         -0.108285   \n",
       "\n",
       "        ('pres', 'amax')  ('pres', 'mean')  ('pres', 'var')  \\\n",
       "0              -0.054030         -0.052794        -0.080322   \n",
       "1              -0.054030         -0.052794        -0.080322   \n",
       "2              -0.054030         -0.052794        -0.080322   \n",
       "3              -0.054030         -0.052794        -0.080322   \n",
       "4              -0.054030         -0.052794        -0.080322   \n",
       "...                  ...               ...              ...   \n",
       "116881         -0.593963          0.002963        -0.080322   \n",
       "116882         -0.054030         -0.052794        -0.080322   \n",
       "116883         -0.054030         -0.052794        -0.080322   \n",
       "116884         -0.054030         -0.052794        -0.080322   \n",
       "116885         -0.054030         -0.052794        -0.080322   \n",
       "\n",
       "        ('pres', 'count')  ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
       "0               -0.367394                          -1.0              -1.595566  \n",
       "1               -0.367394                          -1.0              -1.595566  \n",
       "2               -0.367394                          -1.0              -1.595566  \n",
       "3               -0.367394                          -1.0              -1.595566  \n",
       "4               -0.367394                          -1.0              -1.595566  \n",
       "...                   ...                           ...                    ...  \n",
       "116881           0.677682                           1.0               1.600348  \n",
       "116882          -0.367394                           1.0               1.600348  \n",
       "116883          -0.367394                           1.0               1.600348  \n",
       "116884          -0.367394                           1.0               1.600348  \n",
       "116885          -0.367394                           1.0               1.600348  \n",
       "\n",
       "[116886 rows x 31 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threeday_full_features_test = pd.DataFrame(scaler.transform(threeday_imp_test))\n",
    "threeday_full_features_test.columns = threeday_imp_test.columns\n",
    "threeday_full_features_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Features With No Nulls Training and Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
      "0              0.03              0.18          0.109167         0.002408   \n",
      "1              0.02              0.42          0.129167         0.013354   \n",
      "2              0.00              0.19          0.034167         0.003172   \n",
      "3              0.00              0.33          0.110833         0.015172   \n",
      "4              0.00              0.22          0.061667         0.005233   \n",
      "\n",
      "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
      "0           10.0           15.0      11.916667      4.446970   \n",
      "1            8.0           24.0      16.666667     17.333333   \n",
      "2           21.0           46.0      29.250000     52.022727   \n",
      "3           25.0           48.0      33.916667     63.537879   \n",
      "4           17.0           33.0      23.000000     21.272727   \n",
      "\n",
      "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('pwat', 'amax')  \\\n",
      "0         263.500000         284.799988  ...         10.100000   \n",
      "1         269.899994         286.399994  ...         16.500000   \n",
      "2         267.500000         283.500000  ...         25.900000   \n",
      "3         272.399994         360.100006  ...         28.500000   \n",
      "4         284.100006         310.299988  ...         17.700001   \n",
      "\n",
      "   ('pwat', 'mean')  ('pwat', 'var')  ('paramId_0', 'amin')  \\\n",
      "0          8.358333         1.302652                    0.0   \n",
      "1         11.333333         7.306060                    0.0   \n",
      "2         17.608333        19.888104                    0.0   \n",
      "3         17.616667        36.216058                    0.0   \n",
      "4         12.858334         6.549926                    0.0   \n",
      "\n",
      "   ('paramId_0', 'amax')  ('paramId_0', 'mean')  ('paramId_0', 'var')  \\\n",
      "0                    0.0               0.000000              0.000000   \n",
      "1                    0.0               0.000000              0.000000   \n",
      "2                    0.0               0.000000              0.000000   \n",
      "3                   59.0              15.583333            600.810606   \n",
      "4                    0.0               0.000000              0.000000   \n",
      "\n",
      "   ('pres', 'count')  ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
      "0                  0                             0                      1  \n",
      "1                  0                             0                      1  \n",
      "2                  0                             0                      1  \n",
      "3                  4                             0                      1  \n",
      "4                  0                             0                      1  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>('cwat', 'amin')</th>\n",
       "      <th>('cwat', 'amax')</th>\n",
       "      <th>('cwat', 'mean')</th>\n",
       "      <th>('cwat', 'var')</th>\n",
       "      <th>('r', 'amin')</th>\n",
       "      <th>('r', 'amax')</th>\n",
       "      <th>('r', 'mean')</th>\n",
       "      <th>('r', 'var')</th>\n",
       "      <th>('tozne', 'amin')</th>\n",
       "      <th>('tozne', 'amax')</th>\n",
       "      <th>...</th>\n",
       "      <th>('pwat', 'amax')</th>\n",
       "      <th>('pwat', 'mean')</th>\n",
       "      <th>('pwat', 'var')</th>\n",
       "      <th>('paramId_0', 'amin')</th>\n",
       "      <th>('paramId_0', 'amax')</th>\n",
       "      <th>('paramId_0', 'mean')</th>\n",
       "      <th>('paramId_0', 'var')</th>\n",
       "      <th>('pres', 'count')</th>\n",
       "      <th>('macro_season', '&lt;lambda&gt;')</th>\n",
       "      <th>('month', '&lt;lambda&gt;')</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.897020</td>\n",
       "      <td>-0.205501</td>\n",
       "      <td>0.578897</td>\n",
       "      <td>-0.295475</td>\n",
       "      <td>-0.380295</td>\n",
       "      <td>-1.086629</td>\n",
       "      <td>-0.893432</td>\n",
       "      <td>-0.692110</td>\n",
       "      <td>-1.083547</td>\n",
       "      <td>-1.113394</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.907750</td>\n",
       "      <td>-0.665917</td>\n",
       "      <td>-0.660926</td>\n",
       "      <td>-0.004564</td>\n",
       "      <td>-0.45347</td>\n",
       "      <td>-0.351478</td>\n",
       "      <td>-0.347880</td>\n",
       "      <td>-0.367394</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.595566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.881970</td>\n",
       "      <td>0.410284</td>\n",
       "      <td>0.799291</td>\n",
       "      <td>-0.120191</td>\n",
       "      <td>-0.723584</td>\n",
       "      <td>-0.277542</td>\n",
       "      <td>-0.282145</td>\n",
       "      <td>-0.352465</td>\n",
       "      <td>-0.843844</td>\n",
       "      <td>-1.069683</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.106285</td>\n",
       "      <td>-0.171687</td>\n",
       "      <td>-0.266767</td>\n",
       "      <td>-0.004564</td>\n",
       "      <td>-0.45347</td>\n",
       "      <td>-0.351478</td>\n",
       "      <td>-0.347880</td>\n",
       "      <td>-0.367394</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.595566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.148129</td>\n",
       "      <td>-0.179844</td>\n",
       "      <td>-0.247581</td>\n",
       "      <td>-0.283246</td>\n",
       "      <td>1.507794</td>\n",
       "      <td>1.700226</td>\n",
       "      <td>1.337230</td>\n",
       "      <td>0.561843</td>\n",
       "      <td>-0.933733</td>\n",
       "      <td>-1.148910</td>\n",
       "      <td>...</td>\n",
       "      <td>1.070866</td>\n",
       "      <td>0.870764</td>\n",
       "      <td>0.559319</td>\n",
       "      <td>-0.004564</td>\n",
       "      <td>-0.45347</td>\n",
       "      <td>-0.351478</td>\n",
       "      <td>-0.347880</td>\n",
       "      <td>-0.367394</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.595566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.148129</td>\n",
       "      <td>0.179365</td>\n",
       "      <td>0.597263</td>\n",
       "      <td>-0.091074</td>\n",
       "      <td>2.194372</td>\n",
       "      <td>1.880023</td>\n",
       "      <td>1.937792</td>\n",
       "      <td>0.865348</td>\n",
       "      <td>-0.750210</td>\n",
       "      <td>0.943773</td>\n",
       "      <td>...</td>\n",
       "      <td>1.396461</td>\n",
       "      <td>0.872148</td>\n",
       "      <td>1.631346</td>\n",
       "      <td>-0.004564</td>\n",
       "      <td>3.09219</td>\n",
       "      <td>4.271061</td>\n",
       "      <td>5.151983</td>\n",
       "      <td>3.812911</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.595566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.148129</td>\n",
       "      <td>-0.102871</td>\n",
       "      <td>0.055461</td>\n",
       "      <td>-0.250235</td>\n",
       "      <td>0.821216</td>\n",
       "      <td>0.531545</td>\n",
       "      <td>0.532905</td>\n",
       "      <td>-0.248634</td>\n",
       "      <td>-0.312003</td>\n",
       "      <td>-0.416744</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043989</td>\n",
       "      <td>0.081657</td>\n",
       "      <td>-0.316411</td>\n",
       "      <td>-0.004564</td>\n",
       "      <td>-0.45347</td>\n",
       "      <td>-0.351478</td>\n",
       "      <td>-0.347880</td>\n",
       "      <td>-0.367394</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.595566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
       "0          2.897020         -0.205501          0.578897        -0.295475   \n",
       "1          1.881970          0.410284          0.799291        -0.120191   \n",
       "2         -0.148129         -0.179844         -0.247581        -0.283246   \n",
       "3         -0.148129          0.179365          0.597263        -0.091074   \n",
       "4         -0.148129         -0.102871          0.055461        -0.250235   \n",
       "\n",
       "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
       "0      -0.380295      -1.086629      -0.893432     -0.692110   \n",
       "1      -0.723584      -0.277542      -0.282145     -0.352465   \n",
       "2       1.507794       1.700226       1.337230      0.561843   \n",
       "3       2.194372       1.880023       1.937792      0.865348   \n",
       "4       0.821216       0.531545       0.532905     -0.248634   \n",
       "\n",
       "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('pwat', 'amax')  \\\n",
       "0          -1.083547          -1.113394  ...         -0.907750   \n",
       "1          -0.843844          -1.069683  ...         -0.106285   \n",
       "2          -0.933733          -1.148910  ...          1.070866   \n",
       "3          -0.750210           0.943773  ...          1.396461   \n",
       "4          -0.312003          -0.416744  ...          0.043989   \n",
       "\n",
       "   ('pwat', 'mean')  ('pwat', 'var')  ('paramId_0', 'amin')  \\\n",
       "0         -0.665917        -0.660926              -0.004564   \n",
       "1         -0.171687        -0.266767              -0.004564   \n",
       "2          0.870764         0.559319              -0.004564   \n",
       "3          0.872148         1.631346              -0.004564   \n",
       "4          0.081657        -0.316411              -0.004564   \n",
       "\n",
       "   ('paramId_0', 'amax')  ('paramId_0', 'mean')  ('paramId_0', 'var')  \\\n",
       "0               -0.45347              -0.351478             -0.347880   \n",
       "1               -0.45347              -0.351478             -0.347880   \n",
       "2               -0.45347              -0.351478             -0.347880   \n",
       "3                3.09219               4.271061              5.151983   \n",
       "4               -0.45347              -0.351478             -0.347880   \n",
       "\n",
       "   ('pres', 'count')  ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
       "0          -0.367394                          -1.0              -1.595566  \n",
       "1          -0.367394                          -1.0              -1.595566  \n",
       "2          -0.367394                          -1.0              -1.595566  \n",
       "3           3.812911                          -1.0              -1.595566  \n",
       "4          -0.367394                          -1.0              -1.595566  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(threeday_no_null_train.head())\n",
    "scaler.fit(threeday_no_null_train)\n",
    "threeday_part_features_train = pd.DataFrame(scaler.transform(threeday_no_null_train))\n",
    "threeday_part_features_train.columns = threeday_no_null_train.columns\n",
    "threeday_part_features_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>('cwat', 'amin')</th>\n",
       "      <th>('cwat', 'amax')</th>\n",
       "      <th>('cwat', 'mean')</th>\n",
       "      <th>('cwat', 'var')</th>\n",
       "      <th>('r', 'amin')</th>\n",
       "      <th>('r', 'amax')</th>\n",
       "      <th>('r', 'mean')</th>\n",
       "      <th>('r', 'var')</th>\n",
       "      <th>('tozne', 'amin')</th>\n",
       "      <th>('tozne', 'amax')</th>\n",
       "      <th>...</th>\n",
       "      <th>('pwat', 'amax')</th>\n",
       "      <th>('pwat', 'mean')</th>\n",
       "      <th>('pwat', 'var')</th>\n",
       "      <th>('paramId_0', 'amin')</th>\n",
       "      <th>('paramId_0', 'amax')</th>\n",
       "      <th>('paramId_0', 'mean')</th>\n",
       "      <th>('paramId_0', 'var')</th>\n",
       "      <th>('pres', 'count')</th>\n",
       "      <th>('macro_season', '&lt;lambda&gt;')</th>\n",
       "      <th>('month', '&lt;lambda&gt;')</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.881970</td>\n",
       "      <td>0.384626</td>\n",
       "      <td>1.157432</td>\n",
       "      <td>-0.099408</td>\n",
       "      <td>0.649572</td>\n",
       "      <td>1.430530</td>\n",
       "      <td>1.101294</td>\n",
       "      <td>1.723148</td>\n",
       "      <td>0.032570</td>\n",
       "      <td>0.771659</td>\n",
       "      <td>...</td>\n",
       "      <td>1.546735</td>\n",
       "      <td>0.730940</td>\n",
       "      <td>2.790531</td>\n",
       "      <td>-0.004564</td>\n",
       "      <td>-0.45347</td>\n",
       "      <td>-0.351478</td>\n",
       "      <td>-0.34788</td>\n",
       "      <td>-0.367394</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.595566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.881970</td>\n",
       "      <td>-0.308132</td>\n",
       "      <td>0.404418</td>\n",
       "      <td>-0.308614</td>\n",
       "      <td>0.992861</td>\n",
       "      <td>0.801241</td>\n",
       "      <td>0.801013</td>\n",
       "      <td>0.165690</td>\n",
       "      <td>-1.124747</td>\n",
       "      <td>-0.881178</td>\n",
       "      <td>...</td>\n",
       "      <td>0.958160</td>\n",
       "      <td>0.822310</td>\n",
       "      <td>0.674560</td>\n",
       "      <td>-0.004564</td>\n",
       "      <td>-0.45347</td>\n",
       "      <td>-0.351478</td>\n",
       "      <td>-0.34788</td>\n",
       "      <td>-0.367394</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.595566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.148129</td>\n",
       "      <td>-0.410763</td>\n",
       "      <td>-0.532257</td>\n",
       "      <td>-0.320698</td>\n",
       "      <td>-1.238517</td>\n",
       "      <td>-0.727035</td>\n",
       "      <td>-1.300956</td>\n",
       "      <td>-0.454099</td>\n",
       "      <td>-1.252089</td>\n",
       "      <td>-1.351075</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.619724</td>\n",
       "      <td>-0.978791</td>\n",
       "      <td>-0.459343</td>\n",
       "      <td>-0.004564</td>\n",
       "      <td>-0.45347</td>\n",
       "      <td>-0.351478</td>\n",
       "      <td>-0.34788</td>\n",
       "      <td>-0.367394</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.595566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.148129</td>\n",
       "      <td>-0.667341</td>\n",
       "      <td>-0.624088</td>\n",
       "      <td>-0.334043</td>\n",
       "      <td>-1.066872</td>\n",
       "      <td>-1.086629</td>\n",
       "      <td>-1.322405</td>\n",
       "      <td>-0.591474</td>\n",
       "      <td>-1.709023</td>\n",
       "      <td>-1.091538</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.619724</td>\n",
       "      <td>-0.916493</td>\n",
       "      <td>-0.391374</td>\n",
       "      <td>-0.004564</td>\n",
       "      <td>-0.45347</td>\n",
       "      <td>-0.351478</td>\n",
       "      <td>-0.34788</td>\n",
       "      <td>-0.367394</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.595566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.148129</td>\n",
       "      <td>-0.282475</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>-0.275130</td>\n",
       "      <td>-1.066872</td>\n",
       "      <td>-1.446223</td>\n",
       "      <td>-1.333129</td>\n",
       "      <td>-0.725456</td>\n",
       "      <td>-1.053585</td>\n",
       "      <td>-0.982260</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.258391</td>\n",
       "      <td>-1.050780</td>\n",
       "      <td>-0.689218</td>\n",
       "      <td>-0.004564</td>\n",
       "      <td>-0.45347</td>\n",
       "      <td>-0.351478</td>\n",
       "      <td>-0.34788</td>\n",
       "      <td>-0.367394</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.595566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
       "0          1.881970          0.384626          1.157432        -0.099408   \n",
       "1          1.881970         -0.308132          0.404418        -0.308614   \n",
       "2         -0.148129         -0.410763         -0.532257        -0.320698   \n",
       "3         -0.148129         -0.667341         -0.624088        -0.334043   \n",
       "4         -0.148129         -0.282475          0.000362        -0.275130   \n",
       "\n",
       "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
       "0       0.649572       1.430530       1.101294      1.723148   \n",
       "1       0.992861       0.801241       0.801013      0.165690   \n",
       "2      -1.238517      -0.727035      -1.300956     -0.454099   \n",
       "3      -1.066872      -1.086629      -1.322405     -0.591474   \n",
       "4      -1.066872      -1.446223      -1.333129     -0.725456   \n",
       "\n",
       "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('pwat', 'amax')  \\\n",
       "0           0.032570           0.771659  ...          1.546735   \n",
       "1          -1.124747          -0.881178  ...          0.958160   \n",
       "2          -1.252089          -1.351075  ...         -0.619724   \n",
       "3          -1.709023          -1.091538  ...         -0.619724   \n",
       "4          -1.053585          -0.982260  ...         -1.258391   \n",
       "\n",
       "   ('pwat', 'mean')  ('pwat', 'var')  ('paramId_0', 'amin')  \\\n",
       "0          0.730940         2.790531              -0.004564   \n",
       "1          0.822310         0.674560              -0.004564   \n",
       "2         -0.978791        -0.459343              -0.004564   \n",
       "3         -0.916493        -0.391374              -0.004564   \n",
       "4         -1.050780        -0.689218              -0.004564   \n",
       "\n",
       "   ('paramId_0', 'amax')  ('paramId_0', 'mean')  ('paramId_0', 'var')  \\\n",
       "0               -0.45347              -0.351478              -0.34788   \n",
       "1               -0.45347              -0.351478              -0.34788   \n",
       "2               -0.45347              -0.351478              -0.34788   \n",
       "3               -0.45347              -0.351478              -0.34788   \n",
       "4               -0.45347              -0.351478              -0.34788   \n",
       "\n",
       "   ('pres', 'count')  ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
       "0          -0.367394                          -1.0              -1.595566  \n",
       "1          -0.367394                          -1.0              -1.595566  \n",
       "2          -0.367394                          -1.0              -1.595566  \n",
       "3          -0.367394                          -1.0              -1.595566  \n",
       "4          -0.367394                          -1.0              -1.595566  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threeday_part_features_test = pd.DataFrame(scaler.transform(threeday_no_null_test))\n",
    "threeday_part_features_test.columns = threeday_no_null_test.columns\n",
    "threeday_part_features_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-Day Average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Full Feature Training and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
      "0              0.03              0.24            0.1155         0.003500   \n",
      "1              0.00              0.42            0.0910         0.016104   \n",
      "2              0.00              0.22            0.0605         0.005521   \n",
      "3              0.00              0.22            0.0835         0.005003   \n",
      "4              0.00              0.15            0.0440         0.002920   \n",
      "\n",
      "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
      "0            8.0           19.0          13.35     11.923684   \n",
      "1           15.0           48.0          29.90     98.831579   \n",
      "2           17.0           35.0          25.60     28.673684   \n",
      "3           10.0           31.0          21.25     34.828947   \n",
      "4           18.0           39.0          26.65     29.502632   \n",
      "\n",
      "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('paramId_0', 'amax')  \\\n",
      "0         263.500000         286.399994  ...                    0.0   \n",
      "1         267.500000         304.299988  ...                   59.0   \n",
      "2         284.100006         360.100006  ...                   53.0   \n",
      "3         270.799988         317.600006  ...                    0.0   \n",
      "4         244.000000         292.600006  ...                   14.0   \n",
      "\n",
      "   ('paramId_0', 'mean')  ('paramId_0', 'var')  ('pres', 'amin')  \\\n",
      "0                   0.00              0.000000           45110.0   \n",
      "1                   2.95            174.050000           34430.0   \n",
      "2                   6.40            274.884211           61430.0   \n",
      "3                   0.00              0.000000           45110.0   \n",
      "4                   0.70              9.800000           73310.0   \n",
      "\n",
      "   ('pres', 'amax')  ('pres', 'mean')  ('pres', 'var')  ('pres', 'count')  \\\n",
      "0           55220.0      50786.000000     4.265709e+07                0.0   \n",
      "1           34430.0      34430.000000     4.265709e+07                1.0   \n",
      "2           62960.0      62443.333333     7.702333e+05                3.0   \n",
      "3           55220.0      50786.000000     4.265709e+07                0.0   \n",
      "4           73310.0      73310.000000     4.265709e+07                1.0   \n",
      "\n",
      "   ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
      "0                           0.0                    1.0  \n",
      "1                           0.0                    1.0  \n",
      "2                           0.0                    1.0  \n",
      "3                           0.0                    1.0  \n",
      "4                           0.0                    1.0  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>('cwat', 'amin')</th>\n",
       "      <th>('cwat', 'amax')</th>\n",
       "      <th>('cwat', 'mean')</th>\n",
       "      <th>('cwat', 'var')</th>\n",
       "      <th>('r', 'amin')</th>\n",
       "      <th>('r', 'amax')</th>\n",
       "      <th>('r', 'mean')</th>\n",
       "      <th>('r', 'var')</th>\n",
       "      <th>('tozne', 'amin')</th>\n",
       "      <th>('tozne', 'amax')</th>\n",
       "      <th>...</th>\n",
       "      <th>('paramId_0', 'amax')</th>\n",
       "      <th>('paramId_0', 'mean')</th>\n",
       "      <th>('paramId_0', 'var')</th>\n",
       "      <th>('pres', 'amin')</th>\n",
       "      <th>('pres', 'amax')</th>\n",
       "      <th>('pres', 'mean')</th>\n",
       "      <th>('pres', 'var')</th>\n",
       "      <th>('pres', 'count')</th>\n",
       "      <th>('macro_season', '&lt;lambda&gt;')</th>\n",
       "      <th>('month', '&lt;lambda&gt;')</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.934038</td>\n",
       "      <td>-0.248653</td>\n",
       "      <td>0.742559</td>\n",
       "      <td>-0.337565</td>\n",
       "      <td>-0.543963</td>\n",
       "      <td>-0.970474</td>\n",
       "      <td>-0.768796</td>\n",
       "      <td>-0.652634</td>\n",
       "      <td>-0.962008</td>\n",
       "      <td>-1.213342</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.562812</td>\n",
       "      <td>-0.413642</td>\n",
       "      <td>-0.405075</td>\n",
       "      <td>-0.040903</td>\n",
       "      <td>0.043032</td>\n",
       "      <td>0.025828</td>\n",
       "      <td>-0.124598</td>\n",
       "      <td>-0.433501</td>\n",
       "      <td>-1.001717</td>\n",
       "      <td>-1.597701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.101148</td>\n",
       "      <td>0.151943</td>\n",
       "      <td>0.433620</td>\n",
       "      <td>-0.109614</td>\n",
       "      <td>0.861831</td>\n",
       "      <td>1.618443</td>\n",
       "      <td>1.541509</td>\n",
       "      <td>1.429354</td>\n",
       "      <td>-0.802029</td>\n",
       "      <td>-0.740244</td>\n",
       "      <td>...</td>\n",
       "      <td>2.485583</td>\n",
       "      <td>0.616839</td>\n",
       "      <td>1.358633</td>\n",
       "      <td>-1.675122</td>\n",
       "      <td>-2.874917</td>\n",
       "      <td>-2.553010</td>\n",
       "      <td>-0.124598</td>\n",
       "      <td>0.306827</td>\n",
       "      <td>-1.001717</td>\n",
       "      <td>-1.597701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.101148</td>\n",
       "      <td>-0.293164</td>\n",
       "      <td>0.049022</td>\n",
       "      <td>-0.301015</td>\n",
       "      <td>1.263486</td>\n",
       "      <td>0.457894</td>\n",
       "      <td>0.941249</td>\n",
       "      <td>-0.251367</td>\n",
       "      <td>-0.138117</td>\n",
       "      <td>0.734555</td>\n",
       "      <td>...</td>\n",
       "      <td>2.175577</td>\n",
       "      <td>1.821979</td>\n",
       "      <td>2.380421</td>\n",
       "      <td>2.456332</td>\n",
       "      <td>1.129368</td>\n",
       "      <td>1.863831</td>\n",
       "      <td>-1.155228</td>\n",
       "      <td>1.787484</td>\n",
       "      <td>-1.001717</td>\n",
       "      <td>-1.597701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.101148</td>\n",
       "      <td>-0.293164</td>\n",
       "      <td>0.339046</td>\n",
       "      <td>-0.310381</td>\n",
       "      <td>-0.142307</td>\n",
       "      <td>0.100802</td>\n",
       "      <td>0.334008</td>\n",
       "      <td>-0.103910</td>\n",
       "      <td>-0.670047</td>\n",
       "      <td>-0.388723</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.562812</td>\n",
       "      <td>-0.413642</td>\n",
       "      <td>-0.405075</td>\n",
       "      <td>-0.040903</td>\n",
       "      <td>0.043032</td>\n",
       "      <td>0.025828</td>\n",
       "      <td>-0.124598</td>\n",
       "      <td>-0.433501</td>\n",
       "      <td>-1.001717</td>\n",
       "      <td>-1.597701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.101148</td>\n",
       "      <td>-0.448951</td>\n",
       "      <td>-0.159039</td>\n",
       "      <td>-0.348050</td>\n",
       "      <td>1.464314</td>\n",
       "      <td>0.814986</td>\n",
       "      <td>1.087824</td>\n",
       "      <td>-0.231508</td>\n",
       "      <td>-1.741905</td>\n",
       "      <td>-1.049475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160536</td>\n",
       "      <td>-0.169121</td>\n",
       "      <td>-0.305768</td>\n",
       "      <td>4.274172</td>\n",
       "      <td>2.582026</td>\n",
       "      <td>3.577170</td>\n",
       "      <td>-0.124598</td>\n",
       "      <td>0.306827</td>\n",
       "      <td>-1.001717</td>\n",
       "      <td>-1.597701</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
       "0          7.934038         -0.248653          0.742559        -0.337565   \n",
       "1         -0.101148          0.151943          0.433620        -0.109614   \n",
       "2         -0.101148         -0.293164          0.049022        -0.301015   \n",
       "3         -0.101148         -0.293164          0.339046        -0.310381   \n",
       "4         -0.101148         -0.448951         -0.159039        -0.348050   \n",
       "\n",
       "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
       "0      -0.543963      -0.970474      -0.768796     -0.652634   \n",
       "1       0.861831       1.618443       1.541509      1.429354   \n",
       "2       1.263486       0.457894       0.941249     -0.251367   \n",
       "3      -0.142307       0.100802       0.334008     -0.103910   \n",
       "4       1.464314       0.814986       1.087824     -0.231508   \n",
       "\n",
       "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('paramId_0', 'amax')  \\\n",
       "0          -0.962008          -1.213342  ...              -0.562812   \n",
       "1          -0.802029          -0.740244  ...               2.485583   \n",
       "2          -0.138117           0.734555  ...               2.175577   \n",
       "3          -0.670047          -0.388723  ...              -0.562812   \n",
       "4          -1.741905          -1.049475  ...               0.160536   \n",
       "\n",
       "   ('paramId_0', 'mean')  ('paramId_0', 'var')  ('pres', 'amin')  \\\n",
       "0              -0.413642             -0.405075         -0.040903   \n",
       "1               0.616839              1.358633         -1.675122   \n",
       "2               1.821979              2.380421          2.456332   \n",
       "3              -0.413642             -0.405075         -0.040903   \n",
       "4              -0.169121             -0.305768          4.274172   \n",
       "\n",
       "   ('pres', 'amax')  ('pres', 'mean')  ('pres', 'var')  ('pres', 'count')  \\\n",
       "0          0.043032          0.025828        -0.124598          -0.433501   \n",
       "1         -2.874917         -2.553010        -0.124598           0.306827   \n",
       "2          1.129368          1.863831        -1.155228           1.787484   \n",
       "3          0.043032          0.025828        -0.124598          -0.433501   \n",
       "4          2.582026          3.577170        -0.124598           0.306827   \n",
       "\n",
       "   ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
       "0                     -1.001717              -1.597701  \n",
       "1                     -1.001717              -1.597701  \n",
       "2                     -1.001717              -1.597701  \n",
       "3                     -1.001717              -1.597701  \n",
       "4                     -1.001717              -1.597701  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(fiveday_imp_train.head())\n",
    "scaler.fit(fiveday_imp_train)\n",
    "fiveday_full_features_train = pd.DataFrame(scaler.transform(fiveday_imp_train))\n",
    "fiveday_full_features_train.columns = fiveday_imp_train.columns\n",
    "fiveday_full_features_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>('cwat', 'amin')</th>\n",
       "      <th>('cwat', 'amax')</th>\n",
       "      <th>('cwat', 'mean')</th>\n",
       "      <th>('cwat', 'var')</th>\n",
       "      <th>('r', 'amin')</th>\n",
       "      <th>('r', 'amax')</th>\n",
       "      <th>('r', 'mean')</th>\n",
       "      <th>('r', 'var')</th>\n",
       "      <th>('tozne', 'amin')</th>\n",
       "      <th>('tozne', 'amax')</th>\n",
       "      <th>...</th>\n",
       "      <th>('paramId_0', 'amax')</th>\n",
       "      <th>('paramId_0', 'mean')</th>\n",
       "      <th>('paramId_0', 'var')</th>\n",
       "      <th>('pres', 'amin')</th>\n",
       "      <th>('pres', 'amax')</th>\n",
       "      <th>('pres', 'mean')</th>\n",
       "      <th>('pres', 'var')</th>\n",
       "      <th>('pres', 'count')</th>\n",
       "      <th>('macro_season', '&lt;lambda&gt;')</th>\n",
       "      <th>('month', '&lt;lambda&gt;')</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.101148</td>\n",
       "      <td>0.129688</td>\n",
       "      <td>0.811912</td>\n",
       "      <td>-0.190901</td>\n",
       "      <td>1.062659</td>\n",
       "      <td>1.172078</td>\n",
       "      <td>1.220440</td>\n",
       "      <td>0.749753</td>\n",
       "      <td>-0.582059</td>\n",
       "      <td>0.568045</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.562812</td>\n",
       "      <td>-0.413642</td>\n",
       "      <td>-0.405075</td>\n",
       "      <td>-0.066916</td>\n",
       "      <td>-0.046795</td>\n",
       "      <td>-0.035032</td>\n",
       "      <td>-0.113124</td>\n",
       "      <td>-0.433501</td>\n",
       "      <td>-1.001717</td>\n",
       "      <td>-1.597701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.101148</td>\n",
       "      <td>-0.471206</td>\n",
       "      <td>-0.089685</td>\n",
       "      <td>-0.337565</td>\n",
       "      <td>-1.146446</td>\n",
       "      <td>-0.434836</td>\n",
       "      <td>-0.705978</td>\n",
       "      <td>0.278697</td>\n",
       "      <td>-1.141984</td>\n",
       "      <td>-1.480286</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.562812</td>\n",
       "      <td>-0.413642</td>\n",
       "      <td>-0.405075</td>\n",
       "      <td>-0.066916</td>\n",
       "      <td>-0.046795</td>\n",
       "      <td>-0.035032</td>\n",
       "      <td>-0.113124</td>\n",
       "      <td>-0.433501</td>\n",
       "      <td>-1.001717</td>\n",
       "      <td>-1.597701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.101148</td>\n",
       "      <td>-0.582483</td>\n",
       "      <td>-0.606685</td>\n",
       "      <td>-0.389764</td>\n",
       "      <td>-0.945618</td>\n",
       "      <td>-1.327567</td>\n",
       "      <td>-1.452814</td>\n",
       "      <td>-0.783259</td>\n",
       "      <td>-1.629920</td>\n",
       "      <td>-1.128765</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.562812</td>\n",
       "      <td>-0.413642</td>\n",
       "      <td>-0.405075</td>\n",
       "      <td>-0.066916</td>\n",
       "      <td>-0.046795</td>\n",
       "      <td>-0.035032</td>\n",
       "      <td>-0.113124</td>\n",
       "      <td>-0.433501</td>\n",
       "      <td>-1.001717</td>\n",
       "      <td>-1.597701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.101148</td>\n",
       "      <td>2.177179</td>\n",
       "      <td>1.581107</td>\n",
       "      <td>1.034438</td>\n",
       "      <td>-0.543963</td>\n",
       "      <td>2.064808</td>\n",
       "      <td>0.271190</td>\n",
       "      <td>5.150643</td>\n",
       "      <td>-1.237971</td>\n",
       "      <td>-1.258273</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.562812</td>\n",
       "      <td>-0.413642</td>\n",
       "      <td>-0.405075</td>\n",
       "      <td>-0.066916</td>\n",
       "      <td>-0.046795</td>\n",
       "      <td>-0.035032</td>\n",
       "      <td>-0.113124</td>\n",
       "      <td>-0.433501</td>\n",
       "      <td>-1.001717</td>\n",
       "      <td>-1.597701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.101148</td>\n",
       "      <td>-0.337674</td>\n",
       "      <td>0.175120</td>\n",
       "      <td>-0.321384</td>\n",
       "      <td>1.263486</td>\n",
       "      <td>0.993532</td>\n",
       "      <td>1.513590</td>\n",
       "      <td>0.567433</td>\n",
       "      <td>-1.765902</td>\n",
       "      <td>-1.081192</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.562812</td>\n",
       "      <td>-0.413642</td>\n",
       "      <td>-0.405075</td>\n",
       "      <td>-0.066916</td>\n",
       "      <td>-0.046795</td>\n",
       "      <td>-0.035032</td>\n",
       "      <td>-0.113124</td>\n",
       "      <td>-0.433501</td>\n",
       "      <td>-1.001717</td>\n",
       "      <td>-1.597701</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
       "0         -0.101148          0.129688          0.811912        -0.190901   \n",
       "1         -0.101148         -0.471206         -0.089685        -0.337565   \n",
       "2         -0.101148         -0.582483         -0.606685        -0.389764   \n",
       "3         -0.101148          2.177179          1.581107         1.034438   \n",
       "4         -0.101148         -0.337674          0.175120        -0.321384   \n",
       "\n",
       "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
       "0       1.062659       1.172078       1.220440      0.749753   \n",
       "1      -1.146446      -0.434836      -0.705978      0.278697   \n",
       "2      -0.945618      -1.327567      -1.452814     -0.783259   \n",
       "3      -0.543963       2.064808       0.271190      5.150643   \n",
       "4       1.263486       0.993532       1.513590      0.567433   \n",
       "\n",
       "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('paramId_0', 'amax')  \\\n",
       "0          -0.582059           0.568045  ...              -0.562812   \n",
       "1          -1.141984          -1.480286  ...              -0.562812   \n",
       "2          -1.629920          -1.128765  ...              -0.562812   \n",
       "3          -1.237971          -1.258273  ...              -0.562812   \n",
       "4          -1.765902          -1.081192  ...              -0.562812   \n",
       "\n",
       "   ('paramId_0', 'mean')  ('paramId_0', 'var')  ('pres', 'amin')  \\\n",
       "0              -0.413642             -0.405075         -0.066916   \n",
       "1              -0.413642             -0.405075         -0.066916   \n",
       "2              -0.413642             -0.405075         -0.066916   \n",
       "3              -0.413642             -0.405075         -0.066916   \n",
       "4              -0.413642             -0.405075         -0.066916   \n",
       "\n",
       "   ('pres', 'amax')  ('pres', 'mean')  ('pres', 'var')  ('pres', 'count')  \\\n",
       "0         -0.046795         -0.035032        -0.113124          -0.433501   \n",
       "1         -0.046795         -0.035032        -0.113124          -0.433501   \n",
       "2         -0.046795         -0.035032        -0.113124          -0.433501   \n",
       "3         -0.046795         -0.035032        -0.113124          -0.433501   \n",
       "4         -0.046795         -0.035032        -0.113124          -0.433501   \n",
       "\n",
       "   ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
       "0                     -1.001717              -1.597701  \n",
       "1                     -1.001717              -1.597701  \n",
       "2                     -1.001717              -1.597701  \n",
       "3                     -1.001717              -1.597701  \n",
       "4                     -1.001717              -1.597701  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fiveday_full_features_test = pd.DataFrame(scaler.transform(fiveday_imp_test))\n",
    "fiveday_full_features_test.columns = fiveday_imp_test.columns\n",
    "fiveday_full_features_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Features With No Nulls Training and Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
      "0              0.03              0.24            0.1155         0.003500   \n",
      "1              0.00              0.42            0.0910         0.016104   \n",
      "2              0.00              0.22            0.0605         0.005521   \n",
      "3              0.00              0.22            0.0835         0.005003   \n",
      "4              0.00              0.15            0.0440         0.002920   \n",
      "\n",
      "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
      "0            8.0           19.0          13.35     11.923684   \n",
      "1           15.0           48.0          29.90     98.831579   \n",
      "2           17.0           35.0          25.60     28.673684   \n",
      "3           10.0           31.0          21.25     34.828947   \n",
      "4           18.0           39.0          26.65     29.502632   \n",
      "\n",
      "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('pwat', 'amax')  \\\n",
      "0         263.500000         286.399994  ...         13.300000   \n",
      "1         267.500000         304.299988  ...         28.500000   \n",
      "2         284.100006         360.100006  ...         17.700001   \n",
      "3         270.799988         317.600006  ...         18.299999   \n",
      "4         244.000000         292.600006  ...         22.600000   \n",
      "\n",
      "   ('pwat', 'mean')  ('pwat', 'var')  ('paramId_0', 'amin')  \\\n",
      "0             9.260         4.569895                    0.0   \n",
      "1            18.080        31.830103                    0.0   \n",
      "2            13.325         5.106185                    0.0   \n",
      "3            13.045        10.571026                    0.0   \n",
      "4            15.500        12.087369                    0.0   \n",
      "\n",
      "   ('paramId_0', 'amax')  ('paramId_0', 'mean')  ('paramId_0', 'var')  \\\n",
      "0                    0.0                   0.00              0.000000   \n",
      "1                   59.0                   2.95            174.050000   \n",
      "2                   53.0                   6.40            274.884211   \n",
      "3                    0.0                   0.00              0.000000   \n",
      "4                   14.0                   0.70              9.800000   \n",
      "\n",
      "   ('pres', 'count')  ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
      "0                  0                             0                      1  \n",
      "1                  1                             0                      1  \n",
      "2                  3                             0                      1  \n",
      "3                  0                             0                      1  \n",
      "4                  1                             0                      1  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>('cwat', 'amin')</th>\n",
       "      <th>('cwat', 'amax')</th>\n",
       "      <th>('cwat', 'mean')</th>\n",
       "      <th>('cwat', 'var')</th>\n",
       "      <th>('r', 'amin')</th>\n",
       "      <th>('r', 'amax')</th>\n",
       "      <th>('r', 'mean')</th>\n",
       "      <th>('r', 'var')</th>\n",
       "      <th>('tozne', 'amin')</th>\n",
       "      <th>('tozne', 'amax')</th>\n",
       "      <th>...</th>\n",
       "      <th>('pwat', 'amax')</th>\n",
       "      <th>('pwat', 'mean')</th>\n",
       "      <th>('pwat', 'var')</th>\n",
       "      <th>('paramId_0', 'amin')</th>\n",
       "      <th>('paramId_0', 'amax')</th>\n",
       "      <th>('paramId_0', 'mean')</th>\n",
       "      <th>('paramId_0', 'var')</th>\n",
       "      <th>('pres', 'count')</th>\n",
       "      <th>('macro_season', '&lt;lambda&gt;')</th>\n",
       "      <th>('month', '&lt;lambda&gt;')</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.934038</td>\n",
       "      <td>-0.248653</td>\n",
       "      <td>0.742559</td>\n",
       "      <td>-0.337565</td>\n",
       "      <td>-0.543963</td>\n",
       "      <td>-0.970474</td>\n",
       "      <td>-0.768796</td>\n",
       "      <td>-0.652634</td>\n",
       "      <td>-0.962008</td>\n",
       "      <td>-1.213342</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.695622</td>\n",
       "      <td>-0.543937</td>\n",
       "      <td>-0.584737</td>\n",
       "      <td>-0.003712</td>\n",
       "      <td>-0.562812</td>\n",
       "      <td>-0.413642</td>\n",
       "      <td>-0.405075</td>\n",
       "      <td>-0.433501</td>\n",
       "      <td>-1.001717</td>\n",
       "      <td>-1.597701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.101148</td>\n",
       "      <td>0.151943</td>\n",
       "      <td>0.433620</td>\n",
       "      <td>-0.109614</td>\n",
       "      <td>0.861831</td>\n",
       "      <td>1.618443</td>\n",
       "      <td>1.541509</td>\n",
       "      <td>1.429354</td>\n",
       "      <td>-0.802029</td>\n",
       "      <td>-0.740244</td>\n",
       "      <td>...</td>\n",
       "      <td>1.148237</td>\n",
       "      <td>1.000397</td>\n",
       "      <td>0.976080</td>\n",
       "      <td>-0.003712</td>\n",
       "      <td>2.485583</td>\n",
       "      <td>0.616839</td>\n",
       "      <td>1.358633</td>\n",
       "      <td>0.306827</td>\n",
       "      <td>-1.001717</td>\n",
       "      <td>-1.597701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.101148</td>\n",
       "      <td>-0.293164</td>\n",
       "      <td>0.049022</td>\n",
       "      <td>-0.301015</td>\n",
       "      <td>1.263486</td>\n",
       "      <td>0.457894</td>\n",
       "      <td>0.941249</td>\n",
       "      <td>-0.251367</td>\n",
       "      <td>-0.138117</td>\n",
       "      <td>0.734555</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.161873</td>\n",
       "      <td>0.167823</td>\n",
       "      <td>-0.554031</td>\n",
       "      <td>-0.003712</td>\n",
       "      <td>2.175577</td>\n",
       "      <td>1.821979</td>\n",
       "      <td>2.380421</td>\n",
       "      <td>1.787484</td>\n",
       "      <td>-1.001717</td>\n",
       "      <td>-1.597701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.101148</td>\n",
       "      <td>-0.293164</td>\n",
       "      <td>0.339046</td>\n",
       "      <td>-0.310381</td>\n",
       "      <td>-0.142307</td>\n",
       "      <td>0.100802</td>\n",
       "      <td>0.334008</td>\n",
       "      <td>-0.103910</td>\n",
       "      <td>-0.670047</td>\n",
       "      <td>-0.388723</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.089089</td>\n",
       "      <td>0.118796</td>\n",
       "      <td>-0.241134</td>\n",
       "      <td>-0.003712</td>\n",
       "      <td>-0.562812</td>\n",
       "      <td>-0.413642</td>\n",
       "      <td>-0.405075</td>\n",
       "      <td>-0.433501</td>\n",
       "      <td>-1.001717</td>\n",
       "      <td>-1.597701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.101148</td>\n",
       "      <td>-0.448951</td>\n",
       "      <td>-0.159039</td>\n",
       "      <td>-0.348050</td>\n",
       "      <td>1.464314</td>\n",
       "      <td>0.814986</td>\n",
       "      <td>1.087824</td>\n",
       "      <td>-0.231508</td>\n",
       "      <td>-1.741905</td>\n",
       "      <td>-1.049475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.432529</td>\n",
       "      <td>0.548653</td>\n",
       "      <td>-0.154314</td>\n",
       "      <td>-0.003712</td>\n",
       "      <td>0.160536</td>\n",
       "      <td>-0.169121</td>\n",
       "      <td>-0.305768</td>\n",
       "      <td>0.306827</td>\n",
       "      <td>-1.001717</td>\n",
       "      <td>-1.597701</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
       "0          7.934038         -0.248653          0.742559        -0.337565   \n",
       "1         -0.101148          0.151943          0.433620        -0.109614   \n",
       "2         -0.101148         -0.293164          0.049022        -0.301015   \n",
       "3         -0.101148         -0.293164          0.339046        -0.310381   \n",
       "4         -0.101148         -0.448951         -0.159039        -0.348050   \n",
       "\n",
       "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
       "0      -0.543963      -0.970474      -0.768796     -0.652634   \n",
       "1       0.861831       1.618443       1.541509      1.429354   \n",
       "2       1.263486       0.457894       0.941249     -0.251367   \n",
       "3      -0.142307       0.100802       0.334008     -0.103910   \n",
       "4       1.464314       0.814986       1.087824     -0.231508   \n",
       "\n",
       "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('pwat', 'amax')  \\\n",
       "0          -0.962008          -1.213342  ...         -0.695622   \n",
       "1          -0.802029          -0.740244  ...          1.148237   \n",
       "2          -0.138117           0.734555  ...         -0.161873   \n",
       "3          -0.670047          -0.388723  ...         -0.089089   \n",
       "4          -1.741905          -1.049475  ...          0.432529   \n",
       "\n",
       "   ('pwat', 'mean')  ('pwat', 'var')  ('paramId_0', 'amin')  \\\n",
       "0         -0.543937        -0.584737              -0.003712   \n",
       "1          1.000397         0.976080              -0.003712   \n",
       "2          0.167823        -0.554031              -0.003712   \n",
       "3          0.118796        -0.241134              -0.003712   \n",
       "4          0.548653        -0.154314              -0.003712   \n",
       "\n",
       "   ('paramId_0', 'amax')  ('paramId_0', 'mean')  ('paramId_0', 'var')  \\\n",
       "0              -0.562812              -0.413642             -0.405075   \n",
       "1               2.485583               0.616839              1.358633   \n",
       "2               2.175577               1.821979              2.380421   \n",
       "3              -0.562812              -0.413642             -0.405075   \n",
       "4               0.160536              -0.169121             -0.305768   \n",
       "\n",
       "   ('pres', 'count')  ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
       "0          -0.433501                     -1.001717              -1.597701  \n",
       "1           0.306827                     -1.001717              -1.597701  \n",
       "2           1.787484                     -1.001717              -1.597701  \n",
       "3          -0.433501                     -1.001717              -1.597701  \n",
       "4           0.306827                     -1.001717              -1.597701  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(fiveday_no_null_train.head())\n",
    "scaler.fit(fiveday_no_null_train)\n",
    "fiveday_part_features_train = pd.DataFrame(scaler.transform(fiveday_no_null_train))\n",
    "fiveday_part_features_train.columns = fiveday_no_null_train.columns\n",
    "fiveday_part_features_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>('cwat', 'amin')</th>\n",
       "      <th>('cwat', 'amax')</th>\n",
       "      <th>('cwat', 'mean')</th>\n",
       "      <th>('cwat', 'var')</th>\n",
       "      <th>('r', 'amin')</th>\n",
       "      <th>('r', 'amax')</th>\n",
       "      <th>('r', 'mean')</th>\n",
       "      <th>('r', 'var')</th>\n",
       "      <th>('tozne', 'amin')</th>\n",
       "      <th>('tozne', 'amax')</th>\n",
       "      <th>...</th>\n",
       "      <th>('pwat', 'amax')</th>\n",
       "      <th>('pwat', 'mean')</th>\n",
       "      <th>('pwat', 'var')</th>\n",
       "      <th>('paramId_0', 'amin')</th>\n",
       "      <th>('paramId_0', 'amax')</th>\n",
       "      <th>('paramId_0', 'mean')</th>\n",
       "      <th>('paramId_0', 'var')</th>\n",
       "      <th>('pres', 'count')</th>\n",
       "      <th>('macro_season', '&lt;lambda&gt;')</th>\n",
       "      <th>('month', '&lt;lambda&gt;')</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.101148</td>\n",
       "      <td>0.129688</td>\n",
       "      <td>0.811912</td>\n",
       "      <td>-0.190901</td>\n",
       "      <td>1.062659</td>\n",
       "      <td>1.172078</td>\n",
       "      <td>1.220440</td>\n",
       "      <td>0.749753</td>\n",
       "      <td>-0.582059</td>\n",
       "      <td>0.568045</td>\n",
       "      <td>...</td>\n",
       "      <td>1.293805</td>\n",
       "      <td>0.969756</td>\n",
       "      <td>1.490053</td>\n",
       "      <td>-0.003712</td>\n",
       "      <td>-0.562812</td>\n",
       "      <td>-0.413642</td>\n",
       "      <td>-0.405075</td>\n",
       "      <td>-0.433501</td>\n",
       "      <td>-1.001717</td>\n",
       "      <td>-1.597701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.101148</td>\n",
       "      <td>-0.471206</td>\n",
       "      <td>-0.089685</td>\n",
       "      <td>-0.337565</td>\n",
       "      <td>-1.146446</td>\n",
       "      <td>-0.434836</td>\n",
       "      <td>-0.705978</td>\n",
       "      <td>0.278697</td>\n",
       "      <td>-1.141984</td>\n",
       "      <td>-1.480286</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.137612</td>\n",
       "      <td>-0.475650</td>\n",
       "      <td>0.274053</td>\n",
       "      <td>-0.003712</td>\n",
       "      <td>-0.562812</td>\n",
       "      <td>-0.413642</td>\n",
       "      <td>-0.405075</td>\n",
       "      <td>-0.433501</td>\n",
       "      <td>-1.001717</td>\n",
       "      <td>-1.597701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.101148</td>\n",
       "      <td>-0.582483</td>\n",
       "      <td>-0.606685</td>\n",
       "      <td>-0.389764</td>\n",
       "      <td>-0.945618</td>\n",
       "      <td>-1.327567</td>\n",
       "      <td>-1.452814</td>\n",
       "      <td>-0.783259</td>\n",
       "      <td>-1.629920</td>\n",
       "      <td>-1.128765</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.804797</td>\n",
       "      <td>-1.032450</td>\n",
       "      <td>-0.628450</td>\n",
       "      <td>-0.003712</td>\n",
       "      <td>-0.562812</td>\n",
       "      <td>-0.413642</td>\n",
       "      <td>-0.405075</td>\n",
       "      <td>-0.433501</td>\n",
       "      <td>-1.001717</td>\n",
       "      <td>-1.597701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.101148</td>\n",
       "      <td>2.177179</td>\n",
       "      <td>1.581107</td>\n",
       "      <td>1.034438</td>\n",
       "      <td>-0.543963</td>\n",
       "      <td>2.064808</td>\n",
       "      <td>0.271190</td>\n",
       "      <td>5.150643</td>\n",
       "      <td>-1.237971</td>\n",
       "      <td>-1.258273</td>\n",
       "      <td>...</td>\n",
       "      <td>1.463634</td>\n",
       "      <td>0.034751</td>\n",
       "      <td>3.767212</td>\n",
       "      <td>-0.003712</td>\n",
       "      <td>-0.562812</td>\n",
       "      <td>-0.413642</td>\n",
       "      <td>-0.405075</td>\n",
       "      <td>-0.433501</td>\n",
       "      <td>-1.001717</td>\n",
       "      <td>-1.597701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.101148</td>\n",
       "      <td>-0.337674</td>\n",
       "      <td>0.175120</td>\n",
       "      <td>-0.321384</td>\n",
       "      <td>1.263486</td>\n",
       "      <td>0.993532</td>\n",
       "      <td>1.513590</td>\n",
       "      <td>0.567433</td>\n",
       "      <td>-1.765902</td>\n",
       "      <td>-1.081192</td>\n",
       "      <td>...</td>\n",
       "      <td>0.699403</td>\n",
       "      <td>0.966254</td>\n",
       "      <td>0.120713</td>\n",
       "      <td>-0.003712</td>\n",
       "      <td>-0.562812</td>\n",
       "      <td>-0.413642</td>\n",
       "      <td>-0.405075</td>\n",
       "      <td>-0.433501</td>\n",
       "      <td>-1.001717</td>\n",
       "      <td>-1.597701</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
       "0         -0.101148          0.129688          0.811912        -0.190901   \n",
       "1         -0.101148         -0.471206         -0.089685        -0.337565   \n",
       "2         -0.101148         -0.582483         -0.606685        -0.389764   \n",
       "3         -0.101148          2.177179          1.581107         1.034438   \n",
       "4         -0.101148         -0.337674          0.175120        -0.321384   \n",
       "\n",
       "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
       "0       1.062659       1.172078       1.220440      0.749753   \n",
       "1      -1.146446      -0.434836      -0.705978      0.278697   \n",
       "2      -0.945618      -1.327567      -1.452814     -0.783259   \n",
       "3      -0.543963       2.064808       0.271190      5.150643   \n",
       "4       1.263486       0.993532       1.513590      0.567433   \n",
       "\n",
       "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('pwat', 'amax')  \\\n",
       "0          -0.582059           0.568045  ...          1.293805   \n",
       "1          -1.141984          -1.480286  ...         -0.137612   \n",
       "2          -1.629920          -1.128765  ...         -0.804797   \n",
       "3          -1.237971          -1.258273  ...          1.463634   \n",
       "4          -1.765902          -1.081192  ...          0.699403   \n",
       "\n",
       "   ('pwat', 'mean')  ('pwat', 'var')  ('paramId_0', 'amin')  \\\n",
       "0          0.969756         1.490053              -0.003712   \n",
       "1         -0.475650         0.274053              -0.003712   \n",
       "2         -1.032450        -0.628450              -0.003712   \n",
       "3          0.034751         3.767212              -0.003712   \n",
       "4          0.966254         0.120713              -0.003712   \n",
       "\n",
       "   ('paramId_0', 'amax')  ('paramId_0', 'mean')  ('paramId_0', 'var')  \\\n",
       "0              -0.562812              -0.413642             -0.405075   \n",
       "1              -0.562812              -0.413642             -0.405075   \n",
       "2              -0.562812              -0.413642             -0.405075   \n",
       "3              -0.562812              -0.413642             -0.405075   \n",
       "4              -0.562812              -0.413642             -0.405075   \n",
       "\n",
       "   ('pres', 'count')  ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
       "0          -0.433501                     -1.001717              -1.597701  \n",
       "1          -0.433501                     -1.001717              -1.597701  \n",
       "2          -0.433501                     -1.001717              -1.597701  \n",
       "3          -0.433501                     -1.001717              -1.597701  \n",
       "4          -0.433501                     -1.001717              -1.597701  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fiveday_part_features_test = pd.DataFrame(scaler.transform(fiveday_no_null_test))\n",
    "fiveday_part_features_test.columns = fiveday_no_null_test.columns\n",
    "fiveday_part_features_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10-Day Average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Full Feature Training and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
      "0               0.0              0.42           0.10325         0.009705   \n",
      "1               0.0              0.22           0.07200         0.005263   \n",
      "2               0.0              0.43           0.05825         0.007605   \n",
      "3               0.0              0.32           0.03575         0.005051   \n",
      "4               0.0              0.69           0.11475         0.025159   \n",
      "\n",
      "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
      "0            8.0           48.0         21.625    124.189103   \n",
      "1           10.0           35.0         23.425     35.789103   \n",
      "2           13.0           55.0         27.250     77.935897   \n",
      "3            7.0           29.0         17.275     34.871154   \n",
      "4           18.0           50.0         27.250     39.987179   \n",
      "\n",
      "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('paramId_0', 'amax')  \\\n",
      "0         263.500000         304.299988  ...                   59.0   \n",
      "1         270.799988         360.100006  ...                   53.0   \n",
      "2         244.000000         334.700012  ...                   46.0   \n",
      "3         264.399994         348.000000  ...                    0.0   \n",
      "4         278.700012         407.299988  ...                   64.0   \n",
      "\n",
      "   ('paramId_0', 'mean')  ('paramId_0', 'var')  ('pres', 'amin')  \\\n",
      "0                  1.475             87.025000           34430.0   \n",
      "1                  3.200            144.420513           61430.0   \n",
      "2                  2.400             86.605128           65030.0   \n",
      "3                  0.000              0.000000           43120.0   \n",
      "4                 10.550            408.920513           41470.0   \n",
      "\n",
      "   ('pres', 'amax')  ('pres', 'mean')  ('pres', 'var')  ('pres', 'count')  \\\n",
      "0           34430.0      34430.000000     4.969550e+07                1.0   \n",
      "1           62960.0      62443.333333     7.702333e+05                3.0   \n",
      "2           75960.0      71433.333333     3.250763e+07                3.0   \n",
      "3           55910.0      50540.000000     4.969550e+07                0.0   \n",
      "4           76180.0      57815.000000     2.113109e+08               10.0   \n",
      "\n",
      "   ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
      "0                           0.0                    1.0  \n",
      "1                           0.0                    1.0  \n",
      "2                           0.0                    1.0  \n",
      "3                           0.0                    2.0  \n",
      "4                           0.0                    2.0  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>('cwat', 'amin')</th>\n",
       "      <th>('cwat', 'amax')</th>\n",
       "      <th>('cwat', 'mean')</th>\n",
       "      <th>('cwat', 'var')</th>\n",
       "      <th>('r', 'amin')</th>\n",
       "      <th>('r', 'amax')</th>\n",
       "      <th>('r', 'mean')</th>\n",
       "      <th>('r', 'var')</th>\n",
       "      <th>('tozne', 'amin')</th>\n",
       "      <th>('tozne', 'amax')</th>\n",
       "      <th>...</th>\n",
       "      <th>('paramId_0', 'amax')</th>\n",
       "      <th>('paramId_0', 'mean')</th>\n",
       "      <th>('paramId_0', 'var')</th>\n",
       "      <th>('pres', 'amin')</th>\n",
       "      <th>('pres', 'amax')</th>\n",
       "      <th>('pres', 'mean')</th>\n",
       "      <th>('pres', 'var')</th>\n",
       "      <th>('pres', 'count')</th>\n",
       "      <th>('macro_season', '&lt;lambda&gt;')</th>\n",
       "      <th>('month', '&lt;lambda&gt;')</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.041787</td>\n",
       "      <td>-0.178081</td>\n",
       "      <td>0.698012</td>\n",
       "      <td>-0.293179</td>\n",
       "      <td>-0.209981</td>\n",
       "      <td>1.268567</td>\n",
       "      <td>0.438169</td>\n",
       "      <td>1.704667</td>\n",
       "      <td>-0.752168</td>\n",
       "      <td>-0.975314</td>\n",
       "      <td>...</td>\n",
       "      <td>1.843341</td>\n",
       "      <td>0.125417</td>\n",
       "      <td>0.530528</td>\n",
       "      <td>-1.157448</td>\n",
       "      <td>-2.320225</td>\n",
       "      <td>-2.031007</td>\n",
       "      <td>-0.155444</td>\n",
       "      <td>-0.078436</td>\n",
       "      <td>-0.993174</td>\n",
       "      <td>-1.577616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.041787</td>\n",
       "      <td>-0.558465</td>\n",
       "      <td>0.229878</td>\n",
       "      <td>-0.387740</td>\n",
       "      <td>0.302413</td>\n",
       "      <td>0.092740</td>\n",
       "      <td>0.723553</td>\n",
       "      <td>-0.328350</td>\n",
       "      <td>-0.435469</td>\n",
       "      <td>0.452479</td>\n",
       "      <td>...</td>\n",
       "      <td>1.577662</td>\n",
       "      <td>0.868005</td>\n",
       "      <td>1.203807</td>\n",
       "      <td>2.229245</td>\n",
       "      <td>0.835298</td>\n",
       "      <td>1.575456</td>\n",
       "      <td>-1.080198</td>\n",
       "      <td>0.840588</td>\n",
       "      <td>-0.993174</td>\n",
       "      <td>-1.577616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.041787</td>\n",
       "      <td>-0.159062</td>\n",
       "      <td>0.023900</td>\n",
       "      <td>-0.337884</td>\n",
       "      <td>1.071004</td>\n",
       "      <td>1.901705</td>\n",
       "      <td>1.329995</td>\n",
       "      <td>0.640939</td>\n",
       "      <td>-1.598148</td>\n",
       "      <td>-0.197448</td>\n",
       "      <td>...</td>\n",
       "      <td>1.267704</td>\n",
       "      <td>0.523616</td>\n",
       "      <td>0.525603</td>\n",
       "      <td>2.680805</td>\n",
       "      <td>2.273147</td>\n",
       "      <td>2.732837</td>\n",
       "      <td>-0.480318</td>\n",
       "      <td>0.840588</td>\n",
       "      <td>-0.993174</td>\n",
       "      <td>-1.577616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.041787</td>\n",
       "      <td>-0.368273</td>\n",
       "      <td>-0.313157</td>\n",
       "      <td>-0.392250</td>\n",
       "      <td>-0.466177</td>\n",
       "      <td>-0.449949</td>\n",
       "      <td>-0.251511</td>\n",
       "      <td>-0.349461</td>\n",
       "      <td>-0.713123</td>\n",
       "      <td>0.142868</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.769165</td>\n",
       "      <td>-0.509550</td>\n",
       "      <td>-0.490321</td>\n",
       "      <td>-0.067434</td>\n",
       "      <td>0.055542</td>\n",
       "      <td>0.043010</td>\n",
       "      <td>-0.155444</td>\n",
       "      <td>-0.537948</td>\n",
       "      <td>-0.993174</td>\n",
       "      <td>-1.287525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.041787</td>\n",
       "      <td>0.335437</td>\n",
       "      <td>0.870285</td>\n",
       "      <td>0.035812</td>\n",
       "      <td>2.351988</td>\n",
       "      <td>1.449463</td>\n",
       "      <td>1.329995</td>\n",
       "      <td>-0.231803</td>\n",
       "      <td>-0.092737</td>\n",
       "      <td>1.660218</td>\n",
       "      <td>...</td>\n",
       "      <td>2.064740</td>\n",
       "      <td>4.032076</td>\n",
       "      <td>4.306531</td>\n",
       "      <td>-0.274399</td>\n",
       "      <td>2.297479</td>\n",
       "      <td>0.979600</td>\n",
       "      <td>2.899305</td>\n",
       "      <td>4.057170</td>\n",
       "      <td>-0.993174</td>\n",
       "      <td>-1.287525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
       "0         -0.041787         -0.178081          0.698012        -0.293179   \n",
       "1         -0.041787         -0.558465          0.229878        -0.387740   \n",
       "2         -0.041787         -0.159062          0.023900        -0.337884   \n",
       "3         -0.041787         -0.368273         -0.313157        -0.392250   \n",
       "4         -0.041787          0.335437          0.870285         0.035812   \n",
       "\n",
       "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
       "0      -0.209981       1.268567       0.438169      1.704667   \n",
       "1       0.302413       0.092740       0.723553     -0.328350   \n",
       "2       1.071004       1.901705       1.329995      0.640939   \n",
       "3      -0.466177      -0.449949      -0.251511     -0.349461   \n",
       "4       2.351988       1.449463       1.329995     -0.231803   \n",
       "\n",
       "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('paramId_0', 'amax')  \\\n",
       "0          -0.752168          -0.975314  ...               1.843341   \n",
       "1          -0.435469           0.452479  ...               1.577662   \n",
       "2          -1.598148          -0.197448  ...               1.267704   \n",
       "3          -0.713123           0.142868  ...              -0.769165   \n",
       "4          -0.092737           1.660218  ...               2.064740   \n",
       "\n",
       "   ('paramId_0', 'mean')  ('paramId_0', 'var')  ('pres', 'amin')  \\\n",
       "0               0.125417              0.530528         -1.157448   \n",
       "1               0.868005              1.203807          2.229245   \n",
       "2               0.523616              0.525603          2.680805   \n",
       "3              -0.509550             -0.490321         -0.067434   \n",
       "4               4.032076              4.306531         -0.274399   \n",
       "\n",
       "   ('pres', 'amax')  ('pres', 'mean')  ('pres', 'var')  ('pres', 'count')  \\\n",
       "0         -2.320225         -2.031007        -0.155444          -0.078436   \n",
       "1          0.835298          1.575456        -1.080198           0.840588   \n",
       "2          2.273147          2.732837        -0.480318           0.840588   \n",
       "3          0.055542          0.043010        -0.155444          -0.537948   \n",
       "4          2.297479          0.979600         2.899305           4.057170   \n",
       "\n",
       "   ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
       "0                     -0.993174              -1.577616  \n",
       "1                     -0.993174              -1.577616  \n",
       "2                     -0.993174              -1.577616  \n",
       "3                     -0.993174              -1.287525  \n",
       "4                     -0.993174              -1.287525  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tenday_imp_train.head())\n",
    "scaler.fit(tenday_imp_train)\n",
    "tenday_full_features_train = pd.DataFrame(scaler.transform(tenday_imp_train))\n",
    "tenday_full_features_train.columns = tenday_imp_train.columns\n",
    "tenday_full_features_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>('cwat', 'amin')</th>\n",
       "      <th>('cwat', 'amax')</th>\n",
       "      <th>('cwat', 'mean')</th>\n",
       "      <th>('cwat', 'var')</th>\n",
       "      <th>('r', 'amin')</th>\n",
       "      <th>('r', 'amax')</th>\n",
       "      <th>('r', 'mean')</th>\n",
       "      <th>('r', 'var')</th>\n",
       "      <th>('tozne', 'amin')</th>\n",
       "      <th>('tozne', 'amax')</th>\n",
       "      <th>...</th>\n",
       "      <th>('paramId_0', 'amax')</th>\n",
       "      <th>('paramId_0', 'mean')</th>\n",
       "      <th>('paramId_0', 'var')</th>\n",
       "      <th>('pres', 'amin')</th>\n",
       "      <th>('pres', 'amax')</th>\n",
       "      <th>('pres', 'mean')</th>\n",
       "      <th>('pres', 'var')</th>\n",
       "      <th>('pres', 'count')</th>\n",
       "      <th>('macro_season', '&lt;lambda&gt;')</th>\n",
       "      <th>('month', '&lt;lambda&gt;')</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.041787</td>\n",
       "      <td>-0.710618</td>\n",
       "      <td>-0.414273</td>\n",
       "      <td>-0.447935</td>\n",
       "      <td>-0.978571</td>\n",
       "      <td>-0.811742</td>\n",
       "      <td>-1.226575</td>\n",
       "      <td>-0.340970</td>\n",
       "      <td>-1.476674</td>\n",
       "      <td>-1.351453</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.769165</td>\n",
       "      <td>-0.509550</td>\n",
       "      <td>-0.490321</td>\n",
       "      <td>-0.117607</td>\n",
       "      <td>-0.021880</td>\n",
       "      <td>-0.041959</td>\n",
       "      <td>-0.125574</td>\n",
       "      <td>-0.537948</td>\n",
       "      <td>-0.993174</td>\n",
       "      <td>-1.577616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.041787</td>\n",
       "      <td>1.552666</td>\n",
       "      <td>1.042558</td>\n",
       "      <td>0.436756</td>\n",
       "      <td>-0.209981</td>\n",
       "      <td>1.720808</td>\n",
       "      <td>1.012901</td>\n",
       "      <td>2.867610</td>\n",
       "      <td>-1.624179</td>\n",
       "      <td>-1.305395</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.769165</td>\n",
       "      <td>-0.509550</td>\n",
       "      <td>-0.490321</td>\n",
       "      <td>-0.117607</td>\n",
       "      <td>-0.021880</td>\n",
       "      <td>-0.041959</td>\n",
       "      <td>-0.125574</td>\n",
       "      <td>-0.537948</td>\n",
       "      <td>-0.993174</td>\n",
       "      <td>-1.577616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.041787</td>\n",
       "      <td>-0.082985</td>\n",
       "      <td>-0.129648</td>\n",
       "      <td>-0.248986</td>\n",
       "      <td>-0.466177</td>\n",
       "      <td>0.183188</td>\n",
       "      <td>-0.711297</td>\n",
       "      <td>0.392016</td>\n",
       "      <td>-1.034162</td>\n",
       "      <td>0.733944</td>\n",
       "      <td>...</td>\n",
       "      <td>0.249269</td>\n",
       "      <td>-0.251259</td>\n",
       "      <td>-0.335237</td>\n",
       "      <td>4.177221</td>\n",
       "      <td>2.383750</td>\n",
       "      <td>3.444345</td>\n",
       "      <td>-0.125574</td>\n",
       "      <td>-0.078436</td>\n",
       "      <td>-0.993174</td>\n",
       "      <td>-1.287525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.041787</td>\n",
       "      <td>-0.292196</td>\n",
       "      <td>0.683031</td>\n",
       "      <td>-0.344914</td>\n",
       "      <td>1.583398</td>\n",
       "      <td>0.816326</td>\n",
       "      <td>1.012901</td>\n",
       "      <td>-0.467680</td>\n",
       "      <td>-0.114429</td>\n",
       "      <td>1.419694</td>\n",
       "      <td>...</td>\n",
       "      <td>2.020460</td>\n",
       "      <td>3.859881</td>\n",
       "      <td>4.084313</td>\n",
       "      <td>-0.269381</td>\n",
       "      <td>2.479975</td>\n",
       "      <td>1.261471</td>\n",
       "      <td>3.423547</td>\n",
       "      <td>3.597659</td>\n",
       "      <td>-0.993174</td>\n",
       "      <td>-1.287525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.041787</td>\n",
       "      <td>1.875992</td>\n",
       "      <td>2.581780</td>\n",
       "      <td>3.498951</td>\n",
       "      <td>1.839594</td>\n",
       "      <td>1.720808</td>\n",
       "      <td>1.769963</td>\n",
       "      <td>1.511012</td>\n",
       "      <td>-0.656724</td>\n",
       "      <td>0.989821</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.282088</td>\n",
       "      <td>-0.315831</td>\n",
       "      <td>-0.441624</td>\n",
       "      <td>0.155837</td>\n",
       "      <td>1.560859</td>\n",
       "      <td>0.901712</td>\n",
       "      <td>4.633716</td>\n",
       "      <td>0.381076</td>\n",
       "      <td>-0.993174</td>\n",
       "      <td>-1.287525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
       "0         -0.041787         -0.710618         -0.414273        -0.447935   \n",
       "1         -0.041787          1.552666          1.042558         0.436756   \n",
       "2         -0.041787         -0.082985         -0.129648        -0.248986   \n",
       "3         -0.041787         -0.292196          0.683031        -0.344914   \n",
       "4         -0.041787          1.875992          2.581780         3.498951   \n",
       "\n",
       "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
       "0      -0.978571      -0.811742      -1.226575     -0.340970   \n",
       "1      -0.209981       1.720808       1.012901      2.867610   \n",
       "2      -0.466177       0.183188      -0.711297      0.392016   \n",
       "3       1.583398       0.816326       1.012901     -0.467680   \n",
       "4       1.839594       1.720808       1.769963      1.511012   \n",
       "\n",
       "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('paramId_0', 'amax')  \\\n",
       "0          -1.476674          -1.351453  ...              -0.769165   \n",
       "1          -1.624179          -1.305395  ...              -0.769165   \n",
       "2          -1.034162           0.733944  ...               0.249269   \n",
       "3          -0.114429           1.419694  ...               2.020460   \n",
       "4          -0.656724           0.989821  ...              -0.282088   \n",
       "\n",
       "   ('paramId_0', 'mean')  ('paramId_0', 'var')  ('pres', 'amin')  \\\n",
       "0              -0.509550             -0.490321         -0.117607   \n",
       "1              -0.509550             -0.490321         -0.117607   \n",
       "2              -0.251259             -0.335237          4.177221   \n",
       "3               3.859881              4.084313         -0.269381   \n",
       "4              -0.315831             -0.441624          0.155837   \n",
       "\n",
       "   ('pres', 'amax')  ('pres', 'mean')  ('pres', 'var')  ('pres', 'count')  \\\n",
       "0         -0.021880         -0.041959        -0.125574          -0.537948   \n",
       "1         -0.021880         -0.041959        -0.125574          -0.537948   \n",
       "2          2.383750          3.444345        -0.125574          -0.078436   \n",
       "3          2.479975          1.261471         3.423547           3.597659   \n",
       "4          1.560859          0.901712         4.633716           0.381076   \n",
       "\n",
       "   ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
       "0                     -0.993174              -1.577616  \n",
       "1                     -0.993174              -1.577616  \n",
       "2                     -0.993174              -1.287525  \n",
       "3                     -0.993174              -1.287525  \n",
       "4                     -0.993174              -1.287525  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tenday_full_features_test = pd.DataFrame(scaler.transform(tenday_imp_test))\n",
    "tenday_full_features_test.columns = tenday_imp_test.columns\n",
    "tenday_full_features_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Features With No Nulls Training and Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
      "0               0.0              0.42           0.10325         0.009705   \n",
      "1               0.0              0.22           0.07200         0.005263   \n",
      "2               0.0              0.43           0.05825         0.007605   \n",
      "3               0.0              0.32           0.03575         0.005051   \n",
      "4               0.0              0.69           0.11475         0.025159   \n",
      "\n",
      "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
      "0            8.0           48.0         21.625    124.189103   \n",
      "1           10.0           35.0         23.425     35.789103   \n",
      "2           13.0           55.0         27.250     77.935897   \n",
      "3            7.0           29.0         17.275     34.871154   \n",
      "4           18.0           50.0         27.250     39.987179   \n",
      "\n",
      "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('pwat', 'amax')  \\\n",
      "0         263.500000         304.299988  ...         28.500000   \n",
      "1         270.799988         360.100006  ...         18.299999   \n",
      "2         244.000000         334.700012  ...         22.600000   \n",
      "3         264.399994         348.000000  ...         20.100000   \n",
      "4         278.700012         407.299988  ...         28.000000   \n",
      "\n",
      "   ('pwat', 'mean')  ('pwat', 'var')  ('paramId_0', 'amin')  \\\n",
      "0           13.6700        37.680100                    0.0   \n",
      "1           13.1850         7.657718                    0.0   \n",
      "2           14.3550        16.905103                    0.0   \n",
      "3           11.1450        14.892795                    0.0   \n",
      "4           15.1125        15.711378                    0.0   \n",
      "\n",
      "   ('paramId_0', 'amax')  ('paramId_0', 'mean')  ('paramId_0', 'var')  \\\n",
      "0                   59.0                  1.475             87.025000   \n",
      "1                   53.0                  3.200            144.420513   \n",
      "2                   46.0                  2.400             86.605128   \n",
      "3                    0.0                  0.000              0.000000   \n",
      "4                   64.0                 10.550            408.920513   \n",
      "\n",
      "   ('pres', 'count')  ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
      "0                  1                             0                      1  \n",
      "1                  3                             0                      1  \n",
      "2                  3                             0                      1  \n",
      "3                  0                             0                      2  \n",
      "4                 10                             0                      2  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>('cwat', 'amin')</th>\n",
       "      <th>('cwat', 'amax')</th>\n",
       "      <th>('cwat', 'mean')</th>\n",
       "      <th>('cwat', 'var')</th>\n",
       "      <th>('r', 'amin')</th>\n",
       "      <th>('r', 'amax')</th>\n",
       "      <th>('r', 'mean')</th>\n",
       "      <th>('r', 'var')</th>\n",
       "      <th>('tozne', 'amin')</th>\n",
       "      <th>('tozne', 'amax')</th>\n",
       "      <th>...</th>\n",
       "      <th>('pwat', 'amax')</th>\n",
       "      <th>('pwat', 'mean')</th>\n",
       "      <th>('pwat', 'var')</th>\n",
       "      <th>('paramId_0', 'amin')</th>\n",
       "      <th>('paramId_0', 'amax')</th>\n",
       "      <th>('paramId_0', 'mean')</th>\n",
       "      <th>('paramId_0', 'var')</th>\n",
       "      <th>('pres', 'count')</th>\n",
       "      <th>('macro_season', '&lt;lambda&gt;')</th>\n",
       "      <th>('month', '&lt;lambda&gt;')</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.041787</td>\n",
       "      <td>-0.178081</td>\n",
       "      <td>0.698012</td>\n",
       "      <td>-0.293179</td>\n",
       "      <td>-0.209981</td>\n",
       "      <td>1.268567</td>\n",
       "      <td>0.438169</td>\n",
       "      <td>1.704667</td>\n",
       "      <td>-0.752168</td>\n",
       "      <td>-0.975314</td>\n",
       "      <td>...</td>\n",
       "      <td>0.811691</td>\n",
       "      <td>0.245159</td>\n",
       "      <td>1.054886</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.843341</td>\n",
       "      <td>0.125417</td>\n",
       "      <td>0.530528</td>\n",
       "      <td>-0.078436</td>\n",
       "      <td>-0.993174</td>\n",
       "      <td>-1.577616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.041787</td>\n",
       "      <td>-0.558465</td>\n",
       "      <td>0.229878</td>\n",
       "      <td>-0.387740</td>\n",
       "      <td>0.302413</td>\n",
       "      <td>0.092740</td>\n",
       "      <td>0.723553</td>\n",
       "      <td>-0.328350</td>\n",
       "      <td>-0.435469</td>\n",
       "      <td>0.452479</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.379780</td>\n",
       "      <td>0.154107</td>\n",
       "      <td>-0.619355</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.577662</td>\n",
       "      <td>0.868005</td>\n",
       "      <td>1.203807</td>\n",
       "      <td>0.840588</td>\n",
       "      <td>-0.993174</td>\n",
       "      <td>-1.577616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.041787</td>\n",
       "      <td>-0.159062</td>\n",
       "      <td>0.023900</td>\n",
       "      <td>-0.337884</td>\n",
       "      <td>1.071004</td>\n",
       "      <td>1.901705</td>\n",
       "      <td>1.329995</td>\n",
       "      <td>0.640939</td>\n",
       "      <td>-1.598148</td>\n",
       "      <td>-0.197448</td>\n",
       "      <td>...</td>\n",
       "      <td>0.122507</td>\n",
       "      <td>0.373759</td>\n",
       "      <td>-0.103661</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.267704</td>\n",
       "      <td>0.523616</td>\n",
       "      <td>0.525603</td>\n",
       "      <td>0.840588</td>\n",
       "      <td>-0.993174</td>\n",
       "      <td>-1.577616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.041787</td>\n",
       "      <td>-0.368273</td>\n",
       "      <td>-0.313157</td>\n",
       "      <td>-0.392250</td>\n",
       "      <td>-0.466177</td>\n",
       "      <td>-0.449949</td>\n",
       "      <td>-0.251511</td>\n",
       "      <td>-0.349461</td>\n",
       "      <td>-0.713123</td>\n",
       "      <td>0.142868</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.169520</td>\n",
       "      <td>-0.228875</td>\n",
       "      <td>-0.215881</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.769165</td>\n",
       "      <td>-0.509550</td>\n",
       "      <td>-0.490321</td>\n",
       "      <td>-0.537948</td>\n",
       "      <td>-0.993174</td>\n",
       "      <td>-1.287525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.041787</td>\n",
       "      <td>0.335437</td>\n",
       "      <td>0.870285</td>\n",
       "      <td>0.035812</td>\n",
       "      <td>2.351988</td>\n",
       "      <td>1.449463</td>\n",
       "      <td>1.329995</td>\n",
       "      <td>-0.231803</td>\n",
       "      <td>-0.092737</td>\n",
       "      <td>1.660218</td>\n",
       "      <td>...</td>\n",
       "      <td>0.753286</td>\n",
       "      <td>0.515969</td>\n",
       "      <td>-0.170231</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.064740</td>\n",
       "      <td>4.032076</td>\n",
       "      <td>4.306531</td>\n",
       "      <td>4.057170</td>\n",
       "      <td>-0.993174</td>\n",
       "      <td>-1.287525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
       "0         -0.041787         -0.178081          0.698012        -0.293179   \n",
       "1         -0.041787         -0.558465          0.229878        -0.387740   \n",
       "2         -0.041787         -0.159062          0.023900        -0.337884   \n",
       "3         -0.041787         -0.368273         -0.313157        -0.392250   \n",
       "4         -0.041787          0.335437          0.870285         0.035812   \n",
       "\n",
       "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
       "0      -0.209981       1.268567       0.438169      1.704667   \n",
       "1       0.302413       0.092740       0.723553     -0.328350   \n",
       "2       1.071004       1.901705       1.329995      0.640939   \n",
       "3      -0.466177      -0.449949      -0.251511     -0.349461   \n",
       "4       2.351988       1.449463       1.329995     -0.231803   \n",
       "\n",
       "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('pwat', 'amax')  \\\n",
       "0          -0.752168          -0.975314  ...          0.811691   \n",
       "1          -0.435469           0.452479  ...         -0.379780   \n",
       "2          -1.598148          -0.197448  ...          0.122507   \n",
       "3          -0.713123           0.142868  ...         -0.169520   \n",
       "4          -0.092737           1.660218  ...          0.753286   \n",
       "\n",
       "   ('pwat', 'mean')  ('pwat', 'var')  ('paramId_0', 'amin')  \\\n",
       "0          0.245159         1.054886                    0.0   \n",
       "1          0.154107        -0.619355                    0.0   \n",
       "2          0.373759        -0.103661                    0.0   \n",
       "3         -0.228875        -0.215881                    0.0   \n",
       "4          0.515969        -0.170231                    0.0   \n",
       "\n",
       "   ('paramId_0', 'amax')  ('paramId_0', 'mean')  ('paramId_0', 'var')  \\\n",
       "0               1.843341               0.125417              0.530528   \n",
       "1               1.577662               0.868005              1.203807   \n",
       "2               1.267704               0.523616              0.525603   \n",
       "3              -0.769165              -0.509550             -0.490321   \n",
       "4               2.064740               4.032076              4.306531   \n",
       "\n",
       "   ('pres', 'count')  ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
       "0          -0.078436                     -0.993174              -1.577616  \n",
       "1           0.840588                     -0.993174              -1.577616  \n",
       "2           0.840588                     -0.993174              -1.577616  \n",
       "3          -0.537948                     -0.993174              -1.287525  \n",
       "4           4.057170                     -0.993174              -1.287525  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tenday_no_null_train.head())\n",
    "scaler.fit(tenday_no_null_train)\n",
    "tenday_part_features_train = pd.DataFrame(scaler.transform(tenday_no_null_train))\n",
    "tenday_part_features_train.columns = tenday_no_null_train.columns\n",
    "tenday_part_features_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>('cwat', 'amin')</th>\n",
       "      <th>('cwat', 'amax')</th>\n",
       "      <th>('cwat', 'mean')</th>\n",
       "      <th>('cwat', 'var')</th>\n",
       "      <th>('r', 'amin')</th>\n",
       "      <th>('r', 'amax')</th>\n",
       "      <th>('r', 'mean')</th>\n",
       "      <th>('r', 'var')</th>\n",
       "      <th>('tozne', 'amin')</th>\n",
       "      <th>('tozne', 'amax')</th>\n",
       "      <th>...</th>\n",
       "      <th>('pwat', 'amax')</th>\n",
       "      <th>('pwat', 'mean')</th>\n",
       "      <th>('pwat', 'var')</th>\n",
       "      <th>('paramId_0', 'amin')</th>\n",
       "      <th>('paramId_0', 'amax')</th>\n",
       "      <th>('paramId_0', 'mean')</th>\n",
       "      <th>('paramId_0', 'var')</th>\n",
       "      <th>('pres', 'count')</th>\n",
       "      <th>('macro_season', '&lt;lambda&gt;')</th>\n",
       "      <th>('month', '&lt;lambda&gt;')</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.041787</td>\n",
       "      <td>-0.710618</td>\n",
       "      <td>-0.414273</td>\n",
       "      <td>-0.447935</td>\n",
       "      <td>-0.978571</td>\n",
       "      <td>-0.811742</td>\n",
       "      <td>-1.226575</td>\n",
       "      <td>-0.340970</td>\n",
       "      <td>-1.476674</td>\n",
       "      <td>-1.351453</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.426504</td>\n",
       "      <td>-0.808042</td>\n",
       "      <td>-0.266732</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.769165</td>\n",
       "      <td>-0.509550</td>\n",
       "      <td>-0.490321</td>\n",
       "      <td>-0.537948</td>\n",
       "      <td>-0.993174</td>\n",
       "      <td>-1.577616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.041787</td>\n",
       "      <td>1.552666</td>\n",
       "      <td>1.042558</td>\n",
       "      <td>0.436756</td>\n",
       "      <td>-0.209981</td>\n",
       "      <td>1.720808</td>\n",
       "      <td>1.012901</td>\n",
       "      <td>2.867610</td>\n",
       "      <td>-1.624179</td>\n",
       "      <td>-1.305395</td>\n",
       "      <td>...</td>\n",
       "      <td>1.115400</td>\n",
       "      <td>0.537089</td>\n",
       "      <td>2.006364</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.769165</td>\n",
       "      <td>-0.509550</td>\n",
       "      <td>-0.490321</td>\n",
       "      <td>-0.537948</td>\n",
       "      <td>-0.993174</td>\n",
       "      <td>-1.577616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.041787</td>\n",
       "      <td>-0.082985</td>\n",
       "      <td>-0.129648</td>\n",
       "      <td>-0.248986</td>\n",
       "      <td>-0.466177</td>\n",
       "      <td>0.183188</td>\n",
       "      <td>-0.711297</td>\n",
       "      <td>0.392016</td>\n",
       "      <td>-1.034162</td>\n",
       "      <td>0.733944</td>\n",
       "      <td>...</td>\n",
       "      <td>0.367810</td>\n",
       "      <td>-0.402062</td>\n",
       "      <td>0.410892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.249269</td>\n",
       "      <td>-0.251259</td>\n",
       "      <td>-0.335237</td>\n",
       "      <td>-0.078436</td>\n",
       "      <td>-0.993174</td>\n",
       "      <td>-1.287525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.041787</td>\n",
       "      <td>-0.292196</td>\n",
       "      <td>0.683031</td>\n",
       "      <td>-0.344914</td>\n",
       "      <td>1.583398</td>\n",
       "      <td>0.816326</td>\n",
       "      <td>1.012901</td>\n",
       "      <td>-0.467680</td>\n",
       "      <td>-0.114429</td>\n",
       "      <td>1.419694</td>\n",
       "      <td>...</td>\n",
       "      <td>0.145869</td>\n",
       "      <td>0.410367</td>\n",
       "      <td>-0.618427</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.020460</td>\n",
       "      <td>3.859881</td>\n",
       "      <td>4.084313</td>\n",
       "      <td>3.597659</td>\n",
       "      <td>-0.993174</td>\n",
       "      <td>-1.287525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.041787</td>\n",
       "      <td>1.875992</td>\n",
       "      <td>2.581780</td>\n",
       "      <td>3.498951</td>\n",
       "      <td>1.839594</td>\n",
       "      <td>1.720808</td>\n",
       "      <td>1.769963</td>\n",
       "      <td>1.511012</td>\n",
       "      <td>-0.656724</td>\n",
       "      <td>0.989821</td>\n",
       "      <td>...</td>\n",
       "      <td>1.617687</td>\n",
       "      <td>1.175862</td>\n",
       "      <td>2.157998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.282088</td>\n",
       "      <td>-0.315831</td>\n",
       "      <td>-0.441624</td>\n",
       "      <td>0.381076</td>\n",
       "      <td>-0.993174</td>\n",
       "      <td>-1.287525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
       "0         -0.041787         -0.710618         -0.414273        -0.447935   \n",
       "1         -0.041787          1.552666          1.042558         0.436756   \n",
       "2         -0.041787         -0.082985         -0.129648        -0.248986   \n",
       "3         -0.041787         -0.292196          0.683031        -0.344914   \n",
       "4         -0.041787          1.875992          2.581780         3.498951   \n",
       "\n",
       "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
       "0      -0.978571      -0.811742      -1.226575     -0.340970   \n",
       "1      -0.209981       1.720808       1.012901      2.867610   \n",
       "2      -0.466177       0.183188      -0.711297      0.392016   \n",
       "3       1.583398       0.816326       1.012901     -0.467680   \n",
       "4       1.839594       1.720808       1.769963      1.511012   \n",
       "\n",
       "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('pwat', 'amax')  \\\n",
       "0          -1.476674          -1.351453  ...         -0.426504   \n",
       "1          -1.624179          -1.305395  ...          1.115400   \n",
       "2          -1.034162           0.733944  ...          0.367810   \n",
       "3          -0.114429           1.419694  ...          0.145869   \n",
       "4          -0.656724           0.989821  ...          1.617687   \n",
       "\n",
       "   ('pwat', 'mean')  ('pwat', 'var')  ('paramId_0', 'amin')  \\\n",
       "0         -0.808042        -0.266732                    0.0   \n",
       "1          0.537089         2.006364                    0.0   \n",
       "2         -0.402062         0.410892                    0.0   \n",
       "3          0.410367        -0.618427                    0.0   \n",
       "4          1.175862         2.157998                    0.0   \n",
       "\n",
       "   ('paramId_0', 'amax')  ('paramId_0', 'mean')  ('paramId_0', 'var')  \\\n",
       "0              -0.769165              -0.509550             -0.490321   \n",
       "1              -0.769165              -0.509550             -0.490321   \n",
       "2               0.249269              -0.251259             -0.335237   \n",
       "3               2.020460               3.859881              4.084313   \n",
       "4              -0.282088              -0.315831             -0.441624   \n",
       "\n",
       "   ('pres', 'count')  ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
       "0          -0.537948                     -0.993174              -1.577616  \n",
       "1          -0.537948                     -0.993174              -1.577616  \n",
       "2          -0.078436                     -0.993174              -1.287525  \n",
       "3           3.597659                     -0.993174              -1.287525  \n",
       "4           0.381076                     -0.993174              -1.287525  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tenday_part_features_test = pd.DataFrame(scaler.transform(tenday_no_null_test))\n",
    "tenday_part_features_test.columns = tenday_no_null_test.columns\n",
    "tenday_part_features_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-Day Average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.6192\n",
      "Test set accuracy: 0.6409\n",
      "Training Set Precision: 0.1457\n",
      "Training Set Recall: 0.7365\n",
      "Training Set F1 Score: 0.2433\n",
      "Test Set Precision: 0.1251\n",
      "Test Set Recall: 0.7054\n",
      "Test Set F1 Score: 0.2125\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "clf_log_3day_full = LogisticRegression(class_weight = \"balanced\", max_iter = 10000)\n",
    "clf_log_3day_full.fit(threeday_full_features_train, threeday_target_train)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_log_3day_full.score(threeday_full_features_train, \n",
    "                                                       threeday_target_train), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_log_3day_full.score(threeday_full_features_test,\n",
    "                                                        threeday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_log = clf_log_3day_full.predict(threeday_full_features_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(threeday_target_train, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(threeday_target_train, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(threeday_target_train, \n",
    "                                                    target_pred_train_log), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_log = clf_log_3day_full.predict(threeday_full_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(threeday_target_test, \n",
    "                                                      target_pred_test_log), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(threeday_target_test, \n",
    "                                                target_pred_test_log), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(threeday_target_test, \n",
    "                                                    target_pred_test_log), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.6217\n",
      "Test set accuracy: 0.6482\n",
      "Training Set Precision: 0.1458\n",
      "Training Set Recall: 0.7309\n",
      "Training Set F1 Score: 0.2431\n",
      "Test Set Precision: 0.1255\n",
      "Test Set Recall: 0.6907\n",
      "Test Set F1 Score: 0.2125\n"
     ]
    }
   ],
   "source": [
    "clf_log_3day_part = LogisticRegression(class_weight = \"balanced\", max_iter = 10000)\n",
    "clf_log_3day_part.fit(threeday_part_features_train, threeday_target_train)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_log_3day_part.score(threeday_part_features_train, \n",
    "                                                       threeday_target_train), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_log_3day_part.score(threeday_part_features_test,\n",
    "                                                        threeday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_log = clf_log_3day_part.predict(threeday_part_features_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(threeday_target_train, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(threeday_target_train, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(threeday_target_train, \n",
    "                                                    target_pred_train_log), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_log = clf_log_3day_part.predict(threeday_part_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(threeday_target_test, \n",
    "                                                      target_pred_test_log), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(threeday_target_test, \n",
    "                                                target_pred_test_log), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(threeday_target_test, \n",
    "                                                    target_pred_test_log), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.6286\n",
      "Test set accuracy: 0.6432\n",
      "Training Set Precision: 0.1474\n",
      "Training Set Recall: 0.7254\n",
      "Training Set F1 Score: 0.245\n",
      "Test Set Precision: 0.1245\n",
      "Test Set Recall: 0.6953\n",
      "Test Set F1 Score: 0.2112\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "clf_sgd_3day_full = SGDClassifier(loss = \"log\", class_weight = \"balanced\", random_state = 0)\n",
    "clf_sgd_3day_full.fit(threeday_full_features_train, threeday_target_train)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_sgd_3day_full.score(threeday_full_features_train, \n",
    "                                                       threeday_target_train), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_sgd_3day_full.score(threeday_full_features_test,\n",
    "                                                        threeday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_sgd = clf_sgd_3day_full.predict(threeday_full_features_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(threeday_target_train, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(threeday_target_train, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(threeday_target_train, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_sgd = clf_sgd_3day_full.predict(threeday_full_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(threeday_target_test, \n",
    "                                                      target_pred_test_sgd), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(threeday_target_test, \n",
    "                                                target_pred_test_sgd), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(threeday_target_test, \n",
    "                                                    target_pred_test_sgd), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.6324\n",
      "Test set accuracy: 0.6544\n",
      "Training Set Precision: 0.1475\n",
      "Training Set Recall: 0.7165\n",
      "Training Set F1 Score: 0.2447\n",
      "Test Set Precision: 0.1261\n",
      "Test Set Recall: 0.6794\n",
      "Test Set F1 Score: 0.2127\n"
     ]
    }
   ],
   "source": [
    "clf_sgd_3day_part = SGDClassifier(loss = \"log\", class_weight = \"balanced\", random_state = 0)\n",
    "clf_sgd_3day_part.fit(threeday_part_features_train, threeday_target_train)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_sgd_3day_part.score(threeday_part_features_train, \n",
    "                                                       threeday_target_train), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_sgd_3day_part.score(threeday_part_features_test,\n",
    "                                                        threeday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_sgd = clf_sgd_3day_part.predict(threeday_part_features_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(threeday_target_train, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(threeday_target_train, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(threeday_target_train, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_sgd = clf_sgd_3day_part.predict(threeday_part_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(threeday_target_test, \n",
    "                                                      target_pred_test_sgd), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(threeday_target_test, \n",
    "                                                target_pred_test_sgd), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(threeday_target_test, \n",
    "                                                    target_pred_test_sgd), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-Day Average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.6329\n",
      "Test set accuracy: 0.6555\n",
      "Training Set Precision: 0.1996\n",
      "Training Set Recall: 0.7281\n",
      "Training Set F1 Score: 0.3134\n",
      "Test Set Precision: 0.1752\n",
      "Test Set Recall: 0.6959\n",
      "Test Set F1 Score: 0.2799\n"
     ]
    }
   ],
   "source": [
    "clf_log_5day_full = LogisticRegression(class_weight = \"balanced\", max_iter = 10000)\n",
    "clf_log_5day_full.fit(fiveday_full_features_train, fiveday_target_train)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_log_5day_full.score(fiveday_full_features_train, \n",
    "                                                       fiveday_target_train), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_log_5day_full.score(fiveday_full_features_test,\n",
    "                                                        fiveday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_log = clf_log_5day_full.predict(fiveday_full_features_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(fiveday_target_train, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(fiveday_target_train, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(fiveday_target_train, \n",
    "                                                    target_pred_train_log), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_log = clf_log_5day_full.predict(fiveday_full_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(fiveday_target_test, \n",
    "                                                      target_pred_test_log), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(fiveday_target_test, \n",
    "                                                target_pred_test_log), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(fiveday_target_test, \n",
    "                                                    target_pred_test_log), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.6342\n",
      "Test set accuracy: 0.6606\n",
      "Training Set Precision: 0.1995\n",
      "Training Set Recall: 0.7232\n",
      "Training Set F1 Score: 0.3127\n",
      "Test Set Precision: 0.1764\n",
      "Test Set Recall: 0.689\n",
      "Test Set F1 Score: 0.2809\n"
     ]
    }
   ],
   "source": [
    "clf_log_5day_part = LogisticRegression(class_weight = \"balanced\", max_iter = 10000)\n",
    "clf_log_5day_part.fit(fiveday_part_features_train, fiveday_target_train)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_log_5day_part.score(fiveday_part_features_train, \n",
    "                                                       fiveday_target_train), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_log_5day_part.score(fiveday_part_features_test,\n",
    "                                                        fiveday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_log = clf_log_5day_part.predict(fiveday_part_features_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(fiveday_target_train, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(fiveday_target_train, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(fiveday_target_train, \n",
    "                                                    target_pred_train_log), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_log = clf_log_5day_part.predict(fiveday_part_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(fiveday_target_test, \n",
    "                                                      target_pred_test_log), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(fiveday_target_test, \n",
    "                                                target_pred_test_log), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(fiveday_target_test, \n",
    "                                                    target_pred_test_log), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.6175\n",
      "Test set accuracy: 0.635\n",
      "Training Set Precision: 0.1944\n",
      "Training Set Recall: 0.7392\n",
      "Training Set F1 Score: 0.3078\n",
      "Test Set Precision: 0.1685\n",
      "Test Set Recall: 0.7099\n",
      "Test Set F1 Score: 0.2723\n"
     ]
    }
   ],
   "source": [
    "clf_sgd_5day_full = SGDClassifier(loss = \"log\", class_weight = \"balanced\", random_state = 0)\n",
    "clf_sgd_5day_full.fit(fiveday_full_features_train, fiveday_target_train)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_sgd_5day_full.score(fiveday_full_features_train, \n",
    "                                                       fiveday_target_train), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_sgd_5day_full.score(fiveday_full_features_test,\n",
    "                                                        fiveday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_sgd = clf_sgd_5day_full.predict(fiveday_full_features_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(fiveday_target_train, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(fiveday_target_train, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(fiveday_target_train, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_sgd = clf_sgd_5day_full.predict(fiveday_full_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(fiveday_target_test, \n",
    "                                                      target_pred_test_sgd), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(fiveday_target_test, \n",
    "                                                target_pred_test_sgd), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(fiveday_target_test, \n",
    "                                                    target_pred_test_sgd), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.6184\n",
      "Test set accuracy: 0.6371\n",
      "Training Set Precision: 0.1943\n",
      "Training Set Recall: 0.7361\n",
      "Training Set F1 Score: 0.3074\n",
      "Test Set Precision: 0.1682\n",
      "Test Set Recall: 0.7027\n",
      "Test Set F1 Score: 0.2714\n"
     ]
    }
   ],
   "source": [
    "clf_sgd_5day_part = SGDClassifier(loss = \"log\", class_weight = \"balanced\", random_state = 0)\n",
    "clf_sgd_5day_part.fit(fiveday_part_features_train, fiveday_target_train)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_sgd_5day_part.score(fiveday_part_features_train, \n",
    "                                                       fiveday_target_train), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_sgd_5day_part.score(fiveday_part_features_test,\n",
    "                                                        fiveday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_sgd = clf_sgd_5day_part.predict(fiveday_part_features_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(fiveday_target_train, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(fiveday_target_train, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(fiveday_target_train, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_sgd = clf_sgd_5day_part.predict(fiveday_part_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(fiveday_target_test, \n",
    "                                                      target_pred_test_sgd), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(fiveday_target_test, \n",
    "                                                target_pred_test_sgd), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(fiveday_target_test, \n",
    "                                                    target_pred_test_sgd), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10-Day Average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.6581\n",
      "Test set accuracy: 0.6632\n",
      "Training Set Precision: 0.2914\n",
      "Training Set Recall: 0.7224\n",
      "Training Set F1 Score: 0.4152\n",
      "Test Set Precision: 0.2526\n",
      "Test Set Recall: 0.7062\n",
      "Test Set F1 Score: 0.3721\n"
     ]
    }
   ],
   "source": [
    "clf_log_10day_full = LogisticRegression(class_weight = \"balanced\", max_iter = 10000)\n",
    "clf_log_10day_full.fit(tenday_full_features_train, tenday_target_train)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_log_10day_full.score(tenday_full_features_train, \n",
    "                                                       tenday_target_train), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_log_10day_full.score(tenday_full_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_log = clf_log_10day_full.predict(tenday_full_features_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(tenday_target_train, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(tenday_target_train, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(tenday_target_train, \n",
    "                                                    target_pred_train_log), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_log = clf_log_10day_full.predict(tenday_full_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_log), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_log), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_log), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.6589\n",
      "Test set accuracy: 0.6651\n",
      "Training Set Precision: 0.2916\n",
      "Training Set Recall: 0.7204\n",
      "Training Set F1 Score: 0.4152\n",
      "Test Set Precision: 0.2528\n",
      "Test Set Recall: 0.7002\n",
      "Test Set F1 Score: 0.3714\n"
     ]
    }
   ],
   "source": [
    "clf_log_10day_part = LogisticRegression(class_weight = \"balanced\", max_iter = 10000)\n",
    "clf_log_10day_part.fit(tenday_part_features_train, tenday_target_train)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_log_10day_part.score(tenday_part_features_train, \n",
    "                                                       tenday_target_train), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_log_10day_part.score(tenday_part_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_log = clf_log_10day_part.predict(tenday_part_features_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(tenday_target_train, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(tenday_target_train, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(tenday_target_train, \n",
    "                                                    target_pred_train_log), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_log = clf_log_10day_part.predict(tenday_part_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_log), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_log), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_log), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.6352\n",
      "Test set accuracy: 0.6307\n",
      "Training Set Precision: 0.2802\n",
      "Training Set Recall: 0.7463\n",
      "Training Set F1 Score: 0.4075\n",
      "Test Set Precision: 0.238\n",
      "Test Set Recall: 0.7329\n",
      "Test Set F1 Score: 0.3593\n"
     ]
    }
   ],
   "source": [
    "clf_sgd_10day_full = SGDClassifier(loss = \"log\", class_weight = \"balanced\",  random_state = 0)\n",
    "clf_sgd_10day_full.fit(tenday_full_features_train, tenday_target_train)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_sgd_10day_full.score(tenday_full_features_train, \n",
    "                                                       tenday_target_train), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_sgd_10day_full.score(tenday_full_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_sgd = clf_sgd_10day_full.predict(tenday_full_features_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(tenday_target_train, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(tenday_target_train, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(tenday_target_train, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_sgd = clf_sgd_10day_full.predict(tenday_full_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_sgd), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_sgd), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_sgd), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.6629\n",
      "Test set accuracy: 0.649\n",
      "Training Set Precision: 0.2919\n",
      "Training Set Recall: 0.7055\n",
      "Training Set F1 Score: 0.4129\n",
      "Test Set Precision: 0.2602\n",
      "Test Set Recall: 0.645\n",
      "Test Set F1 Score: 0.3708\n"
     ]
    }
   ],
   "source": [
    "clf_sgd_10day_part = SGDClassifier(loss = \"log\", class_weight = \"balanced\", random_state = 0)\n",
    "clf_sgd_10day_part.fit(tenday_part_features_train, tenday_target_train)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_sgd_10day_part.score(tenday_part_features_train, \n",
    "                                                       tenday_target_train), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_sgd_10day_part.score(tenday_part_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_sgd = clf_sgd_10day_part.predict(tenday_part_features_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(tenday_target_train, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(tenday_target_train, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(tenday_target_train, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_sgd = clf_sgd_3day_part.predict(tenday_part_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_sgd), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_sgd), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_sgd), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Over Sampling Using Only 10-Day Average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1:1 Ratio Majority To Minority  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Full Parameter List "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    117335\n",
       "0    117335\n",
       "Name: fire, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "over_sampler_1_1 = RandomOverSampler(random_state = 0)\n",
    "feature_full_train_1_1, target_train_1_1 = over_sampler_1_1.fit_resample(tenday_full_train, tenday_target_train)\n",
    "target_train_1_1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 234670 entries, 0 to 234669\n",
      "Data columns (total 31 columns):\n",
      " #   Column                        Non-Null Count   Dtype  \n",
      "---  ------                        --------------   -----  \n",
      " 0   ('cwat', 'amin')              234670 non-null  float64\n",
      " 1   ('cwat', 'amax')              234670 non-null  float64\n",
      " 2   ('cwat', 'mean')              234670 non-null  float64\n",
      " 3   ('cwat', 'var')               234670 non-null  float64\n",
      " 4   ('r', 'amin')                 234670 non-null  float64\n",
      " 5   ('r', 'amax')                 234670 non-null  float64\n",
      " 6   ('r', 'mean')                 234670 non-null  float64\n",
      " 7   ('r', 'var')                  234670 non-null  float64\n",
      " 8   ('tozne', 'amin')             234670 non-null  float64\n",
      " 9   ('tozne', 'amax')             234670 non-null  float64\n",
      " 10  ('tozne', 'mean')             234670 non-null  float64\n",
      " 11  ('tozne', 'var')              234670 non-null  float64\n",
      " 12  ('gh', 'amin')                234670 non-null  float64\n",
      " 13  ('gh', 'amax')                234670 non-null  float64\n",
      " 14  ('gh', 'mean')                234670 non-null  float64\n",
      " 15  ('gh', 'var')                 234670 non-null  float64\n",
      " 16  ('pwat', 'amin')              234670 non-null  float64\n",
      " 17  ('pwat', 'amax')              234670 non-null  float64\n",
      " 18  ('pwat', 'mean')              234670 non-null  float64\n",
      " 19  ('pwat', 'var')               234670 non-null  float64\n",
      " 20  ('paramId_0', 'amin')         234670 non-null  float64\n",
      " 21  ('paramId_0', 'amax')         234670 non-null  float64\n",
      " 22  ('paramId_0', 'mean')         234670 non-null  float64\n",
      " 23  ('paramId_0', 'var')          234670 non-null  float64\n",
      " 24  ('pres', 'amin')              82338 non-null   float64\n",
      " 25  ('pres', 'amax')              82338 non-null   float64\n",
      " 26  ('pres', 'mean')              82338 non-null   float64\n",
      " 27  ('pres', 'var')               49700 non-null   float64\n",
      " 28  ('pres', 'count')             234670 non-null  int64  \n",
      " 29  ('macro_season', '<lambda>')  234670 non-null  int64  \n",
      " 30  ('month', '<lambda>')         234670 non-null  int64  \n",
      "dtypes: float64(28), int64(3)\n",
      "memory usage: 55.5 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 234670 entries, 0 to 234669\n",
      "Data columns (total 31 columns):\n",
      " #   Column                        Non-Null Count   Dtype  \n",
      "---  ------                        --------------   -----  \n",
      " 0   ('cwat', 'amin')              234670 non-null  float64\n",
      " 1   ('cwat', 'amax')              234670 non-null  float64\n",
      " 2   ('cwat', 'mean')              234670 non-null  float64\n",
      " 3   ('cwat', 'var')               234670 non-null  float64\n",
      " 4   ('r', 'amin')                 234670 non-null  float64\n",
      " 5   ('r', 'amax')                 234670 non-null  float64\n",
      " 6   ('r', 'mean')                 234670 non-null  float64\n",
      " 7   ('r', 'var')                  234670 non-null  float64\n",
      " 8   ('tozne', 'amin')             234670 non-null  float64\n",
      " 9   ('tozne', 'amax')             234670 non-null  float64\n",
      " 10  ('tozne', 'mean')             234670 non-null  float64\n",
      " 11  ('tozne', 'var')              234670 non-null  float64\n",
      " 12  ('gh', 'amin')                234670 non-null  float64\n",
      " 13  ('gh', 'amax')                234670 non-null  float64\n",
      " 14  ('gh', 'mean')                234670 non-null  float64\n",
      " 15  ('gh', 'var')                 234670 non-null  float64\n",
      " 16  ('pwat', 'amin')              234670 non-null  float64\n",
      " 17  ('pwat', 'amax')              234670 non-null  float64\n",
      " 18  ('pwat', 'mean')              234670 non-null  float64\n",
      " 19  ('pwat', 'var')               234670 non-null  float64\n",
      " 20  ('paramId_0', 'amin')         234670 non-null  float64\n",
      " 21  ('paramId_0', 'amax')         234670 non-null  float64\n",
      " 22  ('paramId_0', 'mean')         234670 non-null  float64\n",
      " 23  ('paramId_0', 'var')          234670 non-null  float64\n",
      " 24  ('pres', 'amin')              234670 non-null  float64\n",
      " 25  ('pres', 'amax')              234670 non-null  float64\n",
      " 26  ('pres', 'mean')              234670 non-null  float64\n",
      " 27  ('pres', 'var')               234670 non-null  float64\n",
      " 28  ('pres', 'count')             234670 non-null  float64\n",
      " 29  ('macro_season', '<lambda>')  234670 non-null  float64\n",
      " 30  ('month', '<lambda>')         234670 non-null  float64\n",
      "dtypes: float64(31)\n",
      "memory usage: 55.5 MB\n"
     ]
    }
   ],
   "source": [
    "feature_full_train_1_1.info()\n",
    "feature_imp_train_1_1 = pd.DataFrame(imp.fit_transform(feature_full_train_1_1))\n",
    "feature_imp_train_1_1.columns = feature_full_train_1_1.columns\n",
    "feature_imp_train_1_1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 35259 entries, 292 to 176294\n",
      "Data columns (total 31 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   ('cwat', 'amin')              35259 non-null  float64\n",
      " 1   ('cwat', 'amax')              35259 non-null  float64\n",
      " 2   ('cwat', 'mean')              35259 non-null  float64\n",
      " 3   ('cwat', 'var')               35259 non-null  float64\n",
      " 4   ('r', 'amin')                 35259 non-null  float64\n",
      " 5   ('r', 'amax')                 35259 non-null  float64\n",
      " 6   ('r', 'mean')                 35259 non-null  float64\n",
      " 7   ('r', 'var')                  35259 non-null  float64\n",
      " 8   ('tozne', 'amin')             35259 non-null  float64\n",
      " 9   ('tozne', 'amax')             35259 non-null  float64\n",
      " 10  ('tozne', 'mean')             35259 non-null  float64\n",
      " 11  ('tozne', 'var')              35259 non-null  float64\n",
      " 12  ('gh', 'amin')                35259 non-null  float64\n",
      " 13  ('gh', 'amax')                35259 non-null  float64\n",
      " 14  ('gh', 'mean')                35259 non-null  float64\n",
      " 15  ('gh', 'var')                 35259 non-null  float64\n",
      " 16  ('pwat', 'amin')              35259 non-null  float64\n",
      " 17  ('pwat', 'amax')              35259 non-null  float64\n",
      " 18  ('pwat', 'mean')              35259 non-null  float64\n",
      " 19  ('pwat', 'var')               35259 non-null  float64\n",
      " 20  ('paramId_0', 'amin')         35259 non-null  float64\n",
      " 21  ('paramId_0', 'amax')         35259 non-null  float64\n",
      " 22  ('paramId_0', 'mean')         35259 non-null  float64\n",
      " 23  ('paramId_0', 'var')          35259 non-null  float64\n",
      " 24  ('pres', 'amin')              15069 non-null  float64\n",
      " 25  ('pres', 'amax')              15069 non-null  float64\n",
      " 26  ('pres', 'mean')              15069 non-null  float64\n",
      " 27  ('pres', 'var')               9461 non-null   float64\n",
      " 28  ('pres', 'count')             35259 non-null  int64  \n",
      " 29  ('macro_season', '<lambda>')  35259 non-null  int64  \n",
      " 30  ('month', '<lambda>')         35259 non-null  int64  \n",
      "dtypes: float64(28), int64(3)\n",
      "memory usage: 8.6 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 35259 entries, 0 to 35258\n",
      "Data columns (total 31 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   ('cwat', 'amin')              35259 non-null  float64\n",
      " 1   ('cwat', 'amax')              35259 non-null  float64\n",
      " 2   ('cwat', 'mean')              35259 non-null  float64\n",
      " 3   ('cwat', 'var')               35259 non-null  float64\n",
      " 4   ('r', 'amin')                 35259 non-null  float64\n",
      " 5   ('r', 'amax')                 35259 non-null  float64\n",
      " 6   ('r', 'mean')                 35259 non-null  float64\n",
      " 7   ('r', 'var')                  35259 non-null  float64\n",
      " 8   ('tozne', 'amin')             35259 non-null  float64\n",
      " 9   ('tozne', 'amax')             35259 non-null  float64\n",
      " 10  ('tozne', 'mean')             35259 non-null  float64\n",
      " 11  ('tozne', 'var')              35259 non-null  float64\n",
      " 12  ('gh', 'amin')                35259 non-null  float64\n",
      " 13  ('gh', 'amax')                35259 non-null  float64\n",
      " 14  ('gh', 'mean')                35259 non-null  float64\n",
      " 15  ('gh', 'var')                 35259 non-null  float64\n",
      " 16  ('pwat', 'amin')              35259 non-null  float64\n",
      " 17  ('pwat', 'amax')              35259 non-null  float64\n",
      " 18  ('pwat', 'mean')              35259 non-null  float64\n",
      " 19  ('pwat', 'var')               35259 non-null  float64\n",
      " 20  ('paramId_0', 'amin')         35259 non-null  float64\n",
      " 21  ('paramId_0', 'amax')         35259 non-null  float64\n",
      " 22  ('paramId_0', 'mean')         35259 non-null  float64\n",
      " 23  ('paramId_0', 'var')          35259 non-null  float64\n",
      " 24  ('pres', 'amin')              35259 non-null  float64\n",
      " 25  ('pres', 'amax')              35259 non-null  float64\n",
      " 26  ('pres', 'mean')              35259 non-null  float64\n",
      " 27  ('pres', 'var')               35259 non-null  float64\n",
      " 28  ('pres', 'count')             35259 non-null  float64\n",
      " 29  ('macro_season', '<lambda>')  35259 non-null  float64\n",
      " 30  ('month', '<lambda>')         35259 non-null  float64\n",
      "dtypes: float64(31)\n",
      "memory usage: 8.3 MB\n"
     ]
    }
   ],
   "source": [
    "tenday_full_test.info()\n",
    "tenday_imp_test = pd.DataFrame(imp.fit_transform(tenday_full_test))\n",
    "tenday_imp_test.columns = tenday_full_test.columns\n",
    "tenday_imp_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
      "0               0.0              0.42           0.10325         0.009705   \n",
      "1               0.0              0.22           0.07200         0.005263   \n",
      "2               0.0              0.43           0.05825         0.007605   \n",
      "3               0.0              0.32           0.03575         0.005051   \n",
      "4               0.0              0.69           0.11475         0.025159   \n",
      "\n",
      "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
      "0            8.0           48.0         21.625    124.189103   \n",
      "1           10.0           35.0         23.425     35.789103   \n",
      "2           13.0           55.0         27.250     77.935897   \n",
      "3            7.0           29.0         17.275     34.871154   \n",
      "4           18.0           50.0         27.250     39.987179   \n",
      "\n",
      "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('paramId_0', 'amax')  \\\n",
      "0         263.500000         304.299988  ...                   59.0   \n",
      "1         270.799988         360.100006  ...                   53.0   \n",
      "2         244.000000         334.700012  ...                   46.0   \n",
      "3         264.399994         348.000000  ...                    0.0   \n",
      "4         278.700012         407.299988  ...                   64.0   \n",
      "\n",
      "   ('paramId_0', 'mean')  ('paramId_0', 'var')  ('pres', 'amin')  \\\n",
      "0                  1.475             87.025000           34430.0   \n",
      "1                  3.200            144.420513           61430.0   \n",
      "2                  2.400             86.605128           65030.0   \n",
      "3                  0.000              0.000000           41470.0   \n",
      "4                 10.550            408.920513           41470.0   \n",
      "\n",
      "   ('pres', 'amax')  ('pres', 'mean')  ('pres', 'var')  ('pres', 'count')  \\\n",
      "0           34430.0      34430.000000     4.209587e+07                1.0   \n",
      "1           62960.0      62443.333333     7.702333e+05                3.0   \n",
      "2           75960.0      71433.333333     3.250763e+07                3.0   \n",
      "3           53900.0      48860.000000     4.209587e+07                0.0   \n",
      "4           76180.0      57815.000000     2.113109e+08               10.0   \n",
      "\n",
      "   ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
      "0                           0.0                    1.0  \n",
      "1                           0.0                    1.0  \n",
      "2                           0.0                    1.0  \n",
      "3                           0.0                    2.0  \n",
      "4                           0.0                    2.0  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>('cwat', 'amin')</th>\n",
       "      <th>('cwat', 'amax')</th>\n",
       "      <th>('cwat', 'mean')</th>\n",
       "      <th>('cwat', 'var')</th>\n",
       "      <th>('r', 'amin')</th>\n",
       "      <th>('r', 'amax')</th>\n",
       "      <th>('r', 'mean')</th>\n",
       "      <th>('r', 'var')</th>\n",
       "      <th>('tozne', 'amin')</th>\n",
       "      <th>('tozne', 'amax')</th>\n",
       "      <th>...</th>\n",
       "      <th>('paramId_0', 'amax')</th>\n",
       "      <th>('paramId_0', 'mean')</th>\n",
       "      <th>('paramId_0', 'var')</th>\n",
       "      <th>('pres', 'amin')</th>\n",
       "      <th>('pres', 'amax')</th>\n",
       "      <th>('pres', 'mean')</th>\n",
       "      <th>('pres', 'var')</th>\n",
       "      <th>('pres', 'count')</th>\n",
       "      <th>('macro_season', '&lt;lambda&gt;')</th>\n",
       "      <th>('month', '&lt;lambda&gt;')</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.033497</td>\n",
       "      <td>-0.082596</td>\n",
       "      <td>0.917604</td>\n",
       "      <td>-0.239304</td>\n",
       "      <td>-0.111030</td>\n",
       "      <td>1.424850</td>\n",
       "      <td>0.590130</td>\n",
       "      <td>1.862974</td>\n",
       "      <td>-0.857469</td>\n",
       "      <td>-0.930662</td>\n",
       "      <td>...</td>\n",
       "      <td>2.023528</td>\n",
       "      <td>0.227975</td>\n",
       "      <td>0.668879</td>\n",
       "      <td>-0.976788</td>\n",
       "      <td>-2.087739</td>\n",
       "      <td>-1.793674</td>\n",
       "      <td>-0.153575</td>\n",
       "      <td>0.010393</td>\n",
       "      <td>-1.08932</td>\n",
       "      <td>-1.818779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.033497</td>\n",
       "      <td>-0.475691</td>\n",
       "      <td>0.406963</td>\n",
       "      <td>-0.341392</td>\n",
       "      <td>0.426361</td>\n",
       "      <td>0.209403</td>\n",
       "      <td>0.890244</td>\n",
       "      <td>-0.264223</td>\n",
       "      <td>-0.523892</td>\n",
       "      <td>0.564136</td>\n",
       "      <td>...</td>\n",
       "      <td>1.745749</td>\n",
       "      <td>1.051335</td>\n",
       "      <td>1.407598</td>\n",
       "      <td>2.487638</td>\n",
       "      <td>1.068933</td>\n",
       "      <td>1.786621</td>\n",
       "      <td>-1.020774</td>\n",
       "      <td>1.031933</td>\n",
       "      <td>-1.08932</td>\n",
       "      <td>-1.818779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.033497</td>\n",
       "      <td>-0.062941</td>\n",
       "      <td>0.182281</td>\n",
       "      <td>-0.287567</td>\n",
       "      <td>1.232448</td>\n",
       "      <td>2.079321</td>\n",
       "      <td>1.527986</td>\n",
       "      <td>0.749969</td>\n",
       "      <td>-1.748533</td>\n",
       "      <td>-0.116291</td>\n",
       "      <td>...</td>\n",
       "      <td>1.421673</td>\n",
       "      <td>0.669487</td>\n",
       "      <td>0.663475</td>\n",
       "      <td>2.949561</td>\n",
       "      <td>2.507304</td>\n",
       "      <td>2.935605</td>\n",
       "      <td>-0.354779</td>\n",
       "      <td>1.031933</td>\n",
       "      <td>-1.08932</td>\n",
       "      <td>-1.818779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.033497</td>\n",
       "      <td>-0.279143</td>\n",
       "      <td>-0.185381</td>\n",
       "      <td>-0.346261</td>\n",
       "      <td>-0.379726</td>\n",
       "      <td>-0.351573</td>\n",
       "      <td>-0.135146</td>\n",
       "      <td>-0.286312</td>\n",
       "      <td>-0.816344</td>\n",
       "      <td>0.239995</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.707969</td>\n",
       "      <td>-0.476057</td>\n",
       "      <td>-0.451192</td>\n",
       "      <td>-0.073471</td>\n",
       "      <td>0.066499</td>\n",
       "      <td>0.050578</td>\n",
       "      <td>-0.153575</td>\n",
       "      <td>-0.500377</td>\n",
       "      <td>-1.08932</td>\n",
       "      <td>-1.497810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.033497</td>\n",
       "      <td>0.448084</td>\n",
       "      <td>1.105520</td>\n",
       "      <td>0.115874</td>\n",
       "      <td>2.575926</td>\n",
       "      <td>1.611841</td>\n",
       "      <td>1.527986</td>\n",
       "      <td>-0.163203</td>\n",
       "      <td>-0.162896</td>\n",
       "      <td>1.828552</td>\n",
       "      <td>...</td>\n",
       "      <td>2.255011</td>\n",
       "      <td>4.559563</td>\n",
       "      <td>4.811895</td>\n",
       "      <td>-0.073471</td>\n",
       "      <td>2.531646</td>\n",
       "      <td>1.195089</td>\n",
       "      <td>3.397323</td>\n",
       "      <td>4.607323</td>\n",
       "      <td>-1.08932</td>\n",
       "      <td>-1.497810</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
       "0         -0.033497         -0.082596          0.917604        -0.239304   \n",
       "1         -0.033497         -0.475691          0.406963        -0.341392   \n",
       "2         -0.033497         -0.062941          0.182281        -0.287567   \n",
       "3         -0.033497         -0.279143         -0.185381        -0.346261   \n",
       "4         -0.033497          0.448084          1.105520         0.115874   \n",
       "\n",
       "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
       "0      -0.111030       1.424850       0.590130      1.862974   \n",
       "1       0.426361       0.209403       0.890244     -0.264223   \n",
       "2       1.232448       2.079321       1.527986      0.749969   \n",
       "3      -0.379726      -0.351573      -0.135146     -0.286312   \n",
       "4       2.575926       1.611841       1.527986     -0.163203   \n",
       "\n",
       "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('paramId_0', 'amax')  \\\n",
       "0          -0.857469          -0.930662  ...               2.023528   \n",
       "1          -0.523892           0.564136  ...               1.745749   \n",
       "2          -1.748533          -0.116291  ...               1.421673   \n",
       "3          -0.816344           0.239995  ...              -0.707969   \n",
       "4          -0.162896           1.828552  ...               2.255011   \n",
       "\n",
       "   ('paramId_0', 'mean')  ('paramId_0', 'var')  ('pres', 'amin')  \\\n",
       "0               0.227975              0.668879         -0.976788   \n",
       "1               1.051335              1.407598          2.487638   \n",
       "2               0.669487              0.663475          2.949561   \n",
       "3              -0.476057             -0.451192         -0.073471   \n",
       "4               4.559563              4.811895         -0.073471   \n",
       "\n",
       "   ('pres', 'amax')  ('pres', 'mean')  ('pres', 'var')  ('pres', 'count')  \\\n",
       "0         -2.087739         -1.793674        -0.153575           0.010393   \n",
       "1          1.068933          1.786621        -1.020774           1.031933   \n",
       "2          2.507304          2.935605        -0.354779           1.031933   \n",
       "3          0.066499          0.050578        -0.153575          -0.500377   \n",
       "4          2.531646          1.195089         3.397323           4.607323   \n",
       "\n",
       "   ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
       "0                      -1.08932              -1.818779  \n",
       "1                      -1.08932              -1.818779  \n",
       "2                      -1.08932              -1.818779  \n",
       "3                      -1.08932              -1.497810  \n",
       "4                      -1.08932              -1.497810  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(feature_imp_train_1_1.head())\n",
    "full_features_1_1_train = pd.DataFrame(scaler.fit_transform(feature_imp_train_1_1))\n",
    "full_features_1_1_train.columns = feature_imp_train_1_1.columns\n",
    "full_features_1_1_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>('cwat', 'amin')</th>\n",
       "      <th>('cwat', 'amax')</th>\n",
       "      <th>('cwat', 'mean')</th>\n",
       "      <th>('cwat', 'var')</th>\n",
       "      <th>('r', 'amin')</th>\n",
       "      <th>('r', 'amax')</th>\n",
       "      <th>('r', 'mean')</th>\n",
       "      <th>('r', 'var')</th>\n",
       "      <th>('tozne', 'amin')</th>\n",
       "      <th>('tozne', 'amax')</th>\n",
       "      <th>...</th>\n",
       "      <th>('paramId_0', 'amax')</th>\n",
       "      <th>('paramId_0', 'mean')</th>\n",
       "      <th>('paramId_0', 'var')</th>\n",
       "      <th>('pres', 'amin')</th>\n",
       "      <th>('pres', 'amax')</th>\n",
       "      <th>('pres', 'mean')</th>\n",
       "      <th>('pres', 'var')</th>\n",
       "      <th>('pres', 'count')</th>\n",
       "      <th>('macro_season', '&lt;lambda&gt;')</th>\n",
       "      <th>('month', '&lt;lambda&gt;')</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.033497</td>\n",
       "      <td>-0.632930</td>\n",
       "      <td>-0.295679</td>\n",
       "      <td>-0.406379</td>\n",
       "      <td>-0.917117</td>\n",
       "      <td>-0.725556</td>\n",
       "      <td>-1.160535</td>\n",
       "      <td>-0.277427</td>\n",
       "      <td>-1.620586</td>\n",
       "      <td>-1.324452</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.707969</td>\n",
       "      <td>-0.476057</td>\n",
       "      <td>-0.451192</td>\n",
       "      <td>0.086919</td>\n",
       "      <td>0.211442</td>\n",
       "      <td>0.180941</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>-0.500377</td>\n",
       "      <td>-1.08932</td>\n",
       "      <td>-1.818779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.033497</td>\n",
       "      <td>1.705991</td>\n",
       "      <td>1.293436</td>\n",
       "      <td>0.548732</td>\n",
       "      <td>-0.111030</td>\n",
       "      <td>1.892329</td>\n",
       "      <td>1.194526</td>\n",
       "      <td>3.079791</td>\n",
       "      <td>-1.775951</td>\n",
       "      <td>-1.276233</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.707969</td>\n",
       "      <td>-0.476057</td>\n",
       "      <td>-0.451192</td>\n",
       "      <td>0.086919</td>\n",
       "      <td>0.211442</td>\n",
       "      <td>0.180941</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>-0.500377</td>\n",
       "      <td>-1.08932</td>\n",
       "      <td>-1.818779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.033497</td>\n",
       "      <td>0.015678</td>\n",
       "      <td>0.014791</td>\n",
       "      <td>-0.191594</td>\n",
       "      <td>-0.379726</td>\n",
       "      <td>0.302899</td>\n",
       "      <td>-0.618663</td>\n",
       "      <td>0.489514</td>\n",
       "      <td>-1.154491</td>\n",
       "      <td>0.858809</td>\n",
       "      <td>...</td>\n",
       "      <td>0.356852</td>\n",
       "      <td>-0.189671</td>\n",
       "      <td>-0.281035</td>\n",
       "      <td>4.480324</td>\n",
       "      <td>2.617948</td>\n",
       "      <td>3.641951</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.010393</td>\n",
       "      <td>-1.08932</td>\n",
       "      <td>-1.497810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.033497</td>\n",
       "      <td>-0.200524</td>\n",
       "      <td>0.901264</td>\n",
       "      <td>-0.295157</td>\n",
       "      <td>1.769839</td>\n",
       "      <td>0.957370</td>\n",
       "      <td>1.194526</td>\n",
       "      <td>-0.410007</td>\n",
       "      <td>-0.185744</td>\n",
       "      <td>1.576740</td>\n",
       "      <td>...</td>\n",
       "      <td>2.208714</td>\n",
       "      <td>4.368639</td>\n",
       "      <td>4.568078</td>\n",
       "      <td>-0.068339</td>\n",
       "      <td>2.714208</td>\n",
       "      <td>1.474915</td>\n",
       "      <td>3.979345</td>\n",
       "      <td>4.096553</td>\n",
       "      <td>-1.08932</td>\n",
       "      <td>-1.497810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.033497</td>\n",
       "      <td>2.040122</td>\n",
       "      <td>2.972424</td>\n",
       "      <td>3.854674</td>\n",
       "      <td>2.038535</td>\n",
       "      <td>1.892329</td>\n",
       "      <td>1.990662</td>\n",
       "      <td>1.660349</td>\n",
       "      <td>-0.756939</td>\n",
       "      <td>1.126694</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.198706</td>\n",
       "      <td>-0.261267</td>\n",
       "      <td>-0.397763</td>\n",
       "      <td>0.366639</td>\n",
       "      <td>1.794757</td>\n",
       "      <td>1.117766</td>\n",
       "      <td>5.322891</td>\n",
       "      <td>0.521163</td>\n",
       "      <td>-1.08932</td>\n",
       "      <td>-1.497810</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
       "0         -0.033497         -0.632930         -0.295679        -0.406379   \n",
       "1         -0.033497          1.705991          1.293436         0.548732   \n",
       "2         -0.033497          0.015678          0.014791        -0.191594   \n",
       "3         -0.033497         -0.200524          0.901264        -0.295157   \n",
       "4         -0.033497          2.040122          2.972424         3.854674   \n",
       "\n",
       "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
       "0      -0.917117      -0.725556      -1.160535     -0.277427   \n",
       "1      -0.111030       1.892329       1.194526      3.079791   \n",
       "2      -0.379726       0.302899      -0.618663      0.489514   \n",
       "3       1.769839       0.957370       1.194526     -0.410007   \n",
       "4       2.038535       1.892329       1.990662      1.660349   \n",
       "\n",
       "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('paramId_0', 'amax')  \\\n",
       "0          -1.620586          -1.324452  ...              -0.707969   \n",
       "1          -1.775951          -1.276233  ...              -0.707969   \n",
       "2          -1.154491           0.858809  ...               0.356852   \n",
       "3          -0.185744           1.576740  ...               2.208714   \n",
       "4          -0.756939           1.126694  ...              -0.198706   \n",
       "\n",
       "   ('paramId_0', 'mean')  ('paramId_0', 'var')  ('pres', 'amin')  \\\n",
       "0              -0.476057             -0.451192          0.086919   \n",
       "1              -0.476057             -0.451192          0.086919   \n",
       "2              -0.189671             -0.281035          4.480324   \n",
       "3               4.368639              4.568078         -0.068339   \n",
       "4              -0.261267             -0.397763          0.366639   \n",
       "\n",
       "   ('pres', 'amax')  ('pres', 'mean')  ('pres', 'var')  ('pres', 'count')  \\\n",
       "0          0.211442          0.180941         0.039062          -0.500377   \n",
       "1          0.211442          0.180941         0.039062          -0.500377   \n",
       "2          2.617948          3.641951         0.039062           0.010393   \n",
       "3          2.714208          1.474915         3.979345           4.096553   \n",
       "4          1.794757          1.117766         5.322891           0.521163   \n",
       "\n",
       "   ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
       "0                      -1.08932              -1.818779  \n",
       "1                      -1.08932              -1.818779  \n",
       "2                      -1.08932              -1.497810  \n",
       "3                      -1.08932              -1.497810  \n",
       "4                      -1.08932              -1.497810  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tenday_full_features_test = pd.DataFrame(scaler.transform(tenday_imp_test))\n",
    "tenday_full_features_test.columns = tenday_imp_test.columns\n",
    "tenday_full_features_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.6846\n",
      "Test set accuracy: 0.6661\n",
      "Training Set Precision: 0.6714\n",
      "Training Set Recall: 0.7233\n",
      "Training Set F1 Score: 0.6964\n",
      "Test Set Precision: 0.2536\n",
      "Test Set Recall: 0.7014\n",
      "Test Set F1 Score: 0.3725\n"
     ]
    }
   ],
   "source": [
    "clf_log_10day_full = LogisticRegression()\n",
    "clf_log_10day_full.fit(full_features_1_1_train, target_train_1_1)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_log_10day_full.score(full_features_1_1_train, \n",
    "                                                       target_train_1_1), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_log_10day_full.score(tenday_full_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_log = clf_log_10day_full.predict(full_features_1_1_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(target_train_1_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(target_train_1_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(target_train_1_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_log = clf_log_10day_full.predict(tenday_full_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_log), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_log), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_log), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.6819\n",
      "Test set accuracy: 0.6876\n",
      "Training Set Precision: 0.6771\n",
      "Training Set Recall: 0.6957\n",
      "Training Set F1 Score: 0.6863\n",
      "Test Set Precision: 0.2598\n",
      "Test Set Recall: 0.6546\n",
      "Test Set F1 Score: 0.372\n"
     ]
    }
   ],
   "source": [
    "clf_sgd_10day_full = SGDClassifier(loss = \"log\", random_state = 0)\n",
    "clf_sgd_10day_full.fit(full_features_1_1_train, target_train_1_1)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_sgd_10day_full.score(full_features_1_1_train, \n",
    "                                                       target_train_1_1), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_sgd_10day_full.score(tenday_full_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_sgd = clf_sgd_10day_full.predict(full_features_1_1_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(target_train_1_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(target_train_1_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(target_train_1_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_sgd = clf_sgd_10day_full.predict(tenday_full_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_sgd), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_sgd), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_sgd), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Part Parameter List "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    117335\n",
       "0    117335\n",
       "Name: fire, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "over_sampler_1_1 = RandomOverSampler(random_state = 0)\n",
    "feature_part_train_1_1, target_train_1_1 = over_sampler_1_1.fit_resample(tenday_no_null_train, tenday_target_train)\n",
    "target_train_1_1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
      "0               0.0              0.42           0.10325         0.009705   \n",
      "1               0.0              0.22           0.07200         0.005263   \n",
      "2               0.0              0.43           0.05825         0.007605   \n",
      "3               0.0              0.32           0.03575         0.005051   \n",
      "4               0.0              0.69           0.11475         0.025159   \n",
      "\n",
      "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
      "0            8.0           48.0         21.625    124.189103   \n",
      "1           10.0           35.0         23.425     35.789103   \n",
      "2           13.0           55.0         27.250     77.935897   \n",
      "3            7.0           29.0         17.275     34.871154   \n",
      "4           18.0           50.0         27.250     39.987179   \n",
      "\n",
      "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('pwat', 'amax')  \\\n",
      "0         263.500000         304.299988  ...         28.500000   \n",
      "1         270.799988         360.100006  ...         18.299999   \n",
      "2         244.000000         334.700012  ...         22.600000   \n",
      "3         264.399994         348.000000  ...         20.100000   \n",
      "4         278.700012         407.299988  ...         28.000000   \n",
      "\n",
      "   ('pwat', 'mean')  ('pwat', 'var')  ('paramId_0', 'amin')  \\\n",
      "0           13.6700        37.680100                    0.0   \n",
      "1           13.1850         7.657718                    0.0   \n",
      "2           14.3550        16.905103                    0.0   \n",
      "3           11.1450        14.892795                    0.0   \n",
      "4           15.1125        15.711378                    0.0   \n",
      "\n",
      "   ('paramId_0', 'amax')  ('paramId_0', 'mean')  ('paramId_0', 'var')  \\\n",
      "0                   59.0                  1.475             87.025000   \n",
      "1                   53.0                  3.200            144.420513   \n",
      "2                   46.0                  2.400             86.605128   \n",
      "3                    0.0                  0.000              0.000000   \n",
      "4                   64.0                 10.550            408.920513   \n",
      "\n",
      "   ('pres', 'count')  ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
      "0                  1                             0                      1  \n",
      "1                  3                             0                      1  \n",
      "2                  3                             0                      1  \n",
      "3                  0                             0                      2  \n",
      "4                 10                             0                      2  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>('cwat', 'amin')</th>\n",
       "      <th>('cwat', 'amax')</th>\n",
       "      <th>('cwat', 'mean')</th>\n",
       "      <th>('cwat', 'var')</th>\n",
       "      <th>('r', 'amin')</th>\n",
       "      <th>('r', 'amax')</th>\n",
       "      <th>('r', 'mean')</th>\n",
       "      <th>('r', 'var')</th>\n",
       "      <th>('tozne', 'amin')</th>\n",
       "      <th>('tozne', 'amax')</th>\n",
       "      <th>...</th>\n",
       "      <th>('pwat', 'amax')</th>\n",
       "      <th>('pwat', 'mean')</th>\n",
       "      <th>('pwat', 'var')</th>\n",
       "      <th>('paramId_0', 'amin')</th>\n",
       "      <th>('paramId_0', 'amax')</th>\n",
       "      <th>('paramId_0', 'mean')</th>\n",
       "      <th>('paramId_0', 'var')</th>\n",
       "      <th>('pres', 'count')</th>\n",
       "      <th>('macro_season', '&lt;lambda&gt;')</th>\n",
       "      <th>('month', '&lt;lambda&gt;')</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.033497</td>\n",
       "      <td>-0.082596</td>\n",
       "      <td>0.917604</td>\n",
       "      <td>-0.239304</td>\n",
       "      <td>-0.111030</td>\n",
       "      <td>1.424850</td>\n",
       "      <td>0.590130</td>\n",
       "      <td>1.862974</td>\n",
       "      <td>-0.857469</td>\n",
       "      <td>-0.930662</td>\n",
       "      <td>...</td>\n",
       "      <td>0.832605</td>\n",
       "      <td>0.228131</td>\n",
       "      <td>1.099017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.023528</td>\n",
       "      <td>0.227975</td>\n",
       "      <td>0.668879</td>\n",
       "      <td>0.010393</td>\n",
       "      <td>-1.08932</td>\n",
       "      <td>-1.818779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.033497</td>\n",
       "      <td>-0.475691</td>\n",
       "      <td>0.406963</td>\n",
       "      <td>-0.341392</td>\n",
       "      <td>0.426361</td>\n",
       "      <td>0.209403</td>\n",
       "      <td>0.890244</td>\n",
       "      <td>-0.264223</td>\n",
       "      <td>-0.523892</td>\n",
       "      <td>0.564136</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.419642</td>\n",
       "      <td>0.133240</td>\n",
       "      <td>-0.648415</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.745749</td>\n",
       "      <td>1.051335</td>\n",
       "      <td>1.407598</td>\n",
       "      <td>1.031933</td>\n",
       "      <td>-1.08932</td>\n",
       "      <td>-1.818779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.033497</td>\n",
       "      <td>-0.062941</td>\n",
       "      <td>0.182281</td>\n",
       "      <td>-0.287567</td>\n",
       "      <td>1.232448</td>\n",
       "      <td>2.079321</td>\n",
       "      <td>1.527986</td>\n",
       "      <td>0.749969</td>\n",
       "      <td>-1.748533</td>\n",
       "      <td>-0.116291</td>\n",
       "      <td>...</td>\n",
       "      <td>0.108266</td>\n",
       "      <td>0.362152</td>\n",
       "      <td>-0.110178</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.421673</td>\n",
       "      <td>0.669487</td>\n",
       "      <td>0.663475</td>\n",
       "      <td>1.031933</td>\n",
       "      <td>-1.08932</td>\n",
       "      <td>-1.818779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.033497</td>\n",
       "      <td>-0.279143</td>\n",
       "      <td>-0.185381</td>\n",
       "      <td>-0.346261</td>\n",
       "      <td>-0.379726</td>\n",
       "      <td>-0.351573</td>\n",
       "      <td>-0.135146</td>\n",
       "      <td>-0.286312</td>\n",
       "      <td>-0.816344</td>\n",
       "      <td>0.239995</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.198657</td>\n",
       "      <td>-0.265890</td>\n",
       "      <td>-0.227303</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.707969</td>\n",
       "      <td>-0.476057</td>\n",
       "      <td>-0.451192</td>\n",
       "      <td>-0.500377</td>\n",
       "      <td>-1.08932</td>\n",
       "      <td>-1.497810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.033497</td>\n",
       "      <td>0.448084</td>\n",
       "      <td>1.105520</td>\n",
       "      <td>0.115874</td>\n",
       "      <td>2.575926</td>\n",
       "      <td>1.611841</td>\n",
       "      <td>1.527986</td>\n",
       "      <td>-0.163203</td>\n",
       "      <td>-0.162896</td>\n",
       "      <td>1.828552</td>\n",
       "      <td>...</td>\n",
       "      <td>0.771220</td>\n",
       "      <td>0.510358</td>\n",
       "      <td>-0.179658</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.255011</td>\n",
       "      <td>4.559563</td>\n",
       "      <td>4.811895</td>\n",
       "      <td>4.607323</td>\n",
       "      <td>-1.08932</td>\n",
       "      <td>-1.497810</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
       "0         -0.033497         -0.082596          0.917604        -0.239304   \n",
       "1         -0.033497         -0.475691          0.406963        -0.341392   \n",
       "2         -0.033497         -0.062941          0.182281        -0.287567   \n",
       "3         -0.033497         -0.279143         -0.185381        -0.346261   \n",
       "4         -0.033497          0.448084          1.105520         0.115874   \n",
       "\n",
       "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
       "0      -0.111030       1.424850       0.590130      1.862974   \n",
       "1       0.426361       0.209403       0.890244     -0.264223   \n",
       "2       1.232448       2.079321       1.527986      0.749969   \n",
       "3      -0.379726      -0.351573      -0.135146     -0.286312   \n",
       "4       2.575926       1.611841       1.527986     -0.163203   \n",
       "\n",
       "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('pwat', 'amax')  \\\n",
       "0          -0.857469          -0.930662  ...          0.832605   \n",
       "1          -0.523892           0.564136  ...         -0.419642   \n",
       "2          -1.748533          -0.116291  ...          0.108266   \n",
       "3          -0.816344           0.239995  ...         -0.198657   \n",
       "4          -0.162896           1.828552  ...          0.771220   \n",
       "\n",
       "   ('pwat', 'mean')  ('pwat', 'var')  ('paramId_0', 'amin')  \\\n",
       "0          0.228131         1.099017                    0.0   \n",
       "1          0.133240        -0.648415                    0.0   \n",
       "2          0.362152        -0.110178                    0.0   \n",
       "3         -0.265890        -0.227303                    0.0   \n",
       "4          0.510358        -0.179658                    0.0   \n",
       "\n",
       "   ('paramId_0', 'amax')  ('paramId_0', 'mean')  ('paramId_0', 'var')  \\\n",
       "0               2.023528               0.227975              0.668879   \n",
       "1               1.745749               1.051335              1.407598   \n",
       "2               1.421673               0.669487              0.663475   \n",
       "3              -0.707969              -0.476057             -0.451192   \n",
       "4               2.255011               4.559563              4.811895   \n",
       "\n",
       "   ('pres', 'count')  ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
       "0           0.010393                      -1.08932              -1.818779  \n",
       "1           1.031933                      -1.08932              -1.818779  \n",
       "2           1.031933                      -1.08932              -1.818779  \n",
       "3          -0.500377                      -1.08932              -1.497810  \n",
       "4           4.607323                      -1.08932              -1.497810  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(feature_part_train_1_1.head())\n",
    "part_features_1_1_train = pd.DataFrame(scaler.fit_transform(feature_part_train_1_1))\n",
    "part_features_1_1_train.columns = feature_part_train_1_1.columns\n",
    "part_features_1_1_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
      "292               0.0              0.14           0.02900         0.002435   \n",
      "293               0.0              1.33           0.12625         0.043993   \n",
      "294               0.0              0.47           0.04800         0.011781   \n",
      "295               0.0              0.36           0.10225         0.007274   \n",
      "296               0.0              1.50           0.22900         0.187840   \n",
      "\n",
      "     ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
      "292            5.0           25.0         11.125     35.240385   \n",
      "293            8.0           53.0         25.250    174.756410   \n",
      "294            7.0           36.0         14.375     67.112179   \n",
      "295           15.0           43.0         25.250     29.730769   \n",
      "296           16.0           53.0         30.025    115.768590   \n",
      "\n",
      "     ('tozne', 'amin')  ('tozne', 'amax')  ...  ('pwat', 'amax')  \\\n",
      "292         246.800003         289.600006  ...         17.900000   \n",
      "293         243.399994         291.399994  ...         31.100000   \n",
      "294         257.000000         371.100006  ...         24.700001   \n",
      "295         278.200012         397.899994  ...         22.799999   \n",
      "296         265.700012         381.100006  ...         35.400002   \n",
      "\n",
      "     ('pwat', 'mean')  ('pwat', 'var')  ('paramId_0', 'amin')  \\\n",
      "292            8.0600        13.980923                    0.0   \n",
      "293           15.2250        54.741923                    0.0   \n",
      "294           10.2225        26.132044                    0.0   \n",
      "295           14.5500         7.674359                    0.0   \n",
      "296           18.6275        57.461017                    0.0   \n",
      "\n",
      "     ('paramId_0', 'amax')  ('paramId_0', 'mean')  ('paramId_0', 'var')  \\\n",
      "292                    0.0                   0.00              0.000000   \n",
      "293                    0.0                   0.00              0.000000   \n",
      "294                   23.0                   0.60             13.220513   \n",
      "295                   63.0                  10.15            389.976923   \n",
      "296                   11.0                   0.45              4.151282   \n",
      "\n",
      "     ('pres', 'count')  ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
      "292                  0                             0                      1  \n",
      "293                  0                             0                      1  \n",
      "294                  1                             0                      2  \n",
      "295                  9                             0                      2  \n",
      "296                  2                             0                      2  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>('cwat', 'amin')</th>\n",
       "      <th>('cwat', 'amax')</th>\n",
       "      <th>('cwat', 'mean')</th>\n",
       "      <th>('cwat', 'var')</th>\n",
       "      <th>('r', 'amin')</th>\n",
       "      <th>('r', 'amax')</th>\n",
       "      <th>('r', 'mean')</th>\n",
       "      <th>('r', 'var')</th>\n",
       "      <th>('tozne', 'amin')</th>\n",
       "      <th>('tozne', 'amax')</th>\n",
       "      <th>...</th>\n",
       "      <th>('pwat', 'amax')</th>\n",
       "      <th>('pwat', 'mean')</th>\n",
       "      <th>('pwat', 'var')</th>\n",
       "      <th>('paramId_0', 'amin')</th>\n",
       "      <th>('paramId_0', 'amax')</th>\n",
       "      <th>('paramId_0', 'mean')</th>\n",
       "      <th>('paramId_0', 'var')</th>\n",
       "      <th>('pres', 'count')</th>\n",
       "      <th>('macro_season', '&lt;lambda&gt;')</th>\n",
       "      <th>('month', '&lt;lambda&gt;')</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.033497</td>\n",
       "      <td>-0.632930</td>\n",
       "      <td>-0.295679</td>\n",
       "      <td>-0.406379</td>\n",
       "      <td>-0.917117</td>\n",
       "      <td>-0.725556</td>\n",
       "      <td>-1.160535</td>\n",
       "      <td>-0.277427</td>\n",
       "      <td>-1.620586</td>\n",
       "      <td>-1.324452</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.468750</td>\n",
       "      <td>-0.869475</td>\n",
       "      <td>-0.280378</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.707969</td>\n",
       "      <td>-0.476057</td>\n",
       "      <td>-0.451192</td>\n",
       "      <td>-0.500377</td>\n",
       "      <td>-1.08932</td>\n",
       "      <td>-1.818779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.033497</td>\n",
       "      <td>1.705991</td>\n",
       "      <td>1.293436</td>\n",
       "      <td>0.548732</td>\n",
       "      <td>-0.111030</td>\n",
       "      <td>1.892329</td>\n",
       "      <td>1.194526</td>\n",
       "      <td>3.079791</td>\n",
       "      <td>-1.775951</td>\n",
       "      <td>-1.276233</td>\n",
       "      <td>...</td>\n",
       "      <td>1.151805</td>\n",
       "      <td>0.532369</td>\n",
       "      <td>2.092089</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.707969</td>\n",
       "      <td>-0.476057</td>\n",
       "      <td>-0.451192</td>\n",
       "      <td>-0.500377</td>\n",
       "      <td>-1.08932</td>\n",
       "      <td>-1.818779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.033497</td>\n",
       "      <td>0.015678</td>\n",
       "      <td>0.014791</td>\n",
       "      <td>-0.191594</td>\n",
       "      <td>-0.379726</td>\n",
       "      <td>0.302899</td>\n",
       "      <td>-0.618663</td>\n",
       "      <td>0.489514</td>\n",
       "      <td>-1.154491</td>\n",
       "      <td>0.858809</td>\n",
       "      <td>...</td>\n",
       "      <td>0.366081</td>\n",
       "      <td>-0.446378</td>\n",
       "      <td>0.426870</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.356852</td>\n",
       "      <td>-0.189671</td>\n",
       "      <td>-0.281035</td>\n",
       "      <td>0.010393</td>\n",
       "      <td>-1.08932</td>\n",
       "      <td>-1.497810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.033497</td>\n",
       "      <td>-0.200524</td>\n",
       "      <td>0.901264</td>\n",
       "      <td>-0.295157</td>\n",
       "      <td>1.769839</td>\n",
       "      <td>0.957370</td>\n",
       "      <td>1.194526</td>\n",
       "      <td>-0.410007</td>\n",
       "      <td>-0.185744</td>\n",
       "      <td>1.576740</td>\n",
       "      <td>...</td>\n",
       "      <td>0.132820</td>\n",
       "      <td>0.400304</td>\n",
       "      <td>-0.647447</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.208714</td>\n",
       "      <td>4.368639</td>\n",
       "      <td>4.568078</td>\n",
       "      <td>4.096553</td>\n",
       "      <td>-1.08932</td>\n",
       "      <td>-1.497810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.033497</td>\n",
       "      <td>2.040122</td>\n",
       "      <td>2.972424</td>\n",
       "      <td>3.854674</td>\n",
       "      <td>2.038535</td>\n",
       "      <td>1.892329</td>\n",
       "      <td>1.990662</td>\n",
       "      <td>1.660349</td>\n",
       "      <td>-0.756939</td>\n",
       "      <td>1.126694</td>\n",
       "      <td>...</td>\n",
       "      <td>1.679713</td>\n",
       "      <td>1.198074</td>\n",
       "      <td>2.250352</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.198706</td>\n",
       "      <td>-0.261267</td>\n",
       "      <td>-0.397763</td>\n",
       "      <td>0.521163</td>\n",
       "      <td>-1.08932</td>\n",
       "      <td>-1.497810</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
       "0         -0.033497         -0.632930         -0.295679        -0.406379   \n",
       "1         -0.033497          1.705991          1.293436         0.548732   \n",
       "2         -0.033497          0.015678          0.014791        -0.191594   \n",
       "3         -0.033497         -0.200524          0.901264        -0.295157   \n",
       "4         -0.033497          2.040122          2.972424         3.854674   \n",
       "\n",
       "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
       "0      -0.917117      -0.725556      -1.160535     -0.277427   \n",
       "1      -0.111030       1.892329       1.194526      3.079791   \n",
       "2      -0.379726       0.302899      -0.618663      0.489514   \n",
       "3       1.769839       0.957370       1.194526     -0.410007   \n",
       "4       2.038535       1.892329       1.990662      1.660349   \n",
       "\n",
       "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('pwat', 'amax')  \\\n",
       "0          -1.620586          -1.324452  ...         -0.468750   \n",
       "1          -1.775951          -1.276233  ...          1.151805   \n",
       "2          -1.154491           0.858809  ...          0.366081   \n",
       "3          -0.185744           1.576740  ...          0.132820   \n",
       "4          -0.756939           1.126694  ...          1.679713   \n",
       "\n",
       "   ('pwat', 'mean')  ('pwat', 'var')  ('paramId_0', 'amin')  \\\n",
       "0         -0.869475        -0.280378                    0.0   \n",
       "1          0.532369         2.092089                    0.0   \n",
       "2         -0.446378         0.426870                    0.0   \n",
       "3          0.400304        -0.647447                    0.0   \n",
       "4          1.198074         2.250352                    0.0   \n",
       "\n",
       "   ('paramId_0', 'amax')  ('paramId_0', 'mean')  ('paramId_0', 'var')  \\\n",
       "0              -0.707969              -0.476057             -0.451192   \n",
       "1              -0.707969              -0.476057             -0.451192   \n",
       "2               0.356852              -0.189671             -0.281035   \n",
       "3               2.208714               4.368639              4.568078   \n",
       "4              -0.198706              -0.261267             -0.397763   \n",
       "\n",
       "   ('pres', 'count')  ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
       "0          -0.500377                      -1.08932              -1.818779  \n",
       "1          -0.500377                      -1.08932              -1.818779  \n",
       "2           0.010393                      -1.08932              -1.497810  \n",
       "3           4.096553                      -1.08932              -1.497810  \n",
       "4           0.521163                      -1.08932              -1.497810  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tenday_no_null_test.head())\n",
    "tenday_part_features_test = pd.DataFrame(scaler.transform(tenday_no_null_test))\n",
    "tenday_part_features_test.columns = tenday_no_null_test.columns\n",
    "tenday_part_features_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.6846\n",
      "Test set accuracy: 0.6647\n",
      "Training Set Precision: 0.6718\n",
      "Training Set Recall: 0.722\n",
      "Training Set F1 Score: 0.696\n",
      "Test Set Precision: 0.2524\n",
      "Test Set Recall: 0.6996\n",
      "Test Set F1 Score: 0.371\n"
     ]
    }
   ],
   "source": [
    "clf_log_10day_part = LogisticRegression()\n",
    "clf_log_10day_part.fit(part_features_1_1_train, target_train_1_1)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_log_10day_part.score(part_features_1_1_train, \n",
    "                                                       target_train_1_1), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_log_10day_part.score(tenday_part_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_log = clf_log_10day_part.predict(part_features_1_1_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(target_train_1_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(target_train_1_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(target_train_1_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_log = clf_log_10day_part.predict(tenday_part_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_log), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_log), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_log), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.682\n",
      "Test set accuracy: 0.6722\n",
      "Training Set Precision: 0.6721\n",
      "Training Set Recall: 0.7109\n",
      "Training Set F1 Score: 0.691\n",
      "Test Set Precision: 0.2563\n",
      "Test Set Recall: 0.694\n",
      "Test Set F1 Score: 0.3743\n"
     ]
    }
   ],
   "source": [
    "clf_sgd_10day_part = SGDClassifier(loss = \"log\", random_state = 0)\n",
    "clf_sgd_10day_part.fit(part_features_1_1_train, target_train_1_1)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_sgd_10day_part.score(part_features_1_1_train, \n",
    "                                                       target_train_1_1), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_sgd_10day_part.score(tenday_part_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_sgd = clf_sgd_10day_part.predict(part_features_1_1_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(target_train_1_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(target_train_1_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(target_train_1_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_sgd = clf_sgd_10day_part.predict(tenday_part_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_sgd), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_sgd), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_sgd), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2:1 Ratio No Fire:Fire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Full Parameter List "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    117335\n",
       "1     58667\n",
       "Name: fire, dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "over_sampler_2_1 = RandomOverSampler(sampling_strategy = 0.5, random_state = 0)\n",
    "feature_full_train_2_1, target_train_2_1 = over_sampler_2_1.fit_resample(tenday_full_train, tenday_target_train)\n",
    "target_train_2_1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 176002 entries, 0 to 176001\n",
      "Data columns (total 31 columns):\n",
      " #   Column                        Non-Null Count   Dtype  \n",
      "---  ------                        --------------   -----  \n",
      " 0   ('cwat', 'amin')              176002 non-null  float64\n",
      " 1   ('cwat', 'amax')              176002 non-null  float64\n",
      " 2   ('cwat', 'mean')              176002 non-null  float64\n",
      " 3   ('cwat', 'var')               176002 non-null  float64\n",
      " 4   ('r', 'amin')                 176002 non-null  float64\n",
      " 5   ('r', 'amax')                 176002 non-null  float64\n",
      " 6   ('r', 'mean')                 176002 non-null  float64\n",
      " 7   ('r', 'var')                  176002 non-null  float64\n",
      " 8   ('tozne', 'amin')             176002 non-null  float64\n",
      " 9   ('tozne', 'amax')             176002 non-null  float64\n",
      " 10  ('tozne', 'mean')             176002 non-null  float64\n",
      " 11  ('tozne', 'var')              176002 non-null  float64\n",
      " 12  ('gh', 'amin')                176002 non-null  float64\n",
      " 13  ('gh', 'amax')                176002 non-null  float64\n",
      " 14  ('gh', 'mean')                176002 non-null  float64\n",
      " 15  ('gh', 'var')                 176002 non-null  float64\n",
      " 16  ('pwat', 'amin')              176002 non-null  float64\n",
      " 17  ('pwat', 'amax')              176002 non-null  float64\n",
      " 18  ('pwat', 'mean')              176002 non-null  float64\n",
      " 19  ('pwat', 'var')               176002 non-null  float64\n",
      " 20  ('paramId_0', 'amin')         176002 non-null  float64\n",
      " 21  ('paramId_0', 'amax')         176002 non-null  float64\n",
      " 22  ('paramId_0', 'mean')         176002 non-null  float64\n",
      " 23  ('paramId_0', 'var')          176002 non-null  float64\n",
      " 24  ('pres', 'amin')              65463 non-null   float64\n",
      " 25  ('pres', 'amax')              65463 non-null   float64\n",
      " 26  ('pres', 'mean')              65463 non-null   float64\n",
      " 27  ('pres', 'var')               40426 non-null   float64\n",
      " 28  ('pres', 'count')             176002 non-null  int64  \n",
      " 29  ('macro_season', '<lambda>')  176002 non-null  int64  \n",
      " 30  ('month', '<lambda>')         176002 non-null  int64  \n",
      "dtypes: float64(28), int64(3)\n",
      "memory usage: 41.6 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 176002 entries, 0 to 176001\n",
      "Data columns (total 31 columns):\n",
      " #   Column                        Non-Null Count   Dtype  \n",
      "---  ------                        --------------   -----  \n",
      " 0   ('cwat', 'amin')              176002 non-null  float64\n",
      " 1   ('cwat', 'amax')              176002 non-null  float64\n",
      " 2   ('cwat', 'mean')              176002 non-null  float64\n",
      " 3   ('cwat', 'var')               176002 non-null  float64\n",
      " 4   ('r', 'amin')                 176002 non-null  float64\n",
      " 5   ('r', 'amax')                 176002 non-null  float64\n",
      " 6   ('r', 'mean')                 176002 non-null  float64\n",
      " 7   ('r', 'var')                  176002 non-null  float64\n",
      " 8   ('tozne', 'amin')             176002 non-null  float64\n",
      " 9   ('tozne', 'amax')             176002 non-null  float64\n",
      " 10  ('tozne', 'mean')             176002 non-null  float64\n",
      " 11  ('tozne', 'var')              176002 non-null  float64\n",
      " 12  ('gh', 'amin')                176002 non-null  float64\n",
      " 13  ('gh', 'amax')                176002 non-null  float64\n",
      " 14  ('gh', 'mean')                176002 non-null  float64\n",
      " 15  ('gh', 'var')                 176002 non-null  float64\n",
      " 16  ('pwat', 'amin')              176002 non-null  float64\n",
      " 17  ('pwat', 'amax')              176002 non-null  float64\n",
      " 18  ('pwat', 'mean')              176002 non-null  float64\n",
      " 19  ('pwat', 'var')               176002 non-null  float64\n",
      " 20  ('paramId_0', 'amin')         176002 non-null  float64\n",
      " 21  ('paramId_0', 'amax')         176002 non-null  float64\n",
      " 22  ('paramId_0', 'mean')         176002 non-null  float64\n",
      " 23  ('paramId_0', 'var')          176002 non-null  float64\n",
      " 24  ('pres', 'amin')              176002 non-null  float64\n",
      " 25  ('pres', 'amax')              176002 non-null  float64\n",
      " 26  ('pres', 'mean')              176002 non-null  float64\n",
      " 27  ('pres', 'var')               176002 non-null  float64\n",
      " 28  ('pres', 'count')             176002 non-null  float64\n",
      " 29  ('macro_season', '<lambda>')  176002 non-null  float64\n",
      " 30  ('month', '<lambda>')         176002 non-null  float64\n",
      "dtypes: float64(31)\n",
      "memory usage: 41.6 MB\n"
     ]
    }
   ],
   "source": [
    "feature_full_train_2_1.info()\n",
    "feature_imp_train_2_1 = pd.DataFrame(imp.fit_transform(feature_full_train_2_1))\n",
    "feature_imp_train_2_1.columns = feature_full_train_2_1.columns\n",
    "feature_imp_train_2_1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 35259 entries, 292 to 176294\n",
      "Data columns (total 31 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   ('cwat', 'amin')              35259 non-null  float64\n",
      " 1   ('cwat', 'amax')              35259 non-null  float64\n",
      " 2   ('cwat', 'mean')              35259 non-null  float64\n",
      " 3   ('cwat', 'var')               35259 non-null  float64\n",
      " 4   ('r', 'amin')                 35259 non-null  float64\n",
      " 5   ('r', 'amax')                 35259 non-null  float64\n",
      " 6   ('r', 'mean')                 35259 non-null  float64\n",
      " 7   ('r', 'var')                  35259 non-null  float64\n",
      " 8   ('tozne', 'amin')             35259 non-null  float64\n",
      " 9   ('tozne', 'amax')             35259 non-null  float64\n",
      " 10  ('tozne', 'mean')             35259 non-null  float64\n",
      " 11  ('tozne', 'var')              35259 non-null  float64\n",
      " 12  ('gh', 'amin')                35259 non-null  float64\n",
      " 13  ('gh', 'amax')                35259 non-null  float64\n",
      " 14  ('gh', 'mean')                35259 non-null  float64\n",
      " 15  ('gh', 'var')                 35259 non-null  float64\n",
      " 16  ('pwat', 'amin')              35259 non-null  float64\n",
      " 17  ('pwat', 'amax')              35259 non-null  float64\n",
      " 18  ('pwat', 'mean')              35259 non-null  float64\n",
      " 19  ('pwat', 'var')               35259 non-null  float64\n",
      " 20  ('paramId_0', 'amin')         35259 non-null  float64\n",
      " 21  ('paramId_0', 'amax')         35259 non-null  float64\n",
      " 22  ('paramId_0', 'mean')         35259 non-null  float64\n",
      " 23  ('paramId_0', 'var')          35259 non-null  float64\n",
      " 24  ('pres', 'amin')              15069 non-null  float64\n",
      " 25  ('pres', 'amax')              15069 non-null  float64\n",
      " 26  ('pres', 'mean')              15069 non-null  float64\n",
      " 27  ('pres', 'var')               9461 non-null   float64\n",
      " 28  ('pres', 'count')             35259 non-null  int64  \n",
      " 29  ('macro_season', '<lambda>')  35259 non-null  int64  \n",
      " 30  ('month', '<lambda>')         35259 non-null  int64  \n",
      "dtypes: float64(28), int64(3)\n",
      "memory usage: 8.6 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 35259 entries, 0 to 35258\n",
      "Data columns (total 31 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   ('cwat', 'amin')              35259 non-null  float64\n",
      " 1   ('cwat', 'amax')              35259 non-null  float64\n",
      " 2   ('cwat', 'mean')              35259 non-null  float64\n",
      " 3   ('cwat', 'var')               35259 non-null  float64\n",
      " 4   ('r', 'amin')                 35259 non-null  float64\n",
      " 5   ('r', 'amax')                 35259 non-null  float64\n",
      " 6   ('r', 'mean')                 35259 non-null  float64\n",
      " 7   ('r', 'var')                  35259 non-null  float64\n",
      " 8   ('tozne', 'amin')             35259 non-null  float64\n",
      " 9   ('tozne', 'amax')             35259 non-null  float64\n",
      " 10  ('tozne', 'mean')             35259 non-null  float64\n",
      " 11  ('tozne', 'var')              35259 non-null  float64\n",
      " 12  ('gh', 'amin')                35259 non-null  float64\n",
      " 13  ('gh', 'amax')                35259 non-null  float64\n",
      " 14  ('gh', 'mean')                35259 non-null  float64\n",
      " 15  ('gh', 'var')                 35259 non-null  float64\n",
      " 16  ('pwat', 'amin')              35259 non-null  float64\n",
      " 17  ('pwat', 'amax')              35259 non-null  float64\n",
      " 18  ('pwat', 'mean')              35259 non-null  float64\n",
      " 19  ('pwat', 'var')               35259 non-null  float64\n",
      " 20  ('paramId_0', 'amin')         35259 non-null  float64\n",
      " 21  ('paramId_0', 'amax')         35259 non-null  float64\n",
      " 22  ('paramId_0', 'mean')         35259 non-null  float64\n",
      " 23  ('paramId_0', 'var')          35259 non-null  float64\n",
      " 24  ('pres', 'amin')              35259 non-null  float64\n",
      " 25  ('pres', 'amax')              35259 non-null  float64\n",
      " 26  ('pres', 'mean')              35259 non-null  float64\n",
      " 27  ('pres', 'var')               35259 non-null  float64\n",
      " 28  ('pres', 'count')             35259 non-null  float64\n",
      " 29  ('macro_season', '<lambda>')  35259 non-null  float64\n",
      " 30  ('month', '<lambda>')         35259 non-null  float64\n",
      "dtypes: float64(31)\n",
      "memory usage: 8.3 MB\n"
     ]
    }
   ],
   "source": [
    "tenday_full_test.info()\n",
    "tenday_imp_test = pd.DataFrame(imp.fit_transform(tenday_full_test))\n",
    "tenday_imp_test.columns = tenday_full_test.columns\n",
    "tenday_imp_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
      "0               0.0              0.42           0.10325         0.009705   \n",
      "1               0.0              0.22           0.07200         0.005263   \n",
      "2               0.0              0.43           0.05825         0.007605   \n",
      "3               0.0              0.32           0.03575         0.005051   \n",
      "4               0.0              0.69           0.11475         0.025159   \n",
      "\n",
      "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
      "0            8.0           48.0         21.625    124.189103   \n",
      "1           10.0           35.0         23.425     35.789103   \n",
      "2           13.0           55.0         27.250     77.935897   \n",
      "3            7.0           29.0         17.275     34.871154   \n",
      "4           18.0           50.0         27.250     39.987179   \n",
      "\n",
      "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('paramId_0', 'amax')  \\\n",
      "0         263.500000         304.299988  ...                   59.0   \n",
      "1         270.799988         360.100006  ...                   53.0   \n",
      "2         244.000000         334.700012  ...                   46.0   \n",
      "3         264.399994         348.000000  ...                    0.0   \n",
      "4         278.700012         407.299988  ...                   64.0   \n",
      "\n",
      "   ('paramId_0', 'mean')  ('paramId_0', 'var')  ('pres', 'amin')  \\\n",
      "0                  1.475             87.025000           34430.0   \n",
      "1                  3.200            144.420513           61430.0   \n",
      "2                  2.400             86.605128           65030.0   \n",
      "3                  0.000              0.000000           42320.0   \n",
      "4                 10.550            408.920513           41470.0   \n",
      "\n",
      "   ('pres', 'amax')  ('pres', 'mean')  ('pres', 'var')  ('pres', 'count')  \\\n",
      "0           34430.0      34430.000000     4.623302e+07                1.0   \n",
      "1           62960.0      62443.333333     7.702333e+05                3.0   \n",
      "2           75960.0      71433.333333     3.250763e+07                3.0   \n",
      "3           55140.0      49855.000000     4.623302e+07                0.0   \n",
      "4           76180.0      57815.000000     2.113109e+08               10.0   \n",
      "\n",
      "   ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
      "0                           0.0                    1.0  \n",
      "1                           0.0                    1.0  \n",
      "2                           0.0                    1.0  \n",
      "3                           0.0                    2.0  \n",
      "4                           0.0                    2.0  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>('cwat', 'amin')</th>\n",
       "      <th>('cwat', 'amax')</th>\n",
       "      <th>('cwat', 'mean')</th>\n",
       "      <th>('cwat', 'var')</th>\n",
       "      <th>('r', 'amin')</th>\n",
       "      <th>('r', 'amax')</th>\n",
       "      <th>('r', 'mean')</th>\n",
       "      <th>('r', 'var')</th>\n",
       "      <th>('tozne', 'amin')</th>\n",
       "      <th>('tozne', 'amax')</th>\n",
       "      <th>...</th>\n",
       "      <th>('paramId_0', 'amax')</th>\n",
       "      <th>('paramId_0', 'mean')</th>\n",
       "      <th>('paramId_0', 'var')</th>\n",
       "      <th>('pres', 'amin')</th>\n",
       "      <th>('pres', 'amax')</th>\n",
       "      <th>('pres', 'mean')</th>\n",
       "      <th>('pres', 'var')</th>\n",
       "      <th>('pres', 'count')</th>\n",
       "      <th>('macro_season', '&lt;lambda&gt;')</th>\n",
       "      <th>('month', '&lt;lambda&gt;')</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.037775</td>\n",
       "      <td>-0.130178</td>\n",
       "      <td>0.800500</td>\n",
       "      <td>-0.266619</td>\n",
       "      <td>-0.161319</td>\n",
       "      <td>1.343027</td>\n",
       "      <td>0.511713</td>\n",
       "      <td>1.779130</td>\n",
       "      <td>-0.802274</td>\n",
       "      <td>-0.952719</td>\n",
       "      <td>...</td>\n",
       "      <td>1.929016</td>\n",
       "      <td>0.173949</td>\n",
       "      <td>0.595503</td>\n",
       "      <td>-1.071509</td>\n",
       "      <td>-2.216088</td>\n",
       "      <td>-1.922423</td>\n",
       "      <td>-0.154571</td>\n",
       "      <td>-0.036188</td>\n",
       "      <td>-1.040288</td>\n",
       "      <td>-1.689143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.037775</td>\n",
       "      <td>-0.516447</td>\n",
       "      <td>0.313824</td>\n",
       "      <td>-0.364745</td>\n",
       "      <td>0.363122</td>\n",
       "      <td>0.150247</td>\n",
       "      <td>0.803748</td>\n",
       "      <td>-0.296747</td>\n",
       "      <td>-0.477777</td>\n",
       "      <td>0.506739</td>\n",
       "      <td>...</td>\n",
       "      <td>1.657794</td>\n",
       "      <td>0.953398</td>\n",
       "      <td>1.298654</td>\n",
       "      <td>2.348679</td>\n",
       "      <td>0.929383</td>\n",
       "      <td>1.661302</td>\n",
       "      <td>-1.056673</td>\n",
       "      <td>0.928919</td>\n",
       "      <td>-1.040288</td>\n",
       "      <td>-1.689143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.037775</td>\n",
       "      <td>-0.110865</td>\n",
       "      <td>0.099687</td>\n",
       "      <td>-0.313009</td>\n",
       "      <td>1.149783</td>\n",
       "      <td>1.985293</td>\n",
       "      <td>1.424321</td>\n",
       "      <td>0.692977</td>\n",
       "      <td>-1.669082</td>\n",
       "      <td>-0.157602</td>\n",
       "      <td>...</td>\n",
       "      <td>1.341368</td>\n",
       "      <td>0.591914</td>\n",
       "      <td>0.590359</td>\n",
       "      <td>2.804703</td>\n",
       "      <td>2.362650</td>\n",
       "      <td>2.811386</td>\n",
       "      <td>-0.426919</td>\n",
       "      <td>0.928919</td>\n",
       "      <td>-1.040288</td>\n",
       "      <td>-1.689143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.037775</td>\n",
       "      <td>-0.323313</td>\n",
       "      <td>-0.250720</td>\n",
       "      <td>-0.369425</td>\n",
       "      <td>-0.423539</td>\n",
       "      <td>-0.400267</td>\n",
       "      <td>-0.194037</td>\n",
       "      <td>-0.318303</td>\n",
       "      <td>-0.762268</td>\n",
       "      <td>0.190262</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.738000</td>\n",
       "      <td>-0.492537</td>\n",
       "      <td>-0.470639</td>\n",
       "      <td>-0.072054</td>\n",
       "      <td>0.067217</td>\n",
       "      <td>0.050886</td>\n",
       "      <td>-0.154571</td>\n",
       "      <td>-0.518742</td>\n",
       "      <td>-1.040288</td>\n",
       "      <td>-1.385019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.037775</td>\n",
       "      <td>0.391284</td>\n",
       "      <td>0.979597</td>\n",
       "      <td>0.074775</td>\n",
       "      <td>2.460886</td>\n",
       "      <td>1.526531</td>\n",
       "      <td>1.424321</td>\n",
       "      <td>-0.198164</td>\n",
       "      <td>-0.126608</td>\n",
       "      <td>1.741262</td>\n",
       "      <td>...</td>\n",
       "      <td>2.155034</td>\n",
       "      <td>4.274530</td>\n",
       "      <td>4.539037</td>\n",
       "      <td>-0.179727</td>\n",
       "      <td>2.386905</td>\n",
       "      <td>1.069203</td>\n",
       "      <td>3.121008</td>\n",
       "      <td>4.306797</td>\n",
       "      <td>-1.040288</td>\n",
       "      <td>-1.385019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
       "0         -0.037775         -0.130178          0.800500        -0.266619   \n",
       "1         -0.037775         -0.516447          0.313824        -0.364745   \n",
       "2         -0.037775         -0.110865          0.099687        -0.313009   \n",
       "3         -0.037775         -0.323313         -0.250720        -0.369425   \n",
       "4         -0.037775          0.391284          0.979597         0.074775   \n",
       "\n",
       "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
       "0      -0.161319       1.343027       0.511713      1.779130   \n",
       "1       0.363122       0.150247       0.803748     -0.296747   \n",
       "2       1.149783       1.985293       1.424321      0.692977   \n",
       "3      -0.423539      -0.400267      -0.194037     -0.318303   \n",
       "4       2.460886       1.526531       1.424321     -0.198164   \n",
       "\n",
       "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('paramId_0', 'amax')  \\\n",
       "0          -0.802274          -0.952719  ...               1.929016   \n",
       "1          -0.477777           0.506739  ...               1.657794   \n",
       "2          -1.669082          -0.157602  ...               1.341368   \n",
       "3          -0.762268           0.190262  ...              -0.738000   \n",
       "4          -0.126608           1.741262  ...               2.155034   \n",
       "\n",
       "   ('paramId_0', 'mean')  ('paramId_0', 'var')  ('pres', 'amin')  \\\n",
       "0               0.173949              0.595503         -1.071509   \n",
       "1               0.953398              1.298654          2.348679   \n",
       "2               0.591914              0.590359          2.804703   \n",
       "3              -0.492537             -0.470639         -0.072054   \n",
       "4               4.274530              4.539037         -0.179727   \n",
       "\n",
       "   ('pres', 'amax')  ('pres', 'mean')  ('pres', 'var')  ('pres', 'count')  \\\n",
       "0         -2.216088         -1.922423        -0.154571          -0.036188   \n",
       "1          0.929383          1.661302        -1.056673           0.928919   \n",
       "2          2.362650          2.811386        -0.426919           0.928919   \n",
       "3          0.067217          0.050886        -0.154571          -0.518742   \n",
       "4          2.386905          1.069203         3.121008           4.306797   \n",
       "\n",
       "   ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
       "0                     -1.040288              -1.689143  \n",
       "1                     -1.040288              -1.689143  \n",
       "2                     -1.040288              -1.689143  \n",
       "3                     -1.040288              -1.385019  \n",
       "4                     -1.040288              -1.385019  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(feature_imp_train_2_1.head())\n",
    "full_features_2_1_train = pd.DataFrame(scaler.fit_transform(feature_imp_train_2_1))\n",
    "full_features_2_1_train.columns = feature_imp_train_2_1.columns\n",
    "full_features_2_1_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>('cwat', 'amin')</th>\n",
       "      <th>('cwat', 'amax')</th>\n",
       "      <th>('cwat', 'mean')</th>\n",
       "      <th>('cwat', 'var')</th>\n",
       "      <th>('r', 'amin')</th>\n",
       "      <th>('r', 'amax')</th>\n",
       "      <th>('r', 'mean')</th>\n",
       "      <th>('r', 'var')</th>\n",
       "      <th>('tozne', 'amin')</th>\n",
       "      <th>('tozne', 'amax')</th>\n",
       "      <th>...</th>\n",
       "      <th>('paramId_0', 'amax')</th>\n",
       "      <th>('paramId_0', 'mean')</th>\n",
       "      <th>('paramId_0', 'var')</th>\n",
       "      <th>('pres', 'amin')</th>\n",
       "      <th>('pres', 'amax')</th>\n",
       "      <th>('pres', 'mean')</th>\n",
       "      <th>('pres', 'var')</th>\n",
       "      <th>('pres', 'count')</th>\n",
       "      <th>('macro_season', '&lt;lambda&gt;')</th>\n",
       "      <th>('month', '&lt;lambda&gt;')</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.03801</td>\n",
       "      <td>-0.670864</td>\n",
       "      <td>-0.355975</td>\n",
       "      <td>-0.426868</td>\n",
       "      <td>-0.943816</td>\n",
       "      <td>-0.766303</td>\n",
       "      <td>-1.189225</td>\n",
       "      <td>-0.309246</td>\n",
       "      <td>-1.544284</td>\n",
       "      <td>-1.337052</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.737355</td>\n",
       "      <td>-0.491580</td>\n",
       "      <td>-0.469885</td>\n",
       "      <td>-0.018022</td>\n",
       "      <td>0.077717</td>\n",
       "      <td>0.056080</td>\n",
       "      <td>-0.051342</td>\n",
       "      <td>-0.518135</td>\n",
       "      <td>-1.038643</td>\n",
       "      <td>-1.686755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.03801</td>\n",
       "      <td>1.623781</td>\n",
       "      <td>1.154754</td>\n",
       "      <td>0.487954</td>\n",
       "      <td>-0.160088</td>\n",
       "      <td>1.801651</td>\n",
       "      <td>1.097879</td>\n",
       "      <td>2.964082</td>\n",
       "      <td>-1.695579</td>\n",
       "      <td>-1.289942</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.737355</td>\n",
       "      <td>-0.491580</td>\n",
       "      <td>-0.469885</td>\n",
       "      <td>-0.018022</td>\n",
       "      <td>0.077717</td>\n",
       "      <td>0.056080</td>\n",
       "      <td>-0.051342</td>\n",
       "      <td>-0.518135</td>\n",
       "      <td>-1.038643</td>\n",
       "      <td>-1.686755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.03801</td>\n",
       "      <td>-0.034534</td>\n",
       "      <td>-0.060819</td>\n",
       "      <td>-0.221143</td>\n",
       "      <td>-0.421331</td>\n",
       "      <td>0.242536</td>\n",
       "      <td>-0.662989</td>\n",
       "      <td>0.438531</td>\n",
       "      <td>-1.090398</td>\n",
       "      <td>0.796010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.302257</td>\n",
       "      <td>-0.221219</td>\n",
       "      <td>-0.308345</td>\n",
       "      <td>4.321397</td>\n",
       "      <td>2.477679</td>\n",
       "      <td>3.522089</td>\n",
       "      <td>-0.051342</td>\n",
       "      <td>-0.036243</td>\n",
       "      <td>-1.038643</td>\n",
       "      <td>-1.382849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.03801</td>\n",
       "      <td>-0.246644</td>\n",
       "      <td>0.781926</td>\n",
       "      <td>-0.320338</td>\n",
       "      <td>1.668610</td>\n",
       "      <td>0.884525</td>\n",
       "      <td>1.097879</td>\n",
       "      <td>-0.438513</td>\n",
       "      <td>-0.147027</td>\n",
       "      <td>1.497434</td>\n",
       "      <td>...</td>\n",
       "      <td>2.110277</td>\n",
       "      <td>4.082032</td>\n",
       "      <td>4.295186</td>\n",
       "      <td>-0.171371</td>\n",
       "      <td>2.573677</td>\n",
       "      <td>1.351923</td>\n",
       "      <td>3.681117</td>\n",
       "      <td>3.818897</td>\n",
       "      <td>-1.038643</td>\n",
       "      <td>-1.382849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.03801</td>\n",
       "      <td>1.951587</td>\n",
       "      <td>2.750922</td>\n",
       "      <td>3.654443</td>\n",
       "      <td>1.929853</td>\n",
       "      <td>1.801651</td>\n",
       "      <td>1.871042</td>\n",
       "      <td>1.580109</td>\n",
       "      <td>-0.703259</td>\n",
       "      <td>1.057735</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.240149</td>\n",
       "      <td>-0.288809</td>\n",
       "      <td>-0.419161</td>\n",
       "      <td>0.258262</td>\n",
       "      <td>1.656726</td>\n",
       "      <td>0.994258</td>\n",
       "      <td>4.953800</td>\n",
       "      <td>0.445650</td>\n",
       "      <td>-1.038643</td>\n",
       "      <td>-1.382849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
       "0          -0.03801         -0.670864         -0.355975        -0.426868   \n",
       "1          -0.03801          1.623781          1.154754         0.487954   \n",
       "2          -0.03801         -0.034534         -0.060819        -0.221143   \n",
       "3          -0.03801         -0.246644          0.781926        -0.320338   \n",
       "4          -0.03801          1.951587          2.750922         3.654443   \n",
       "\n",
       "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
       "0      -0.943816      -0.766303      -1.189225     -0.309246   \n",
       "1      -0.160088       1.801651       1.097879      2.964082   \n",
       "2      -0.421331       0.242536      -0.662989      0.438531   \n",
       "3       1.668610       0.884525       1.097879     -0.438513   \n",
       "4       1.929853       1.801651       1.871042      1.580109   \n",
       "\n",
       "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('paramId_0', 'amax')  \\\n",
       "0          -1.544284          -1.337052  ...              -0.737355   \n",
       "1          -1.695579          -1.289942  ...              -0.737355   \n",
       "2          -1.090398           0.796010  ...               0.302257   \n",
       "3          -0.147027           1.497434  ...               2.110277   \n",
       "4          -0.703259           1.057735  ...              -0.240149   \n",
       "\n",
       "   ('paramId_0', 'mean')  ('paramId_0', 'var')  ('pres', 'amin')  \\\n",
       "0              -0.491580             -0.469885         -0.018022   \n",
       "1              -0.491580             -0.469885         -0.018022   \n",
       "2              -0.221219             -0.308345          4.321397   \n",
       "3               4.082032              4.295186         -0.171371   \n",
       "4              -0.288809             -0.419161          0.258262   \n",
       "\n",
       "   ('pres', 'amax')  ('pres', 'mean')  ('pres', 'var')  ('pres', 'count')  \\\n",
       "0          0.077717          0.056080        -0.051342          -0.518135   \n",
       "1          0.077717          0.056080        -0.051342          -0.518135   \n",
       "2          2.477679          3.522089        -0.051342          -0.036243   \n",
       "3          2.573677          1.351923         3.681117           3.818897   \n",
       "4          1.656726          0.994258         4.953800           0.445650   \n",
       "\n",
       "   ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
       "0                     -1.038643              -1.686755  \n",
       "1                     -1.038643              -1.686755  \n",
       "2                     -1.038643              -1.382849  \n",
       "3                     -1.038643              -1.382849  \n",
       "4                     -1.038643              -1.382849  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tenday_full_features_test = pd.DataFrame(scaler.transform(tenday_imp_test))\n",
    "tenday_full_features_test.columns = tenday_imp_test.columns\n",
    "tenday_full_features_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.6714\n",
      "Test set accuracy: 0.6643\n",
      "Training Set Precision: 0.5049\n",
      "Training Set Recall: 0.7233\n",
      "Training Set F1 Score: 0.5947\n",
      "Test Set Precision: 0.2531\n",
      "Test Set Recall: 0.7052\n",
      "Test Set F1 Score: 0.3725\n"
     ]
    }
   ],
   "source": [
    "clf_log_10day_full = LogisticRegression(class_weight = \"balanced\", max_iter = 100000)\n",
    "clf_log_10day_full.fit(full_features_2_1_train, target_train_2_1)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_log_10day_full.score(full_features_2_1_train, \n",
    "                                                       target_train_2_1), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_log_10day_full.score(tenday_full_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_log = clf_log_10day_full.predict(full_features_2_1_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(target_train_2_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(target_train_2_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(target_train_2_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_log = clf_log_10day_full.predict(tenday_full_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_log), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_log), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_log), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.7163\n",
      "Test set accuracy: 0.8002\n",
      "Training Set Precision: 0.6143\n",
      "Training Set Recall: 0.4001\n",
      "Training Set F1 Score: 0.4846\n",
      "Test Set Precision: 0.3318\n",
      "Test Set Recall: 0.408\n",
      "Test Set F1 Score: 0.3659\n"
     ]
    }
   ],
   "source": [
    "clf_log_10day_full = LogisticRegression(max_iter = 100000)\n",
    "clf_log_10day_full.fit(full_features_2_1_train, target_train_2_1)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_log_10day_full.score(full_features_2_1_train, \n",
    "                                                       target_train_2_1), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_log_10day_full.score(tenday_full_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_log = clf_log_10day_full.predict(full_features_2_1_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(target_train_2_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(target_train_2_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(target_train_2_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_log = clf_log_10day_full.predict(tenday_full_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_log), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_log), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_log), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.6677\n",
      "Test set accuracy: 0.6547\n",
      "Training Set Precision: 0.501\n",
      "Training Set Recall: 0.7147\n",
      "Training Set F1 Score: 0.5891\n",
      "Test Set Precision: 0.2455\n",
      "Test Set Recall: 0.6962\n",
      "Test Set F1 Score: 0.363\n"
     ]
    }
   ],
   "source": [
    "clf_sgd_10day_full = SGDClassifier(loss = \"log\", class_weight = \"balanced\", random_state = 0)\n",
    "clf_sgd_10day_full.fit(full_features_2_1_train, target_train_2_1)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_sgd_10day_full.score(full_features_2_1_train, \n",
    "                                                       target_train_2_1), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_sgd_10day_full.score(tenday_full_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_sgd = clf_sgd_10day_full.predict(full_features_2_1_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(target_train_2_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(target_train_2_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(target_train_2_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_sgd = clf_sgd_10day_full.predict(tenday_full_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_sgd), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_sgd), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_sgd), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.7161\n",
      "Test set accuracy: 0.7935\n",
      "Training Set Precision: 0.6104\n",
      "Training Set Recall: 0.4101\n",
      "Training Set F1 Score: 0.4906\n",
      "Test Set Precision: 0.3209\n",
      "Test Set Recall: 0.413\n",
      "Test Set F1 Score: 0.3612\n"
     ]
    }
   ],
   "source": [
    "clf_sgd_10day_full = SGDClassifier(loss = \"log\", random_state = 0)\n",
    "clf_sgd_10day_full.fit(full_features_2_1_train, target_train_2_1)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_sgd_10day_full.score(full_features_2_1_train, \n",
    "                                                       target_train_2_1), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_sgd_10day_full.score(tenday_full_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_sgd = clf_sgd_10day_full.predict(full_features_2_1_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(target_train_2_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(target_train_2_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(target_train_2_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_sgd = clf_sgd_10day_full.predict(tenday_full_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_sgd), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_sgd), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_sgd), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Part Parameter List "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    117335\n",
       "1     58667\n",
       "Name: fire, dtype: int64"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_part_train_2_1, target_train_2_1 = over_sampler_2_1.fit_resample(tenday_no_null_train, tenday_target_train)\n",
    "target_train_2_1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
      "0               0.0              0.42           0.10325         0.009705   \n",
      "1               0.0              0.22           0.07200         0.005263   \n",
      "2               0.0              0.43           0.05825         0.007605   \n",
      "3               0.0              0.32           0.03575         0.005051   \n",
      "4               0.0              0.69           0.11475         0.025159   \n",
      "\n",
      "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
      "0            8.0           48.0         21.625    124.189103   \n",
      "1           10.0           35.0         23.425     35.789103   \n",
      "2           13.0           55.0         27.250     77.935897   \n",
      "3            7.0           29.0         17.275     34.871154   \n",
      "4           18.0           50.0         27.250     39.987179   \n",
      "\n",
      "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('pwat', 'amax')  \\\n",
      "0         263.500000         304.299988  ...         28.500000   \n",
      "1         270.799988         360.100006  ...         18.299999   \n",
      "2         244.000000         334.700012  ...         22.600000   \n",
      "3         264.399994         348.000000  ...         20.100000   \n",
      "4         278.700012         407.299988  ...         28.000000   \n",
      "\n",
      "   ('pwat', 'mean')  ('pwat', 'var')  ('paramId_0', 'amin')  \\\n",
      "0           13.6700        37.680100                    0.0   \n",
      "1           13.1850         7.657718                    0.0   \n",
      "2           14.3550        16.905103                    0.0   \n",
      "3           11.1450        14.892795                    0.0   \n",
      "4           15.1125        15.711378                    0.0   \n",
      "\n",
      "   ('paramId_0', 'amax')  ('paramId_0', 'mean')  ('paramId_0', 'var')  \\\n",
      "0                   59.0                  1.475             87.025000   \n",
      "1                   53.0                  3.200            144.420513   \n",
      "2                   46.0                  2.400             86.605128   \n",
      "3                    0.0                  0.000              0.000000   \n",
      "4                   64.0                 10.550            408.920513   \n",
      "\n",
      "   ('pres', 'count')  ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
      "0                  1                             0                      1  \n",
      "1                  3                             0                      1  \n",
      "2                  3                             0                      1  \n",
      "3                  0                             0                      2  \n",
      "4                 10                             0                      2  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>('cwat', 'amin')</th>\n",
       "      <th>('cwat', 'amax')</th>\n",
       "      <th>('cwat', 'mean')</th>\n",
       "      <th>('cwat', 'var')</th>\n",
       "      <th>('r', 'amin')</th>\n",
       "      <th>('r', 'amax')</th>\n",
       "      <th>('r', 'mean')</th>\n",
       "      <th>('r', 'var')</th>\n",
       "      <th>('tozne', 'amin')</th>\n",
       "      <th>('tozne', 'amax')</th>\n",
       "      <th>...</th>\n",
       "      <th>('pwat', 'amax')</th>\n",
       "      <th>('pwat', 'mean')</th>\n",
       "      <th>('pwat', 'var')</th>\n",
       "      <th>('paramId_0', 'amin')</th>\n",
       "      <th>('paramId_0', 'amax')</th>\n",
       "      <th>('paramId_0', 'mean')</th>\n",
       "      <th>('paramId_0', 'var')</th>\n",
       "      <th>('pres', 'count')</th>\n",
       "      <th>('macro_season', '&lt;lambda&gt;')</th>\n",
       "      <th>('month', '&lt;lambda&gt;')</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.037798</td>\n",
       "      <td>-0.130169</td>\n",
       "      <td>0.796976</td>\n",
       "      <td>-0.266073</td>\n",
       "      <td>-0.160140</td>\n",
       "      <td>1.344242</td>\n",
       "      <td>0.511000</td>\n",
       "      <td>1.780008</td>\n",
       "      <td>-0.803542</td>\n",
       "      <td>-0.952548</td>\n",
       "      <td>...</td>\n",
       "      <td>0.823280</td>\n",
       "      <td>0.237088</td>\n",
       "      <td>1.077189</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.929794</td>\n",
       "      <td>0.172849</td>\n",
       "      <td>0.593843</td>\n",
       "      <td>-0.036491</td>\n",
       "      <td>-1.038844</td>\n",
       "      <td>-1.687242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.037798</td>\n",
       "      <td>-0.516509</td>\n",
       "      <td>0.311886</td>\n",
       "      <td>-0.363751</td>\n",
       "      <td>0.363029</td>\n",
       "      <td>0.151279</td>\n",
       "      <td>0.802405</td>\n",
       "      <td>-0.296109</td>\n",
       "      <td>-0.478509</td>\n",
       "      <td>0.506607</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.397888</td>\n",
       "      <td>0.144268</td>\n",
       "      <td>-0.633115</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.658480</td>\n",
       "      <td>0.950436</td>\n",
       "      <td>1.295513</td>\n",
       "      <td>0.927411</td>\n",
       "      <td>-1.038844</td>\n",
       "      <td>-1.687242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.037798</td>\n",
       "      <td>-0.110852</td>\n",
       "      <td>0.098446</td>\n",
       "      <td>-0.312252</td>\n",
       "      <td>1.147784</td>\n",
       "      <td>1.986607</td>\n",
       "      <td>1.421642</td>\n",
       "      <td>0.693729</td>\n",
       "      <td>-1.671783</td>\n",
       "      <td>-0.157596</td>\n",
       "      <td>...</td>\n",
       "      <td>0.116918</td>\n",
       "      <td>0.368184</td>\n",
       "      <td>-0.106313</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.341947</td>\n",
       "      <td>0.589816</td>\n",
       "      <td>0.588710</td>\n",
       "      <td>0.927411</td>\n",
       "      <td>-1.038844</td>\n",
       "      <td>-1.687242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.037798</td>\n",
       "      <td>-0.323339</td>\n",
       "      <td>-0.250819</td>\n",
       "      <td>-0.368410</td>\n",
       "      <td>-0.421725</td>\n",
       "      <td>-0.399320</td>\n",
       "      <td>-0.193231</td>\n",
       "      <td>-0.317667</td>\n",
       "      <td>-0.763470</td>\n",
       "      <td>0.190195</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.182388</td>\n",
       "      <td>-0.246149</td>\n",
       "      <td>-0.220950</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.738129</td>\n",
       "      <td>-0.492044</td>\n",
       "      <td>-0.470053</td>\n",
       "      <td>-0.518442</td>\n",
       "      <td>-1.038844</td>\n",
       "      <td>-1.383170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.037798</td>\n",
       "      <td>0.391390</td>\n",
       "      <td>0.975489</td>\n",
       "      <td>0.073763</td>\n",
       "      <td>2.455707</td>\n",
       "      <td>1.527775</td>\n",
       "      <td>1.421642</td>\n",
       "      <td>-0.197515</td>\n",
       "      <td>-0.126759</td>\n",
       "      <td>1.740874</td>\n",
       "      <td>...</td>\n",
       "      <td>0.763419</td>\n",
       "      <td>0.513155</td>\n",
       "      <td>-0.174317</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.155890</td>\n",
       "      <td>4.263632</td>\n",
       "      <td>4.529070</td>\n",
       "      <td>4.301069</td>\n",
       "      <td>-1.038844</td>\n",
       "      <td>-1.383170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
       "0         -0.037798         -0.130169          0.796976        -0.266073   \n",
       "1         -0.037798         -0.516509          0.311886        -0.363751   \n",
       "2         -0.037798         -0.110852          0.098446        -0.312252   \n",
       "3         -0.037798         -0.323339         -0.250819        -0.368410   \n",
       "4         -0.037798          0.391390          0.975489         0.073763   \n",
       "\n",
       "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
       "0      -0.160140       1.344242       0.511000      1.780008   \n",
       "1       0.363029       0.151279       0.802405     -0.296109   \n",
       "2       1.147784       1.986607       1.421642      0.693729   \n",
       "3      -0.421725      -0.399320      -0.193231     -0.317667   \n",
       "4       2.455707       1.527775       1.421642     -0.197515   \n",
       "\n",
       "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('pwat', 'amax')  \\\n",
       "0          -0.803542          -0.952548  ...          0.823280   \n",
       "1          -0.478509           0.506607  ...         -0.397888   \n",
       "2          -1.671783          -0.157596  ...          0.116918   \n",
       "3          -0.763470           0.190195  ...         -0.182388   \n",
       "4          -0.126759           1.740874  ...          0.763419   \n",
       "\n",
       "   ('pwat', 'mean')  ('pwat', 'var')  ('paramId_0', 'amin')  \\\n",
       "0          0.237088         1.077189                    0.0   \n",
       "1          0.144268        -0.633115                    0.0   \n",
       "2          0.368184        -0.106313                    0.0   \n",
       "3         -0.246149        -0.220950                    0.0   \n",
       "4          0.513155        -0.174317                    0.0   \n",
       "\n",
       "   ('paramId_0', 'amax')  ('paramId_0', 'mean')  ('paramId_0', 'var')  \\\n",
       "0               1.929794               0.172849              0.593843   \n",
       "1               1.658480               0.950436              1.295513   \n",
       "2               1.341947               0.589816              0.588710   \n",
       "3              -0.738129              -0.492044             -0.470053   \n",
       "4               2.155890               4.263632              4.529070   \n",
       "\n",
       "   ('pres', 'count')  ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
       "0          -0.036491                     -1.038844              -1.687242  \n",
       "1           0.927411                     -1.038844              -1.687242  \n",
       "2           0.927411                     -1.038844              -1.687242  \n",
       "3          -0.518442                     -1.038844              -1.383170  \n",
       "4           4.301069                     -1.038844              -1.383170  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(feature_part_train_2_1.head())\n",
    "part_features_2_1_train = pd.DataFrame(scaler.fit_transform(feature_part_train_2_1))\n",
    "part_features_2_1_train.columns = feature_part_train_2_1.columns\n",
    "part_features_2_1_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
      "292               0.0              0.14           0.02900         0.002435   \n",
      "293               0.0              1.33           0.12625         0.043993   \n",
      "294               0.0              0.47           0.04800         0.011781   \n",
      "295               0.0              0.36           0.10225         0.007274   \n",
      "296               0.0              1.50           0.22900         0.187840   \n",
      "\n",
      "     ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
      "292            5.0           25.0         11.125     35.240385   \n",
      "293            8.0           53.0         25.250    174.756410   \n",
      "294            7.0           36.0         14.375     67.112179   \n",
      "295           15.0           43.0         25.250     29.730769   \n",
      "296           16.0           53.0         30.025    115.768590   \n",
      "\n",
      "     ('tozne', 'amin')  ('tozne', 'amax')  ...  ('pwat', 'amax')  \\\n",
      "292         246.800003         289.600006  ...         17.900000   \n",
      "293         243.399994         291.399994  ...         31.100000   \n",
      "294         257.000000         371.100006  ...         24.700001   \n",
      "295         278.200012         397.899994  ...         22.799999   \n",
      "296         265.700012         381.100006  ...         35.400002   \n",
      "\n",
      "     ('pwat', 'mean')  ('pwat', 'var')  ('paramId_0', 'amin')  \\\n",
      "292            8.0600        13.980923                    0.0   \n",
      "293           15.2250        54.741923                    0.0   \n",
      "294           10.2225        26.132044                    0.0   \n",
      "295           14.5500         7.674359                    0.0   \n",
      "296           18.6275        57.461017                    0.0   \n",
      "\n",
      "     ('paramId_0', 'amax')  ('paramId_0', 'mean')  ('paramId_0', 'var')  \\\n",
      "292                    0.0                   0.00              0.000000   \n",
      "293                    0.0                   0.00              0.000000   \n",
      "294                   23.0                   0.60             13.220513   \n",
      "295                   63.0                  10.15            389.976923   \n",
      "296                   11.0                   0.45              4.151282   \n",
      "\n",
      "     ('pres', 'count')  ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
      "292                  0                             0                      1  \n",
      "293                  0                             0                      1  \n",
      "294                  1                             0                      2  \n",
      "295                  9                             0                      2  \n",
      "296                  2                             0                      2  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>('cwat', 'amin')</th>\n",
       "      <th>('cwat', 'amax')</th>\n",
       "      <th>('cwat', 'mean')</th>\n",
       "      <th>('cwat', 'var')</th>\n",
       "      <th>('r', 'amin')</th>\n",
       "      <th>('r', 'amax')</th>\n",
       "      <th>('r', 'mean')</th>\n",
       "      <th>('r', 'var')</th>\n",
       "      <th>('tozne', 'amin')</th>\n",
       "      <th>('tozne', 'amax')</th>\n",
       "      <th>...</th>\n",
       "      <th>('pwat', 'amax')</th>\n",
       "      <th>('pwat', 'mean')</th>\n",
       "      <th>('pwat', 'var')</th>\n",
       "      <th>('paramId_0', 'amin')</th>\n",
       "      <th>('paramId_0', 'amax')</th>\n",
       "      <th>('paramId_0', 'mean')</th>\n",
       "      <th>('paramId_0', 'var')</th>\n",
       "      <th>('pres', 'count')</th>\n",
       "      <th>('macro_season', '&lt;lambda&gt;')</th>\n",
       "      <th>('month', '&lt;lambda&gt;')</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.037798</td>\n",
       "      <td>-0.671045</td>\n",
       "      <td>-0.355598</td>\n",
       "      <td>-0.425931</td>\n",
       "      <td>-0.944894</td>\n",
       "      <td>-0.766385</td>\n",
       "      <td>-1.188867</td>\n",
       "      <td>-0.308996</td>\n",
       "      <td>-1.547112</td>\n",
       "      <td>-1.336949</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.445777</td>\n",
       "      <td>-0.836560</td>\n",
       "      <td>-0.272897</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.738129</td>\n",
       "      <td>-0.492044</td>\n",
       "      <td>-0.470053</td>\n",
       "      <td>-0.518442</td>\n",
       "      <td>-1.038844</td>\n",
       "      <td>-1.687242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.037798</td>\n",
       "      <td>1.627678</td>\n",
       "      <td>1.154002</td>\n",
       "      <td>0.487925</td>\n",
       "      <td>-0.160140</td>\n",
       "      <td>1.803075</td>\n",
       "      <td>1.097858</td>\n",
       "      <td>2.967606</td>\n",
       "      <td>-1.698498</td>\n",
       "      <td>-1.289879</td>\n",
       "      <td>...</td>\n",
       "      <td>1.134558</td>\n",
       "      <td>0.534686</td>\n",
       "      <td>2.049161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.738129</td>\n",
       "      <td>-0.492044</td>\n",
       "      <td>-0.470053</td>\n",
       "      <td>-0.518442</td>\n",
       "      <td>-1.038844</td>\n",
       "      <td>-1.687242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.037798</td>\n",
       "      <td>-0.033584</td>\n",
       "      <td>-0.060664</td>\n",
       "      <td>-0.220423</td>\n",
       "      <td>-0.421725</td>\n",
       "      <td>0.243045</td>\n",
       "      <td>-0.662718</td>\n",
       "      <td>0.439529</td>\n",
       "      <td>-1.092956</td>\n",
       "      <td>0.794254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.368335</td>\n",
       "      <td>-0.422698</td>\n",
       "      <td>0.419324</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.301909</td>\n",
       "      <td>-0.221579</td>\n",
       "      <td>-0.308430</td>\n",
       "      <td>-0.036491</td>\n",
       "      <td>-1.038844</td>\n",
       "      <td>-1.383170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.037798</td>\n",
       "      <td>-0.246071</td>\n",
       "      <td>0.781453</td>\n",
       "      <td>-0.319514</td>\n",
       "      <td>1.670953</td>\n",
       "      <td>0.885410</td>\n",
       "      <td>1.097858</td>\n",
       "      <td>-0.438392</td>\n",
       "      <td>-0.149022</td>\n",
       "      <td>1.495067</td>\n",
       "      <td>...</td>\n",
       "      <td>0.140862</td>\n",
       "      <td>0.405504</td>\n",
       "      <td>-0.632167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.110671</td>\n",
       "      <td>4.083322</td>\n",
       "      <td>4.297482</td>\n",
       "      <td>3.819118</td>\n",
       "      <td>-1.038844</td>\n",
       "      <td>-1.383170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.037798</td>\n",
       "      <td>1.956067</td>\n",
       "      <td>2.748979</td>\n",
       "      <td>3.651070</td>\n",
       "      <td>1.932538</td>\n",
       "      <td>1.803075</td>\n",
       "      <td>1.870893</td>\n",
       "      <td>1.582249</td>\n",
       "      <td>-0.705586</td>\n",
       "      <td>1.055752</td>\n",
       "      <td>...</td>\n",
       "      <td>1.649365</td>\n",
       "      <td>1.185860</td>\n",
       "      <td>2.204061</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.240719</td>\n",
       "      <td>-0.289195</td>\n",
       "      <td>-0.419302</td>\n",
       "      <td>0.445460</td>\n",
       "      <td>-1.038844</td>\n",
       "      <td>-1.383170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
       "0         -0.037798         -0.671045         -0.355598        -0.425931   \n",
       "1         -0.037798          1.627678          1.154002         0.487925   \n",
       "2         -0.037798         -0.033584         -0.060664        -0.220423   \n",
       "3         -0.037798         -0.246071          0.781453        -0.319514   \n",
       "4         -0.037798          1.956067          2.748979         3.651070   \n",
       "\n",
       "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
       "0      -0.944894      -0.766385      -1.188867     -0.308996   \n",
       "1      -0.160140       1.803075       1.097858      2.967606   \n",
       "2      -0.421725       0.243045      -0.662718      0.439529   \n",
       "3       1.670953       0.885410       1.097858     -0.438392   \n",
       "4       1.932538       1.803075       1.870893      1.582249   \n",
       "\n",
       "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('pwat', 'amax')  \\\n",
       "0          -1.547112          -1.336949  ...         -0.445777   \n",
       "1          -1.698498          -1.289879  ...          1.134558   \n",
       "2          -1.092956           0.794254  ...          0.368335   \n",
       "3          -0.149022           1.495067  ...          0.140862   \n",
       "4          -0.705586           1.055752  ...          1.649365   \n",
       "\n",
       "   ('pwat', 'mean')  ('pwat', 'var')  ('paramId_0', 'amin')  \\\n",
       "0         -0.836560        -0.272897                    0.0   \n",
       "1          0.534686         2.049161                    0.0   \n",
       "2         -0.422698         0.419324                    0.0   \n",
       "3          0.405504        -0.632167                    0.0   \n",
       "4          1.185860         2.204061                    0.0   \n",
       "\n",
       "   ('paramId_0', 'amax')  ('paramId_0', 'mean')  ('paramId_0', 'var')  \\\n",
       "0              -0.738129              -0.492044             -0.470053   \n",
       "1              -0.738129              -0.492044             -0.470053   \n",
       "2               0.301909              -0.221579             -0.308430   \n",
       "3               2.110671               4.083322              4.297482   \n",
       "4              -0.240719              -0.289195             -0.419302   \n",
       "\n",
       "   ('pres', 'count')  ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
       "0          -0.518442                     -1.038844              -1.687242  \n",
       "1          -0.518442                     -1.038844              -1.687242  \n",
       "2          -0.036491                     -1.038844              -1.383170  \n",
       "3           3.819118                     -1.038844              -1.383170  \n",
       "4           0.445460                     -1.038844              -1.383170  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tenday_no_null_test.head())\n",
    "tenday_part_features_test = pd.DataFrame(scaler.transform(tenday_no_null_test))\n",
    "tenday_part_features_test.columns = tenday_no_null_test.columns\n",
    "tenday_part_features_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.6719\n",
      "Test set accuracy: 0.6642\n",
      "Training Set Precision: 0.5055\n",
      "Training Set Recall: 0.7212\n",
      "Training Set F1 Score: 0.5944\n",
      "Test Set Precision: 0.2518\n",
      "Test Set Recall: 0.6982\n",
      "Test Set F1 Score: 0.3701\n"
     ]
    }
   ],
   "source": [
    "clf_log_10day_part = LogisticRegression(class_weight = \"balanced\")\n",
    "clf_log_10day_part.fit(part_features_2_1_train, target_train_2_1)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_log_10day_part.score(part_features_2_1_train, \n",
    "                                                       target_train_2_1), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_log_10day_part.score(tenday_part_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_log = clf_log_10day_part.predict(part_features_2_1_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(target_train_2_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(target_train_2_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(target_train_2_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_log = clf_log_10day_part.predict(tenday_part_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_log), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_log), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_log), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.7163\n",
      "Test set accuracy: 0.7998\n",
      "Training Set Precision: 0.6144\n",
      "Training Set Recall: 0.4003\n",
      "Training Set F1 Score: 0.4847\n",
      "Test Set Precision: 0.3304\n",
      "Test Set Recall: 0.4062\n",
      "Test Set F1 Score: 0.3644\n"
     ]
    }
   ],
   "source": [
    "clf_log_10day_part = LogisticRegression()\n",
    "clf_log_10day_part.fit(part_features_2_1_train, target_train_2_1)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_log_10day_part.score(part_features_2_1_train, \n",
    "                                                       target_train_2_1), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_log_10day_part.score(tenday_part_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_log = clf_log_10day_part.predict(part_features_2_1_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(target_train_2_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(target_train_2_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(target_train_2_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_log = clf_log_10day_part.predict(tenday_part_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_log), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_log), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_log), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.6785\n",
      "Test set accuracy: 0.6778\n",
      "Training Set Precision: 0.5132\n",
      "Training Set Recall: 0.6898\n",
      "Training Set F1 Score: 0.5885\n",
      "Test Set Precision: 0.2567\n",
      "Test Set Recall: 0.6751\n",
      "Test Set F1 Score: 0.3719\n"
     ]
    }
   ],
   "source": [
    "clf_sgd_10day_part = SGDClassifier(loss = \"log\", class_weight = \"balanced\", random_state = 0)\n",
    "clf_sgd_10day_part.fit(part_features_2_1_train, target_train_2_1)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_sgd_10day_part.score(part_features_2_1_train, \n",
    "                                                       target_train_2_1), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_sgd_10day_part.score(tenday_part_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_sgd = clf_sgd_10day_part.predict(part_features_2_1_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(target_train_2_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(target_train_2_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(target_train_2_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_sgd = clf_sgd_10day_part.predict(tenday_part_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_sgd), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_sgd), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_sgd), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.7138\n",
      "Test set accuracy: 0.7885\n",
      "Training Set Precision: 0.5951\n",
      "Training Set Recall: 0.4422\n",
      "Training Set F1 Score: 0.5074\n",
      "Test Set Precision: 0.3181\n",
      "Test Set Recall: 0.4341\n",
      "Test Set F1 Score: 0.3671\n"
     ]
    }
   ],
   "source": [
    "clf_sgd_10day_part = SGDClassifier(loss = \"log\", random_state = 0)\n",
    "clf_sgd_10day_part.fit(part_features_2_1_train, target_train_2_1)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_sgd_10day_part.score(part_features_2_1_train, \n",
    "                                                       target_train_2_1), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_sgd_10day_part.score(tenday_part_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_sgd = clf_sgd_10day_part.predict(part_features_2_1_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(target_train_2_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(target_train_2_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(target_train_2_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_sgd = clf_sgd_10day_part.predict(tenday_part_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_sgd), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_sgd), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_sgd), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3:1 Ratio No Fire:Fire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Full Parameter List "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    117335\n",
       "1     39111\n",
       "Name: fire, dtype: int64"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "over_sampler_3_1 = RandomOverSampler(sampling_strategy = 0.3333333333333, random_state = 0)\n",
    "feature_full_train_3_1, target_train_3_1 = over_sampler_3_1.fit_resample(tenday_full_train, tenday_target_train)\n",
    "target_train_3_1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 156446 entries, 0 to 156445\n",
      "Data columns (total 31 columns):\n",
      " #   Column                        Non-Null Count   Dtype  \n",
      "---  ------                        --------------   -----  \n",
      " 0   ('cwat', 'amin')              156446 non-null  float64\n",
      " 1   ('cwat', 'amax')              156446 non-null  float64\n",
      " 2   ('cwat', 'mean')              156446 non-null  float64\n",
      " 3   ('cwat', 'var')               156446 non-null  float64\n",
      " 4   ('r', 'amin')                 156446 non-null  float64\n",
      " 5   ('r', 'amax')                 156446 non-null  float64\n",
      " 6   ('r', 'mean')                 156446 non-null  float64\n",
      " 7   ('r', 'var')                  156446 non-null  float64\n",
      " 8   ('tozne', 'amin')             156446 non-null  float64\n",
      " 9   ('tozne', 'amax')             156446 non-null  float64\n",
      " 10  ('tozne', 'mean')             156446 non-null  float64\n",
      " 11  ('tozne', 'var')              156446 non-null  float64\n",
      " 12  ('gh', 'amin')                156446 non-null  float64\n",
      " 13  ('gh', 'amax')                156446 non-null  float64\n",
      " 14  ('gh', 'mean')                156446 non-null  float64\n",
      " 15  ('gh', 'var')                 156446 non-null  float64\n",
      " 16  ('pwat', 'amin')              156446 non-null  float64\n",
      " 17  ('pwat', 'amax')              156446 non-null  float64\n",
      " 18  ('pwat', 'mean')              156446 non-null  float64\n",
      " 19  ('pwat', 'var')               156446 non-null  float64\n",
      " 20  ('paramId_0', 'amin')         156446 non-null  float64\n",
      " 21  ('paramId_0', 'amax')         156446 non-null  float64\n",
      " 22  ('paramId_0', 'mean')         156446 non-null  float64\n",
      " 23  ('paramId_0', 'var')          156446 non-null  float64\n",
      " 24  ('pres', 'amin')              59954 non-null   float64\n",
      " 25  ('pres', 'amax')              59954 non-null   float64\n",
      " 26  ('pres', 'mean')              59954 non-null   float64\n",
      " 27  ('pres', 'var')               37431 non-null   float64\n",
      " 28  ('pres', 'count')             156446 non-null  int64  \n",
      " 29  ('macro_season', '<lambda>')  156446 non-null  int64  \n",
      " 30  ('month', '<lambda>')         156446 non-null  int64  \n",
      "dtypes: float64(28), int64(3)\n",
      "memory usage: 37.0 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 156446 entries, 0 to 156445\n",
      "Data columns (total 31 columns):\n",
      " #   Column                        Non-Null Count   Dtype  \n",
      "---  ------                        --------------   -----  \n",
      " 0   ('cwat', 'amin')              156446 non-null  float64\n",
      " 1   ('cwat', 'amax')              156446 non-null  float64\n",
      " 2   ('cwat', 'mean')              156446 non-null  float64\n",
      " 3   ('cwat', 'var')               156446 non-null  float64\n",
      " 4   ('r', 'amin')                 156446 non-null  float64\n",
      " 5   ('r', 'amax')                 156446 non-null  float64\n",
      " 6   ('r', 'mean')                 156446 non-null  float64\n",
      " 7   ('r', 'var')                  156446 non-null  float64\n",
      " 8   ('tozne', 'amin')             156446 non-null  float64\n",
      " 9   ('tozne', 'amax')             156446 non-null  float64\n",
      " 10  ('tozne', 'mean')             156446 non-null  float64\n",
      " 11  ('tozne', 'var')              156446 non-null  float64\n",
      " 12  ('gh', 'amin')                156446 non-null  float64\n",
      " 13  ('gh', 'amax')                156446 non-null  float64\n",
      " 14  ('gh', 'mean')                156446 non-null  float64\n",
      " 15  ('gh', 'var')                 156446 non-null  float64\n",
      " 16  ('pwat', 'amin')              156446 non-null  float64\n",
      " 17  ('pwat', 'amax')              156446 non-null  float64\n",
      " 18  ('pwat', 'mean')              156446 non-null  float64\n",
      " 19  ('pwat', 'var')               156446 non-null  float64\n",
      " 20  ('paramId_0', 'amin')         156446 non-null  float64\n",
      " 21  ('paramId_0', 'amax')         156446 non-null  float64\n",
      " 22  ('paramId_0', 'mean')         156446 non-null  float64\n",
      " 23  ('paramId_0', 'var')          156446 non-null  float64\n",
      " 24  ('pres', 'amin')              156446 non-null  float64\n",
      " 25  ('pres', 'amax')              156446 non-null  float64\n",
      " 26  ('pres', 'mean')              156446 non-null  float64\n",
      " 27  ('pres', 'var')               156446 non-null  float64\n",
      " 28  ('pres', 'count')             156446 non-null  float64\n",
      " 29  ('macro_season', '<lambda>')  156446 non-null  float64\n",
      " 30  ('month', '<lambda>')         156446 non-null  float64\n",
      "dtypes: float64(31)\n",
      "memory usage: 37.0 MB\n"
     ]
    }
   ],
   "source": [
    "feature_full_train_3_1.info()\n",
    "feature_imp_train_3_1 = pd.DataFrame(imp.fit_transform(feature_full_train_3_1))\n",
    "feature_imp_train_3_1.columns = feature_full_train_3_1.columns\n",
    "feature_imp_train_3_1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 35259 entries, 292 to 176294\n",
      "Data columns (total 31 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   ('cwat', 'amin')              35259 non-null  float64\n",
      " 1   ('cwat', 'amax')              35259 non-null  float64\n",
      " 2   ('cwat', 'mean')              35259 non-null  float64\n",
      " 3   ('cwat', 'var')               35259 non-null  float64\n",
      " 4   ('r', 'amin')                 35259 non-null  float64\n",
      " 5   ('r', 'amax')                 35259 non-null  float64\n",
      " 6   ('r', 'mean')                 35259 non-null  float64\n",
      " 7   ('r', 'var')                  35259 non-null  float64\n",
      " 8   ('tozne', 'amin')             35259 non-null  float64\n",
      " 9   ('tozne', 'amax')             35259 non-null  float64\n",
      " 10  ('tozne', 'mean')             35259 non-null  float64\n",
      " 11  ('tozne', 'var')              35259 non-null  float64\n",
      " 12  ('gh', 'amin')                35259 non-null  float64\n",
      " 13  ('gh', 'amax')                35259 non-null  float64\n",
      " 14  ('gh', 'mean')                35259 non-null  float64\n",
      " 15  ('gh', 'var')                 35259 non-null  float64\n",
      " 16  ('pwat', 'amin')              35259 non-null  float64\n",
      " 17  ('pwat', 'amax')              35259 non-null  float64\n",
      " 18  ('pwat', 'mean')              35259 non-null  float64\n",
      " 19  ('pwat', 'var')               35259 non-null  float64\n",
      " 20  ('paramId_0', 'amin')         35259 non-null  float64\n",
      " 21  ('paramId_0', 'amax')         35259 non-null  float64\n",
      " 22  ('paramId_0', 'mean')         35259 non-null  float64\n",
      " 23  ('paramId_0', 'var')          35259 non-null  float64\n",
      " 24  ('pres', 'amin')              15069 non-null  float64\n",
      " 25  ('pres', 'amax')              15069 non-null  float64\n",
      " 26  ('pres', 'mean')              15069 non-null  float64\n",
      " 27  ('pres', 'var')               9461 non-null   float64\n",
      " 28  ('pres', 'count')             35259 non-null  int64  \n",
      " 29  ('macro_season', '<lambda>')  35259 non-null  int64  \n",
      " 30  ('month', '<lambda>')         35259 non-null  int64  \n",
      "dtypes: float64(28), int64(3)\n",
      "memory usage: 8.6 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 35259 entries, 0 to 35258\n",
      "Data columns (total 31 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   ('cwat', 'amin')              35259 non-null  float64\n",
      " 1   ('cwat', 'amax')              35259 non-null  float64\n",
      " 2   ('cwat', 'mean')              35259 non-null  float64\n",
      " 3   ('cwat', 'var')               35259 non-null  float64\n",
      " 4   ('r', 'amin')                 35259 non-null  float64\n",
      " 5   ('r', 'amax')                 35259 non-null  float64\n",
      " 6   ('r', 'mean')                 35259 non-null  float64\n",
      " 7   ('r', 'var')                  35259 non-null  float64\n",
      " 8   ('tozne', 'amin')             35259 non-null  float64\n",
      " 9   ('tozne', 'amax')             35259 non-null  float64\n",
      " 10  ('tozne', 'mean')             35259 non-null  float64\n",
      " 11  ('tozne', 'var')              35259 non-null  float64\n",
      " 12  ('gh', 'amin')                35259 non-null  float64\n",
      " 13  ('gh', 'amax')                35259 non-null  float64\n",
      " 14  ('gh', 'mean')                35259 non-null  float64\n",
      " 15  ('gh', 'var')                 35259 non-null  float64\n",
      " 16  ('pwat', 'amin')              35259 non-null  float64\n",
      " 17  ('pwat', 'amax')              35259 non-null  float64\n",
      " 18  ('pwat', 'mean')              35259 non-null  float64\n",
      " 19  ('pwat', 'var')               35259 non-null  float64\n",
      " 20  ('paramId_0', 'amin')         35259 non-null  float64\n",
      " 21  ('paramId_0', 'amax')         35259 non-null  float64\n",
      " 22  ('paramId_0', 'mean')         35259 non-null  float64\n",
      " 23  ('paramId_0', 'var')          35259 non-null  float64\n",
      " 24  ('pres', 'amin')              35259 non-null  float64\n",
      " 25  ('pres', 'amax')              35259 non-null  float64\n",
      " 26  ('pres', 'mean')              35259 non-null  float64\n",
      " 27  ('pres', 'var')               35259 non-null  float64\n",
      " 28  ('pres', 'count')             35259 non-null  float64\n",
      " 29  ('macro_season', '<lambda>')  35259 non-null  float64\n",
      " 30  ('month', '<lambda>')         35259 non-null  float64\n",
      "dtypes: float64(31)\n",
      "memory usage: 8.3 MB\n"
     ]
    }
   ],
   "source": [
    "tenday_full_test.info()\n",
    "tenday_imp_test = pd.DataFrame(imp.fit_transform(tenday_full_test))\n",
    "tenday_imp_test.columns = tenday_full_test.columns\n",
    "tenday_imp_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
      "0               0.0              0.42           0.10325         0.009705   \n",
      "1               0.0              0.22           0.07200         0.005263   \n",
      "2               0.0              0.43           0.05825         0.007605   \n",
      "3               0.0              0.32           0.03575         0.005051   \n",
      "4               0.0              0.69           0.11475         0.025159   \n",
      "\n",
      "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
      "0            8.0           48.0         21.625    124.189103   \n",
      "1           10.0           35.0         23.425     35.789103   \n",
      "2           13.0           55.0         27.250     77.935897   \n",
      "3            7.0           29.0         17.275     34.871154   \n",
      "4           18.0           50.0         27.250     39.987179   \n",
      "\n",
      "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('paramId_0', 'amax')  \\\n",
      "0         263.500000         304.299988  ...                   59.0   \n",
      "1         270.799988         360.100006  ...                   53.0   \n",
      "2         244.000000         334.700012  ...                   46.0   \n",
      "3         264.399994         348.000000  ...                    0.0   \n",
      "4         278.700012         407.299988  ...                   64.0   \n",
      "\n",
      "   ('paramId_0', 'mean')  ('paramId_0', 'var')  ('pres', 'amin')  \\\n",
      "0                  1.475             87.025000           34430.0   \n",
      "1                  3.200            144.420513           61430.0   \n",
      "2                  2.400             86.605128           65030.0   \n",
      "3                  0.000              0.000000           42750.0   \n",
      "4                 10.550            408.920513           41470.0   \n",
      "\n",
      "   ('pres', 'amax')  ('pres', 'mean')  ('pres', 'var')  ('pres', 'count')  \\\n",
      "0           34430.0      34430.000000     4.782063e+07                1.0   \n",
      "1           62960.0      62443.333333     7.702333e+05                3.0   \n",
      "2           75960.0      71433.333333     3.250763e+07                3.0   \n",
      "3           55550.0      50180.000000     4.782063e+07                0.0   \n",
      "4           76180.0      57815.000000     2.113109e+08               10.0   \n",
      "\n",
      "   ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
      "0                           0.0                    1.0  \n",
      "1                           0.0                    1.0  \n",
      "2                           0.0                    1.0  \n",
      "3                           0.0                    2.0  \n",
      "4                           0.0                    2.0  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>('cwat', 'amin')</th>\n",
       "      <th>('cwat', 'amax')</th>\n",
       "      <th>('cwat', 'mean')</th>\n",
       "      <th>('cwat', 'var')</th>\n",
       "      <th>('r', 'amin')</th>\n",
       "      <th>('r', 'amax')</th>\n",
       "      <th>('r', 'mean')</th>\n",
       "      <th>('r', 'var')</th>\n",
       "      <th>('tozne', 'amin')</th>\n",
       "      <th>('tozne', 'amax')</th>\n",
       "      <th>...</th>\n",
       "      <th>('paramId_0', 'amax')</th>\n",
       "      <th>('paramId_0', 'mean')</th>\n",
       "      <th>('paramId_0', 'var')</th>\n",
       "      <th>('pres', 'amin')</th>\n",
       "      <th>('pres', 'amax')</th>\n",
       "      <th>('pres', 'mean')</th>\n",
       "      <th>('pres', 'var')</th>\n",
       "      <th>('pres', 'count')</th>\n",
       "      <th>('macro_season', '&lt;lambda&gt;')</th>\n",
       "      <th>('month', '&lt;lambda&gt;')</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.039729</td>\n",
       "      <td>-0.156283</td>\n",
       "      <td>0.745399</td>\n",
       "      <td>-0.281014</td>\n",
       "      <td>-0.186225</td>\n",
       "      <td>1.302854</td>\n",
       "      <td>0.472826</td>\n",
       "      <td>1.737119</td>\n",
       "      <td>-0.774583</td>\n",
       "      <td>-0.963086</td>\n",
       "      <td>...</td>\n",
       "      <td>1.882702</td>\n",
       "      <td>0.148752</td>\n",
       "      <td>0.561564</td>\n",
       "      <td>-1.115741</td>\n",
       "      <td>-2.263753</td>\n",
       "      <td>-1.969621</td>\n",
       "      <td>-0.154830</td>\n",
       "      <td>-0.058270</td>\n",
       "      <td>-1.016942</td>\n",
       "      <td>-1.629429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.039729</td>\n",
       "      <td>-0.539001</td>\n",
       "      <td>0.268950</td>\n",
       "      <td>-0.377088</td>\n",
       "      <td>0.330998</td>\n",
       "      <td>0.119567</td>\n",
       "      <td>0.761115</td>\n",
       "      <td>-0.313967</td>\n",
       "      <td>-0.453977</td>\n",
       "      <td>0.479614</td>\n",
       "      <td>...</td>\n",
       "      <td>1.614499</td>\n",
       "      <td>0.909508</td>\n",
       "      <td>1.249308</td>\n",
       "      <td>2.282624</td>\n",
       "      <td>0.879803</td>\n",
       "      <td>1.616485</td>\n",
       "      <td>-1.068240</td>\n",
       "      <td>0.883372</td>\n",
       "      <td>-1.016942</td>\n",
       "      <td>-1.629429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.039729</td>\n",
       "      <td>-0.137147</td>\n",
       "      <td>0.059312</td>\n",
       "      <td>-0.326434</td>\n",
       "      <td>1.106831</td>\n",
       "      <td>1.940009</td>\n",
       "      <td>1.373728</td>\n",
       "      <td>0.663936</td>\n",
       "      <td>-1.630997</td>\n",
       "      <td>-0.177098</td>\n",
       "      <td>...</td>\n",
       "      <td>1.301596</td>\n",
       "      <td>0.556694</td>\n",
       "      <td>0.556533</td>\n",
       "      <td>2.735739</td>\n",
       "      <td>2.312197</td>\n",
       "      <td>2.767333</td>\n",
       "      <td>-0.452108</td>\n",
       "      <td>0.883372</td>\n",
       "      <td>-1.016942</td>\n",
       "      <td>-1.629429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.039729</td>\n",
       "      <td>-0.347642</td>\n",
       "      <td>-0.283732</td>\n",
       "      <td>-0.381670</td>\n",
       "      <td>-0.444836</td>\n",
       "      <td>-0.426566</td>\n",
       "      <td>-0.223871</td>\n",
       "      <td>-0.335266</td>\n",
       "      <td>-0.735056</td>\n",
       "      <td>0.166770</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.754625</td>\n",
       "      <td>-0.501749</td>\n",
       "      <td>-0.481218</td>\n",
       "      <td>-0.068541</td>\n",
       "      <td>0.063338</td>\n",
       "      <td>0.046604</td>\n",
       "      <td>-0.154830</td>\n",
       "      <td>-0.529090</td>\n",
       "      <td>-1.016942</td>\n",
       "      <td>-1.332920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.039729</td>\n",
       "      <td>0.360387</td>\n",
       "      <td>0.920732</td>\n",
       "      <td>0.053239</td>\n",
       "      <td>2.399887</td>\n",
       "      <td>1.484898</td>\n",
       "      <td>1.373728</td>\n",
       "      <td>-0.216562</td>\n",
       "      <td>-0.107018</td>\n",
       "      <td>1.699961</td>\n",
       "      <td>...</td>\n",
       "      <td>2.106204</td>\n",
       "      <td>4.150989</td>\n",
       "      <td>4.418693</td>\n",
       "      <td>-0.229649</td>\n",
       "      <td>2.336438</td>\n",
       "      <td>1.023992</td>\n",
       "      <td>3.019077</td>\n",
       "      <td>4.179118</td>\n",
       "      <td>-1.016942</td>\n",
       "      <td>-1.332920</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
       "0         -0.039729         -0.156283          0.745399        -0.281014   \n",
       "1         -0.039729         -0.539001          0.268950        -0.377088   \n",
       "2         -0.039729         -0.137147          0.059312        -0.326434   \n",
       "3         -0.039729         -0.347642         -0.283732        -0.381670   \n",
       "4         -0.039729          0.360387          0.920732         0.053239   \n",
       "\n",
       "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
       "0      -0.186225       1.302854       0.472826      1.737119   \n",
       "1       0.330998       0.119567       0.761115     -0.313967   \n",
       "2       1.106831       1.940009       1.373728      0.663936   \n",
       "3      -0.444836      -0.426566      -0.223871     -0.335266   \n",
       "4       2.399887       1.484898       1.373728     -0.216562   \n",
       "\n",
       "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('paramId_0', 'amax')  \\\n",
       "0          -0.774583          -0.963086  ...               1.882702   \n",
       "1          -0.453977           0.479614  ...               1.614499   \n",
       "2          -1.630997          -0.177098  ...               1.301596   \n",
       "3          -0.735056           0.166770  ...              -0.754625   \n",
       "4          -0.107018           1.699961  ...               2.106204   \n",
       "\n",
       "   ('paramId_0', 'mean')  ('paramId_0', 'var')  ('pres', 'amin')  \\\n",
       "0               0.148752              0.561564         -1.115741   \n",
       "1               0.909508              1.249308          2.282624   \n",
       "2               0.556694              0.556533          2.735739   \n",
       "3              -0.501749             -0.481218         -0.068541   \n",
       "4               4.150989              4.418693         -0.229649   \n",
       "\n",
       "   ('pres', 'amax')  ('pres', 'mean')  ('pres', 'var')  ('pres', 'count')  \\\n",
       "0         -2.263753         -1.969621        -0.154830          -0.058270   \n",
       "1          0.879803          1.616485        -1.068240           0.883372   \n",
       "2          2.312197          2.767333        -0.452108           0.883372   \n",
       "3          0.063338          0.046604        -0.154830          -0.529090   \n",
       "4          2.336438          1.023992         3.019077           4.179118   \n",
       "\n",
       "   ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
       "0                     -1.016942              -1.629429  \n",
       "1                     -1.016942              -1.629429  \n",
       "2                     -1.016942              -1.629429  \n",
       "3                     -1.016942              -1.332920  \n",
       "4                     -1.016942              -1.332920  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(feature_imp_train_3_1.head())\n",
    "full_features_3_1_train = pd.DataFrame(scaler.fit_transform(feature_imp_train_3_1))\n",
    "full_features_3_1_train.columns = feature_imp_train_3_1.columns\n",
    "full_features_3_1_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>('cwat', 'amin')</th>\n",
       "      <th>('cwat', 'amax')</th>\n",
       "      <th>('cwat', 'mean')</th>\n",
       "      <th>('cwat', 'var')</th>\n",
       "      <th>('r', 'amin')</th>\n",
       "      <th>('r', 'amax')</th>\n",
       "      <th>('r', 'mean')</th>\n",
       "      <th>('r', 'var')</th>\n",
       "      <th>('tozne', 'amin')</th>\n",
       "      <th>('tozne', 'amax')</th>\n",
       "      <th>...</th>\n",
       "      <th>('paramId_0', 'amax')</th>\n",
       "      <th>('paramId_0', 'mean')</th>\n",
       "      <th>('paramId_0', 'var')</th>\n",
       "      <th>('pres', 'amin')</th>\n",
       "      <th>('pres', 'amax')</th>\n",
       "      <th>('pres', 'mean')</th>\n",
       "      <th>('pres', 'var')</th>\n",
       "      <th>('pres', 'count')</th>\n",
       "      <th>('macro_season', '&lt;lambda&gt;')</th>\n",
       "      <th>('month', '&lt;lambda&gt;')</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.039729</td>\n",
       "      <td>-0.692089</td>\n",
       "      <td>-0.386645</td>\n",
       "      <td>-0.438246</td>\n",
       "      <td>-0.962058</td>\n",
       "      <td>-0.790654</td>\n",
       "      <td>-1.208857</td>\n",
       "      <td>-0.326699</td>\n",
       "      <td>-1.508024</td>\n",
       "      <td>-1.343151</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.754625</td>\n",
       "      <td>-0.501749</td>\n",
       "      <td>-0.481218</td>\n",
       "      <td>-0.072317</td>\n",
       "      <td>0.025875</td>\n",
       "      <td>0.008199</td>\n",
       "      <td>-0.087753</td>\n",
       "      <td>-0.529090</td>\n",
       "      <td>-1.016942</td>\n",
       "      <td>-1.629429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.039729</td>\n",
       "      <td>1.585086</td>\n",
       "      <td>1.096066</td>\n",
       "      <td>0.460597</td>\n",
       "      <td>-0.186225</td>\n",
       "      <td>1.757965</td>\n",
       "      <td>1.053407</td>\n",
       "      <td>2.910398</td>\n",
       "      <td>-1.657348</td>\n",
       "      <td>-1.296613</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.754625</td>\n",
       "      <td>-0.501749</td>\n",
       "      <td>-0.481218</td>\n",
       "      <td>-0.072317</td>\n",
       "      <td>0.025875</td>\n",
       "      <td>0.008199</td>\n",
       "      <td>-0.087753</td>\n",
       "      <td>-0.529090</td>\n",
       "      <td>-1.016942</td>\n",
       "      <td>-1.629429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.039729</td>\n",
       "      <td>-0.060603</td>\n",
       "      <td>-0.096963</td>\n",
       "      <td>-0.236115</td>\n",
       "      <td>-0.444836</td>\n",
       "      <td>0.210589</td>\n",
       "      <td>-0.688336</td>\n",
       "      <td>0.412801</td>\n",
       "      <td>-1.060054</td>\n",
       "      <td>0.764017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.273485</td>\n",
       "      <td>-0.237138</td>\n",
       "      <td>-0.322802</td>\n",
       "      <td>4.237313</td>\n",
       "      <td>2.422382</td>\n",
       "      <td>3.474825</td>\n",
       "      <td>-0.087753</td>\n",
       "      <td>-0.058270</td>\n",
       "      <td>-1.016942</td>\n",
       "      <td>-1.332920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.039729</td>\n",
       "      <td>-0.271098</td>\n",
       "      <td>0.730153</td>\n",
       "      <td>-0.333577</td>\n",
       "      <td>1.624054</td>\n",
       "      <td>0.847744</td>\n",
       "      <td>1.053407</td>\n",
       "      <td>-0.454535</td>\n",
       "      <td>-0.128978</td>\n",
       "      <td>1.456926</td>\n",
       "      <td>...</td>\n",
       "      <td>2.061503</td>\n",
       "      <td>3.974581</td>\n",
       "      <td>4.191701</td>\n",
       "      <td>-0.224614</td>\n",
       "      <td>2.518242</td>\n",
       "      <td>1.304272</td>\n",
       "      <td>3.557523</td>\n",
       "      <td>3.708297</td>\n",
       "      <td>-1.016942</td>\n",
       "      <td>-1.332920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.039729</td>\n",
       "      <td>1.910397</td>\n",
       "      <td>2.662631</td>\n",
       "      <td>3.571778</td>\n",
       "      <td>1.882665</td>\n",
       "      <td>1.757965</td>\n",
       "      <td>1.818172</td>\n",
       "      <td>1.541743</td>\n",
       "      <td>-0.677961</td>\n",
       "      <td>1.022565</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.262920</td>\n",
       "      <td>-0.303291</td>\n",
       "      <td>-0.431475</td>\n",
       "      <td>0.202070</td>\n",
       "      <td>1.602611</td>\n",
       "      <td>0.946544</td>\n",
       "      <td>4.800479</td>\n",
       "      <td>0.412551</td>\n",
       "      <td>-1.016942</td>\n",
       "      <td>-1.332920</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
       "0         -0.039729         -0.692089         -0.386645        -0.438246   \n",
       "1         -0.039729          1.585086          1.096066         0.460597   \n",
       "2         -0.039729         -0.060603         -0.096963        -0.236115   \n",
       "3         -0.039729         -0.271098          0.730153        -0.333577   \n",
       "4         -0.039729          1.910397          2.662631         3.571778   \n",
       "\n",
       "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
       "0      -0.962058      -0.790654      -1.208857     -0.326699   \n",
       "1      -0.186225       1.757965       1.053407      2.910398   \n",
       "2      -0.444836       0.210589      -0.688336      0.412801   \n",
       "3       1.624054       0.847744       1.053407     -0.454535   \n",
       "4       1.882665       1.757965       1.818172      1.541743   \n",
       "\n",
       "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('paramId_0', 'amax')  \\\n",
       "0          -1.508024          -1.343151  ...              -0.754625   \n",
       "1          -1.657348          -1.296613  ...              -0.754625   \n",
       "2          -1.060054           0.764017  ...               0.273485   \n",
       "3          -0.128978           1.456926  ...               2.061503   \n",
       "4          -0.677961           1.022565  ...              -0.262920   \n",
       "\n",
       "   ('paramId_0', 'mean')  ('paramId_0', 'var')  ('pres', 'amin')  \\\n",
       "0              -0.501749             -0.481218         -0.072317   \n",
       "1              -0.501749             -0.481218         -0.072317   \n",
       "2              -0.237138             -0.322802          4.237313   \n",
       "3               3.974581              4.191701         -0.224614   \n",
       "4              -0.303291             -0.431475          0.202070   \n",
       "\n",
       "   ('pres', 'amax')  ('pres', 'mean')  ('pres', 'var')  ('pres', 'count')  \\\n",
       "0          0.025875          0.008199        -0.087753          -0.529090   \n",
       "1          0.025875          0.008199        -0.087753          -0.529090   \n",
       "2          2.422382          3.474825        -0.087753          -0.058270   \n",
       "3          2.518242          1.304272         3.557523           3.708297   \n",
       "4          1.602611          0.946544         4.800479           0.412551   \n",
       "\n",
       "   ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
       "0                     -1.016942              -1.629429  \n",
       "1                     -1.016942              -1.629429  \n",
       "2                     -1.016942              -1.332920  \n",
       "3                     -1.016942              -1.332920  \n",
       "4                     -1.016942              -1.332920  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tenday_full_features_test = pd.DataFrame(scaler.transform(tenday_imp_test))\n",
    "tenday_full_features_test.columns = tenday_imp_test.columns\n",
    "tenday_full_features_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.6633\n",
      "Test set accuracy: 0.6627\n",
      "Training Set Precision: 0.4031\n",
      "Training Set Recall: 0.7214\n",
      "Training Set F1 Score: 0.5172\n",
      "Test Set Precision: 0.2521\n",
      "Test Set Recall: 0.7052\n",
      "Test Set F1 Score: 0.3714\n"
     ]
    }
   ],
   "source": [
    "clf_log_10day_full = LogisticRegression(class_weight = \"balanced\", max_iter = 100000)\n",
    "clf_log_10day_full.fit(full_features_3_1_train, target_train_3_1)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_log_10day_full.score(full_features_3_1_train, \n",
    "                                                       target_train_3_1), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_log_10day_full.score(tenday_full_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_log = clf_log_10day_full.predict(full_features_3_1_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(target_train_3_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(target_train_3_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(target_train_3_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_log = clf_log_10day_full.predict(tenday_full_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_log), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_log), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_log), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.7601\n",
      "Test set accuracy: 0.8411\n",
      "Training Set Precision: 0.5628\n",
      "Training Set Recall: 0.1809\n",
      "Training Set F1 Score: 0.2738\n",
      "Test Set Precision: 0.3923\n",
      "Test Set Recall: 0.2262\n",
      "Test Set F1 Score: 0.2869\n"
     ]
    }
   ],
   "source": [
    "clf_log_10day_full = LogisticRegression(max_iter = 100000)\n",
    "clf_log_10day_full.fit(full_features_3_1_train, target_train_3_1)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_log_10day_full.score(full_features_3_1_train, \n",
    "                                                       target_train_3_1), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_log_10day_full.score(tenday_full_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_log = clf_log_10day_full.predict(full_features_3_1_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(target_train_3_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(target_train_3_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(target_train_3_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_log = clf_log_10day_full.predict(tenday_full_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_log), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_log), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_log), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.6505\n",
      "Test set accuracy: 0.6482\n",
      "Training Set Precision: 0.3925\n",
      "Training Set Recall: 0.7264\n",
      "Training Set F1 Score: 0.5096\n",
      "Test Set Precision: 0.243\n",
      "Test Set Recall: 0.7038\n",
      "Test Set F1 Score: 0.3612\n"
     ]
    }
   ],
   "source": [
    "clf_sgd_10day_full = SGDClassifier(loss = \"log\", class_weight = \"balanced\", random_state = 0)\n",
    "clf_sgd_10day_full.fit(full_features_3_1_train, target_train_3_1)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_sgd_10day_full.score(full_features_3_1_train, \n",
    "                                                       target_train_3_1), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_sgd_10day_full.score(tenday_full_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_sgd = clf_sgd_10day_full.predict(full_features_3_1_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(target_train_3_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(target_train_3_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(target_train_3_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_sgd = clf_sgd_10day_full.predict(tenday_full_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_sgd), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_sgd), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_sgd), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.7593\n",
      "Test set accuracy: 0.8399\n",
      "Training Set Precision: 0.5579\n",
      "Training Set Recall: 0.1793\n",
      "Training Set F1 Score: 0.2714\n",
      "Test Set Precision: 0.3836\n",
      "Test Set Recall: 0.2185\n",
      "Test Set F1 Score: 0.2784\n"
     ]
    }
   ],
   "source": [
    "clf_sgd_10day_full = SGDClassifier(loss = \"log\", random_state = 0)\n",
    "clf_sgd_10day_full.fit(full_features_3_1_train, target_train_3_1)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_sgd_10day_full.score(full_features_3_1_train, \n",
    "                                                       target_train_3_1), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_sgd_10day_full.score(tenday_full_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_sgd = clf_sgd_10day_full.predict(full_features_3_1_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(target_train_3_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(target_train_3_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(target_train_3_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_sgd = clf_sgd_10day_full.predict(tenday_full_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_sgd), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_sgd), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_sgd), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Part Parameter List "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    117335\n",
       "1     39111\n",
       "Name: fire, dtype: int64"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_part_train_3_1, target_train_3_1 = over_sampler_3_1.fit_resample(tenday_no_null_train, tenday_target_train)\n",
    "target_train_3_1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
      "0               0.0              0.42           0.10325         0.009705   \n",
      "1               0.0              0.22           0.07200         0.005263   \n",
      "2               0.0              0.43           0.05825         0.007605   \n",
      "3               0.0              0.32           0.03575         0.005051   \n",
      "4               0.0              0.69           0.11475         0.025159   \n",
      "\n",
      "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
      "0            8.0           48.0         21.625    124.189103   \n",
      "1           10.0           35.0         23.425     35.789103   \n",
      "2           13.0           55.0         27.250     77.935897   \n",
      "3            7.0           29.0         17.275     34.871154   \n",
      "4           18.0           50.0         27.250     39.987179   \n",
      "\n",
      "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('pwat', 'amax')  \\\n",
      "0         263.500000         304.299988  ...         28.500000   \n",
      "1         270.799988         360.100006  ...         18.299999   \n",
      "2         244.000000         334.700012  ...         22.600000   \n",
      "3         264.399994         348.000000  ...         20.100000   \n",
      "4         278.700012         407.299988  ...         28.000000   \n",
      "\n",
      "   ('pwat', 'mean')  ('pwat', 'var')  ('paramId_0', 'amin')  \\\n",
      "0           13.6700        37.680100                    0.0   \n",
      "1           13.1850         7.657718                    0.0   \n",
      "2           14.3550        16.905103                    0.0   \n",
      "3           11.1450        14.892795                    0.0   \n",
      "4           15.1125        15.711378                    0.0   \n",
      "\n",
      "   ('paramId_0', 'amax')  ('paramId_0', 'mean')  ('paramId_0', 'var')  \\\n",
      "0                   59.0                  1.475             87.025000   \n",
      "1                   53.0                  3.200            144.420513   \n",
      "2                   46.0                  2.400             86.605128   \n",
      "3                    0.0                  0.000              0.000000   \n",
      "4                   64.0                 10.550            408.920513   \n",
      "\n",
      "   ('pres', 'count')  ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
      "0                  1                             0                      1  \n",
      "1                  3                             0                      1  \n",
      "2                  3                             0                      1  \n",
      "3                  0                             0                      2  \n",
      "4                 10                             0                      2  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>('cwat', 'amin')</th>\n",
       "      <th>('cwat', 'amax')</th>\n",
       "      <th>('cwat', 'mean')</th>\n",
       "      <th>('cwat', 'var')</th>\n",
       "      <th>('r', 'amin')</th>\n",
       "      <th>('r', 'amax')</th>\n",
       "      <th>('r', 'mean')</th>\n",
       "      <th>('r', 'var')</th>\n",
       "      <th>('tozne', 'amin')</th>\n",
       "      <th>('tozne', 'amax')</th>\n",
       "      <th>...</th>\n",
       "      <th>('pwat', 'amax')</th>\n",
       "      <th>('pwat', 'mean')</th>\n",
       "      <th>('pwat', 'var')</th>\n",
       "      <th>('paramId_0', 'amin')</th>\n",
       "      <th>('paramId_0', 'amax')</th>\n",
       "      <th>('paramId_0', 'mean')</th>\n",
       "      <th>('paramId_0', 'var')</th>\n",
       "      <th>('pres', 'count')</th>\n",
       "      <th>('macro_season', '&lt;lambda&gt;')</th>\n",
       "      <th>('month', '&lt;lambda&gt;')</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.039969</td>\n",
       "      <td>-0.154164</td>\n",
       "      <td>0.747304</td>\n",
       "      <td>-0.279928</td>\n",
       "      <td>-0.186099</td>\n",
       "      <td>1.304254</td>\n",
       "      <td>0.473136</td>\n",
       "      <td>1.739631</td>\n",
       "      <td>-0.776053</td>\n",
       "      <td>-0.963169</td>\n",
       "      <td>...</td>\n",
       "      <td>0.816511</td>\n",
       "      <td>0.240834</td>\n",
       "      <td>1.065975</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.884534</td>\n",
       "      <td>0.148744</td>\n",
       "      <td>0.561781</td>\n",
       "      <td>-0.058223</td>\n",
       "      <td>-1.016591</td>\n",
       "      <td>-1.630538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.039969</td>\n",
       "      <td>-0.537447</td>\n",
       "      <td>0.270558</td>\n",
       "      <td>-0.376214</td>\n",
       "      <td>0.330931</td>\n",
       "      <td>0.120764</td>\n",
       "      <td>0.761394</td>\n",
       "      <td>-0.312975</td>\n",
       "      <td>-0.455693</td>\n",
       "      <td>0.478855</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.389155</td>\n",
       "      <td>0.148937</td>\n",
       "      <td>-0.626555</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.616252</td>\n",
       "      <td>0.908710</td>\n",
       "      <td>1.249283</td>\n",
       "      <td>0.882420</td>\n",
       "      <td>-1.016591</td>\n",
       "      <td>-1.630538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.039969</td>\n",
       "      <td>-0.135000</td>\n",
       "      <td>0.060790</td>\n",
       "      <td>-0.325448</td>\n",
       "      <td>1.106477</td>\n",
       "      <td>1.941519</td>\n",
       "      <td>1.373942</td>\n",
       "      <td>0.665653</td>\n",
       "      <td>-1.631811</td>\n",
       "      <td>-0.177550</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119116</td>\n",
       "      <td>0.370626</td>\n",
       "      <td>-0.105228</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.303257</td>\n",
       "      <td>0.556262</td>\n",
       "      <td>0.556752</td>\n",
       "      <td>0.882420</td>\n",
       "      <td>-1.016591</td>\n",
       "      <td>-1.630538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.039969</td>\n",
       "      <td>-0.345805</td>\n",
       "      <td>-0.282468</td>\n",
       "      <td>-0.380806</td>\n",
       "      <td>-0.444615</td>\n",
       "      <td>-0.425462</td>\n",
       "      <td>-0.223487</td>\n",
       "      <td>-0.334289</td>\n",
       "      <td>-0.736557</td>\n",
       "      <td>0.166158</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.176390</td>\n",
       "      <td>-0.237596</td>\n",
       "      <td>-0.218673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.753567</td>\n",
       "      <td>-0.501082</td>\n",
       "      <td>-0.480632</td>\n",
       "      <td>-0.528544</td>\n",
       "      <td>-1.016591</td>\n",
       "      <td>-1.333825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.039969</td>\n",
       "      <td>0.363268</td>\n",
       "      <td>0.922747</td>\n",
       "      <td>0.055063</td>\n",
       "      <td>2.399054</td>\n",
       "      <td>1.486330</td>\n",
       "      <td>1.373942</td>\n",
       "      <td>-0.215498</td>\n",
       "      <td>-0.109000</td>\n",
       "      <td>1.698631</td>\n",
       "      <td>...</td>\n",
       "      <td>0.757409</td>\n",
       "      <td>0.514155</td>\n",
       "      <td>-0.172525</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.108102</td>\n",
       "      <td>4.146827</td>\n",
       "      <td>4.417549</td>\n",
       "      <td>4.174669</td>\n",
       "      <td>-1.016591</td>\n",
       "      <td>-1.333825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
       "0         -0.039969         -0.154164          0.747304        -0.279928   \n",
       "1         -0.039969         -0.537447          0.270558        -0.376214   \n",
       "2         -0.039969         -0.135000          0.060790        -0.325448   \n",
       "3         -0.039969         -0.345805         -0.282468        -0.380806   \n",
       "4         -0.039969          0.363268          0.922747         0.055063   \n",
       "\n",
       "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
       "0      -0.186099       1.304254       0.473136      1.739631   \n",
       "1       0.330931       0.120764       0.761394     -0.312975   \n",
       "2       1.106477       1.941519       1.373942      0.665653   \n",
       "3      -0.444615      -0.425462      -0.223487     -0.334289   \n",
       "4       2.399054       1.486330       1.373942     -0.215498   \n",
       "\n",
       "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('pwat', 'amax')  \\\n",
       "0          -0.776053          -0.963169  ...          0.816511   \n",
       "1          -0.455693           0.478855  ...         -0.389155   \n",
       "2          -1.631811          -0.177550  ...          0.119116   \n",
       "3          -0.736557           0.166158  ...         -0.176390   \n",
       "4          -0.109000           1.698631  ...          0.757409   \n",
       "\n",
       "   ('pwat', 'mean')  ('pwat', 'var')  ('paramId_0', 'amin')  \\\n",
       "0          0.240834         1.065975                    0.0   \n",
       "1          0.148937        -0.626555                    0.0   \n",
       "2          0.370626        -0.105228                    0.0   \n",
       "3         -0.237596        -0.218673                    0.0   \n",
       "4          0.514155        -0.172525                    0.0   \n",
       "\n",
       "   ('paramId_0', 'amax')  ('paramId_0', 'mean')  ('paramId_0', 'var')  \\\n",
       "0               1.884534               0.148744              0.561781   \n",
       "1               1.616252               0.908710              1.249283   \n",
       "2               1.303257               0.556262              0.556752   \n",
       "3              -0.753567              -0.501082             -0.480632   \n",
       "4               2.108102               4.146827              4.417549   \n",
       "\n",
       "   ('pres', 'count')  ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
       "0          -0.058223                     -1.016591              -1.630538  \n",
       "1           0.882420                     -1.016591              -1.630538  \n",
       "2           0.882420                     -1.016591              -1.630538  \n",
       "3          -0.528544                     -1.016591              -1.333825  \n",
       "4           4.174669                     -1.016591              -1.333825  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(feature_part_train_3_1.head())\n",
    "part_features_3_1_train = pd.DataFrame(scaler.fit_transform(feature_part_train_3_1))\n",
    "part_features_3_1_train.columns = feature_part_train_3_1.columns\n",
    "part_features_3_1_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
      "292               0.0              0.14           0.02900         0.002435   \n",
      "293               0.0              1.33           0.12625         0.043993   \n",
      "294               0.0              0.47           0.04800         0.011781   \n",
      "295               0.0              0.36           0.10225         0.007274   \n",
      "296               0.0              1.50           0.22900         0.187840   \n",
      "\n",
      "     ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
      "292            5.0           25.0         11.125     35.240385   \n",
      "293            8.0           53.0         25.250    174.756410   \n",
      "294            7.0           36.0         14.375     67.112179   \n",
      "295           15.0           43.0         25.250     29.730769   \n",
      "296           16.0           53.0         30.025    115.768590   \n",
      "\n",
      "     ('tozne', 'amin')  ('tozne', 'amax')  ...  ('pwat', 'amax')  \\\n",
      "292         246.800003         289.600006  ...         17.900000   \n",
      "293         243.399994         291.399994  ...         31.100000   \n",
      "294         257.000000         371.100006  ...         24.700001   \n",
      "295         278.200012         397.899994  ...         22.799999   \n",
      "296         265.700012         381.100006  ...         35.400002   \n",
      "\n",
      "     ('pwat', 'mean')  ('pwat', 'var')  ('paramId_0', 'amin')  \\\n",
      "292            8.0600        13.980923                    0.0   \n",
      "293           15.2250        54.741923                    0.0   \n",
      "294           10.2225        26.132044                    0.0   \n",
      "295           14.5500         7.674359                    0.0   \n",
      "296           18.6275        57.461017                    0.0   \n",
      "\n",
      "     ('paramId_0', 'amax')  ('paramId_0', 'mean')  ('paramId_0', 'var')  \\\n",
      "292                    0.0                   0.00              0.000000   \n",
      "293                    0.0                   0.00              0.000000   \n",
      "294                   23.0                   0.60             13.220513   \n",
      "295                   63.0                  10.15            389.976923   \n",
      "296                   11.0                   0.45              4.151282   \n",
      "\n",
      "     ('pres', 'count')  ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
      "292                  0                             0                      1  \n",
      "293                  0                             0                      1  \n",
      "294                  1                             0                      2  \n",
      "295                  9                             0                      2  \n",
      "296                  2                             0                      2  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>('cwat', 'amin')</th>\n",
       "      <th>('cwat', 'amax')</th>\n",
       "      <th>('cwat', 'mean')</th>\n",
       "      <th>('cwat', 'var')</th>\n",
       "      <th>('r', 'amin')</th>\n",
       "      <th>('r', 'amax')</th>\n",
       "      <th>('r', 'mean')</th>\n",
       "      <th>('r', 'var')</th>\n",
       "      <th>('tozne', 'amin')</th>\n",
       "      <th>('tozne', 'amax')</th>\n",
       "      <th>...</th>\n",
       "      <th>('pwat', 'amax')</th>\n",
       "      <th>('pwat', 'mean')</th>\n",
       "      <th>('pwat', 'var')</th>\n",
       "      <th>('paramId_0', 'amin')</th>\n",
       "      <th>('paramId_0', 'amax')</th>\n",
       "      <th>('paramId_0', 'mean')</th>\n",
       "      <th>('paramId_0', 'var')</th>\n",
       "      <th>('pres', 'count')</th>\n",
       "      <th>('macro_season', '&lt;lambda&gt;')</th>\n",
       "      <th>('month', '&lt;lambda&gt;')</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.039969</td>\n",
       "      <td>-0.690760</td>\n",
       "      <td>-0.385445</td>\n",
       "      <td>-0.437507</td>\n",
       "      <td>-0.961646</td>\n",
       "      <td>-0.789613</td>\n",
       "      <td>-1.208368</td>\n",
       "      <td>-0.325716</td>\n",
       "      <td>-1.508933</td>\n",
       "      <td>-1.343057</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.436436</td>\n",
       "      <td>-0.822134</td>\n",
       "      <td>-0.270080</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.753567</td>\n",
       "      <td>-0.501082</td>\n",
       "      <td>-0.480632</td>\n",
       "      <td>-0.528544</td>\n",
       "      <td>-1.016591</td>\n",
       "      <td>-1.630538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.039969</td>\n",
       "      <td>1.589773</td>\n",
       "      <td>1.098189</td>\n",
       "      <td>0.463320</td>\n",
       "      <td>-0.186099</td>\n",
       "      <td>1.759443</td>\n",
       "      <td>1.053655</td>\n",
       "      <td>2.913779</td>\n",
       "      <td>-1.658142</td>\n",
       "      <td>-1.296540</td>\n",
       "      <td>...</td>\n",
       "      <td>1.123837</td>\n",
       "      <td>0.535471</td>\n",
       "      <td>2.027846</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.753567</td>\n",
       "      <td>-0.501082</td>\n",
       "      <td>-0.480632</td>\n",
       "      <td>-0.528544</td>\n",
       "      <td>-1.016591</td>\n",
       "      <td>-1.630538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.039969</td>\n",
       "      <td>-0.058343</td>\n",
       "      <td>-0.095583</td>\n",
       "      <td>-0.234930</td>\n",
       "      <td>-0.444615</td>\n",
       "      <td>0.211802</td>\n",
       "      <td>-0.687903</td>\n",
       "      <td>0.414332</td>\n",
       "      <td>-1.061306</td>\n",
       "      <td>0.763125</td>\n",
       "      <td>...</td>\n",
       "      <td>0.367341</td>\n",
       "      <td>-0.412389</td>\n",
       "      <td>0.414946</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.274845</td>\n",
       "      <td>-0.236746</td>\n",
       "      <td>-0.322273</td>\n",
       "      <td>-0.058223</td>\n",
       "      <td>-1.016591</td>\n",
       "      <td>-1.333825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.039969</td>\n",
       "      <td>-0.269149</td>\n",
       "      <td>0.732048</td>\n",
       "      <td>-0.332607</td>\n",
       "      <td>1.623508</td>\n",
       "      <td>0.849066</td>\n",
       "      <td>1.053655</td>\n",
       "      <td>-0.453647</td>\n",
       "      <td>-0.130943</td>\n",
       "      <td>1.455710</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142756</td>\n",
       "      <td>0.407574</td>\n",
       "      <td>-0.625617</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.063388</td>\n",
       "      <td>3.970603</td>\n",
       "      <td>4.190637</td>\n",
       "      <td>3.704348</td>\n",
       "      <td>-1.016591</td>\n",
       "      <td>-1.333825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.039969</td>\n",
       "      <td>1.915563</td>\n",
       "      <td>2.665730</td>\n",
       "      <td>3.581366</td>\n",
       "      <td>1.882023</td>\n",
       "      <td>1.759443</td>\n",
       "      <td>1.818339</td>\n",
       "      <td>1.544110</td>\n",
       "      <td>-0.679506</td>\n",
       "      <td>1.021552</td>\n",
       "      <td>...</td>\n",
       "      <td>1.632108</td>\n",
       "      <td>1.180167</td>\n",
       "      <td>2.181136</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.261718</td>\n",
       "      <td>-0.302830</td>\n",
       "      <td>-0.430907</td>\n",
       "      <td>0.412099</td>\n",
       "      <td>-1.016591</td>\n",
       "      <td>-1.333825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
       "0         -0.039969         -0.690760         -0.385445        -0.437507   \n",
       "1         -0.039969          1.589773          1.098189         0.463320   \n",
       "2         -0.039969         -0.058343         -0.095583        -0.234930   \n",
       "3         -0.039969         -0.269149          0.732048        -0.332607   \n",
       "4         -0.039969          1.915563          2.665730         3.581366   \n",
       "\n",
       "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
       "0      -0.961646      -0.789613      -1.208368     -0.325716   \n",
       "1      -0.186099       1.759443       1.053655      2.913779   \n",
       "2      -0.444615       0.211802      -0.687903      0.414332   \n",
       "3       1.623508       0.849066       1.053655     -0.453647   \n",
       "4       1.882023       1.759443       1.818339      1.544110   \n",
       "\n",
       "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('pwat', 'amax')  \\\n",
       "0          -1.508933          -1.343057  ...         -0.436436   \n",
       "1          -1.658142          -1.296540  ...          1.123837   \n",
       "2          -1.061306           0.763125  ...          0.367341   \n",
       "3          -0.130943           1.455710  ...          0.142756   \n",
       "4          -0.679506           1.021552  ...          1.632108   \n",
       "\n",
       "   ('pwat', 'mean')  ('pwat', 'var')  ('paramId_0', 'amin')  \\\n",
       "0         -0.822134        -0.270080                    0.0   \n",
       "1          0.535471         2.027846                    0.0   \n",
       "2         -0.412389         0.414946                    0.0   \n",
       "3          0.407574        -0.625617                    0.0   \n",
       "4          1.180167         2.181136                    0.0   \n",
       "\n",
       "   ('paramId_0', 'amax')  ('paramId_0', 'mean')  ('paramId_0', 'var')  \\\n",
       "0              -0.753567              -0.501082             -0.480632   \n",
       "1              -0.753567              -0.501082             -0.480632   \n",
       "2               0.274845              -0.236746             -0.322273   \n",
       "3               2.063388               3.970603              4.190637   \n",
       "4              -0.261718              -0.302830             -0.430907   \n",
       "\n",
       "   ('pres', 'count')  ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
       "0          -0.528544                     -1.016591              -1.630538  \n",
       "1          -0.528544                     -1.016591              -1.630538  \n",
       "2          -0.058223                     -1.016591              -1.333825  \n",
       "3           3.704348                     -1.016591              -1.333825  \n",
       "4           0.412099                     -1.016591              -1.333825  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tenday_no_null_test.head())\n",
    "tenday_part_features_test = pd.DataFrame(scaler.transform(tenday_no_null_test))\n",
    "tenday_part_features_test.columns = tenday_no_null_test.columns\n",
    "tenday_part_features_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.6653\n",
      "Test set accuracy: 0.6639\n",
      "Training Set Precision: 0.405\n",
      "Training Set Recall: 0.722\n",
      "Training Set F1 Score: 0.5189\n",
      "Test Set Precision: 0.2522\n",
      "Test Set Recall: 0.7012\n",
      "Test Set F1 Score: 0.371\n"
     ]
    }
   ],
   "source": [
    "clf_log_10day_part = LogisticRegression(class_weight = \"balanced\")\n",
    "clf_log_10day_part.fit(part_features_3_1_train, target_train_3_1)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_log_10day_part.score(part_features_3_1_train, \n",
    "                                                       target_train_3_1), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_log_10day_part.score(tenday_part_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_log = clf_log_10day_part.predict(part_features_3_1_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(target_train_3_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(target_train_3_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(target_train_3_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_log = clf_log_10day_part.predict(tenday_part_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_log), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_log), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_log), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.7599\n",
      "Test set accuracy: 0.8404\n",
      "Training Set Precision: 0.5597\n",
      "Training Set Recall: 0.1852\n",
      "Training Set F1 Score: 0.2784\n",
      "Test Set Precision: 0.3877\n",
      "Test Set Recall: 0.2232\n",
      "Test Set F1 Score: 0.2833\n"
     ]
    }
   ],
   "source": [
    "clf_log_10day_part = LogisticRegression()\n",
    "clf_log_10day_part.fit(part_features_3_1_train, target_train_3_1)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_log_10day_part.score(part_features_3_1_train, \n",
    "                                                       target_train_3_1), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_log_10day_part.score(tenday_part_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_log = clf_log_10day_part.predict(part_features_3_1_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(target_train_3_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(target_train_3_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(target_train_3_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_log = clf_log_10day_part.predict(tenday_part_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_log), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_log), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_log), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.6646\n",
      "Test set accuracy: 0.6644\n",
      "Training Set Precision: 0.4039\n",
      "Training Set Recall: 0.7178\n",
      "Training Set F1 Score: 0.5169\n",
      "Test Set Precision: 0.2521\n",
      "Test Set Recall: 0.6988\n",
      "Test Set F1 Score: 0.3705\n"
     ]
    }
   ],
   "source": [
    "clf_sgd_10day_part = SGDClassifier(loss = \"log\", class_weight = \"balanced\", random_state = 0)\n",
    "clf_sgd_10day_part.fit(part_features_3_1_train, target_train_3_1)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_sgd_10day_part.score(part_features_3_1_train, \n",
    "                                                       target_train_3_1), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_sgd_10day_part.score(tenday_part_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_sgd = clf_sgd_10day_part.predict(part_features_3_1_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(target_train_3_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(target_train_3_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(target_train_3_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_sgd = clf_sgd_10day_part.predict(tenday_part_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_sgd), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_sgd), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_sgd), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.7596\n",
      "Test set accuracy: 0.8404\n",
      "Training Set Precision: 0.5571\n",
      "Training Set Recall: 0.1869\n",
      "Training Set F1 Score: 0.2799\n",
      "Test Set Precision: 0.3856\n",
      "Test Set Recall: 0.2185\n",
      "Test Set F1 Score: 0.279\n"
     ]
    }
   ],
   "source": [
    "clf_sgd_10day_part = SGDClassifier(loss = \"log\", random_state = 0)\n",
    "clf_sgd_10day_part.fit(part_features_3_1_train, target_train_3_1)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_sgd_10day_part.score(part_features_3_1_train, \n",
    "                                                       target_train_3_1), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_sgd_10day_part.score(tenday_part_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_sgd = clf_sgd_10day_part.predict(part_features_3_1_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(target_train_3_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(target_train_3_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(target_train_3_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_sgd = clf_sgd_10day_part.predict(tenday_part_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_sgd), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_sgd), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_sgd), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4:1 Ratio No Fire:Fire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Full Parameter List "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    117335\n",
       "1     29333\n",
       "Name: fire, dtype: int64"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "over_sampler_4_1 = RandomOverSampler(sampling_strategy = 0.25, random_state = 0)\n",
    "feature_full_train_4_1, target_train_4_1 = over_sampler_4_1.fit_resample(tenday_full_train, tenday_target_train)\n",
    "target_train_4_1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 146668 entries, 0 to 146667\n",
      "Data columns (total 31 columns):\n",
      " #   Column                        Non-Null Count   Dtype  \n",
      "---  ------                        --------------   -----  \n",
      " 0   ('cwat', 'amin')              146668 non-null  float64\n",
      " 1   ('cwat', 'amax')              146668 non-null  float64\n",
      " 2   ('cwat', 'mean')              146668 non-null  float64\n",
      " 3   ('cwat', 'var')               146668 non-null  float64\n",
      " 4   ('r', 'amin')                 146668 non-null  float64\n",
      " 5   ('r', 'amax')                 146668 non-null  float64\n",
      " 6   ('r', 'mean')                 146668 non-null  float64\n",
      " 7   ('r', 'var')                  146668 non-null  float64\n",
      " 8   ('tozne', 'amin')             146668 non-null  float64\n",
      " 9   ('tozne', 'amax')             146668 non-null  float64\n",
      " 10  ('tozne', 'mean')             146668 non-null  float64\n",
      " 11  ('tozne', 'var')              146668 non-null  float64\n",
      " 12  ('gh', 'amin')                146668 non-null  float64\n",
      " 13  ('gh', 'amax')                146668 non-null  float64\n",
      " 14  ('gh', 'mean')                146668 non-null  float64\n",
      " 15  ('gh', 'var')                 146668 non-null  float64\n",
      " 16  ('pwat', 'amin')              146668 non-null  float64\n",
      " 17  ('pwat', 'amax')              146668 non-null  float64\n",
      " 18  ('pwat', 'mean')              146668 non-null  float64\n",
      " 19  ('pwat', 'var')               146668 non-null  float64\n",
      " 20  ('paramId_0', 'amin')         146668 non-null  float64\n",
      " 21  ('paramId_0', 'amax')         146668 non-null  float64\n",
      " 22  ('paramId_0', 'mean')         146668 non-null  float64\n",
      " 23  ('paramId_0', 'var')          146668 non-null  float64\n",
      " 24  ('pres', 'amin')              57073 non-null   float64\n",
      " 25  ('pres', 'amax')              57073 non-null   float64\n",
      " 26  ('pres', 'mean')              57073 non-null   float64\n",
      " 27  ('pres', 'var')               35848 non-null   float64\n",
      " 28  ('pres', 'count')             146668 non-null  int64  \n",
      " 29  ('macro_season', '<lambda>')  146668 non-null  int64  \n",
      " 30  ('month', '<lambda>')         146668 non-null  int64  \n",
      "dtypes: float64(28), int64(3)\n",
      "memory usage: 34.7 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 146668 entries, 0 to 146667\n",
      "Data columns (total 31 columns):\n",
      " #   Column                        Non-Null Count   Dtype  \n",
      "---  ------                        --------------   -----  \n",
      " 0   ('cwat', 'amin')              146668 non-null  float64\n",
      " 1   ('cwat', 'amax')              146668 non-null  float64\n",
      " 2   ('cwat', 'mean')              146668 non-null  float64\n",
      " 3   ('cwat', 'var')               146668 non-null  float64\n",
      " 4   ('r', 'amin')                 146668 non-null  float64\n",
      " 5   ('r', 'amax')                 146668 non-null  float64\n",
      " 6   ('r', 'mean')                 146668 non-null  float64\n",
      " 7   ('r', 'var')                  146668 non-null  float64\n",
      " 8   ('tozne', 'amin')             146668 non-null  float64\n",
      " 9   ('tozne', 'amax')             146668 non-null  float64\n",
      " 10  ('tozne', 'mean')             146668 non-null  float64\n",
      " 11  ('tozne', 'var')              146668 non-null  float64\n",
      " 12  ('gh', 'amin')                146668 non-null  float64\n",
      " 13  ('gh', 'amax')                146668 non-null  float64\n",
      " 14  ('gh', 'mean')                146668 non-null  float64\n",
      " 15  ('gh', 'var')                 146668 non-null  float64\n",
      " 16  ('pwat', 'amin')              146668 non-null  float64\n",
      " 17  ('pwat', 'amax')              146668 non-null  float64\n",
      " 18  ('pwat', 'mean')              146668 non-null  float64\n",
      " 19  ('pwat', 'var')               146668 non-null  float64\n",
      " 20  ('paramId_0', 'amin')         146668 non-null  float64\n",
      " 21  ('paramId_0', 'amax')         146668 non-null  float64\n",
      " 22  ('paramId_0', 'mean')         146668 non-null  float64\n",
      " 23  ('paramId_0', 'var')          146668 non-null  float64\n",
      " 24  ('pres', 'amin')              146668 non-null  float64\n",
      " 25  ('pres', 'amax')              146668 non-null  float64\n",
      " 26  ('pres', 'mean')              146668 non-null  float64\n",
      " 27  ('pres', 'var')               146668 non-null  float64\n",
      " 28  ('pres', 'count')             146668 non-null  float64\n",
      " 29  ('macro_season', '<lambda>')  146668 non-null  float64\n",
      " 30  ('month', '<lambda>')         146668 non-null  float64\n",
      "dtypes: float64(31)\n",
      "memory usage: 34.7 MB\n"
     ]
    }
   ],
   "source": [
    "feature_full_train_4_1.info()\n",
    "feature_imp_train_4_1 = pd.DataFrame(imp.fit_transform(feature_full_train_4_1))\n",
    "feature_imp_train_4_1.columns = feature_full_train_4_1.columns\n",
    "feature_imp_train_4_1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 35259 entries, 292 to 176294\n",
      "Data columns (total 31 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   ('cwat', 'amin')              35259 non-null  float64\n",
      " 1   ('cwat', 'amax')              35259 non-null  float64\n",
      " 2   ('cwat', 'mean')              35259 non-null  float64\n",
      " 3   ('cwat', 'var')               35259 non-null  float64\n",
      " 4   ('r', 'amin')                 35259 non-null  float64\n",
      " 5   ('r', 'amax')                 35259 non-null  float64\n",
      " 6   ('r', 'mean')                 35259 non-null  float64\n",
      " 7   ('r', 'var')                  35259 non-null  float64\n",
      " 8   ('tozne', 'amin')             35259 non-null  float64\n",
      " 9   ('tozne', 'amax')             35259 non-null  float64\n",
      " 10  ('tozne', 'mean')             35259 non-null  float64\n",
      " 11  ('tozne', 'var')              35259 non-null  float64\n",
      " 12  ('gh', 'amin')                35259 non-null  float64\n",
      " 13  ('gh', 'amax')                35259 non-null  float64\n",
      " 14  ('gh', 'mean')                35259 non-null  float64\n",
      " 15  ('gh', 'var')                 35259 non-null  float64\n",
      " 16  ('pwat', 'amin')              35259 non-null  float64\n",
      " 17  ('pwat', 'amax')              35259 non-null  float64\n",
      " 18  ('pwat', 'mean')              35259 non-null  float64\n",
      " 19  ('pwat', 'var')               35259 non-null  float64\n",
      " 20  ('paramId_0', 'amin')         35259 non-null  float64\n",
      " 21  ('paramId_0', 'amax')         35259 non-null  float64\n",
      " 22  ('paramId_0', 'mean')         35259 non-null  float64\n",
      " 23  ('paramId_0', 'var')          35259 non-null  float64\n",
      " 24  ('pres', 'amin')              15069 non-null  float64\n",
      " 25  ('pres', 'amax')              15069 non-null  float64\n",
      " 26  ('pres', 'mean')              15069 non-null  float64\n",
      " 27  ('pres', 'var')               9461 non-null   float64\n",
      " 28  ('pres', 'count')             35259 non-null  int64  \n",
      " 29  ('macro_season', '<lambda>')  35259 non-null  int64  \n",
      " 30  ('month', '<lambda>')         35259 non-null  int64  \n",
      "dtypes: float64(28), int64(3)\n",
      "memory usage: 8.6 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 35259 entries, 0 to 35258\n",
      "Data columns (total 31 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   ('cwat', 'amin')              35259 non-null  float64\n",
      " 1   ('cwat', 'amax')              35259 non-null  float64\n",
      " 2   ('cwat', 'mean')              35259 non-null  float64\n",
      " 3   ('cwat', 'var')               35259 non-null  float64\n",
      " 4   ('r', 'amin')                 35259 non-null  float64\n",
      " 5   ('r', 'amax')                 35259 non-null  float64\n",
      " 6   ('r', 'mean')                 35259 non-null  float64\n",
      " 7   ('r', 'var')                  35259 non-null  float64\n",
      " 8   ('tozne', 'amin')             35259 non-null  float64\n",
      " 9   ('tozne', 'amax')             35259 non-null  float64\n",
      " 10  ('tozne', 'mean')             35259 non-null  float64\n",
      " 11  ('tozne', 'var')              35259 non-null  float64\n",
      " 12  ('gh', 'amin')                35259 non-null  float64\n",
      " 13  ('gh', 'amax')                35259 non-null  float64\n",
      " 14  ('gh', 'mean')                35259 non-null  float64\n",
      " 15  ('gh', 'var')                 35259 non-null  float64\n",
      " 16  ('pwat', 'amin')              35259 non-null  float64\n",
      " 17  ('pwat', 'amax')              35259 non-null  float64\n",
      " 18  ('pwat', 'mean')              35259 non-null  float64\n",
      " 19  ('pwat', 'var')               35259 non-null  float64\n",
      " 20  ('paramId_0', 'amin')         35259 non-null  float64\n",
      " 21  ('paramId_0', 'amax')         35259 non-null  float64\n",
      " 22  ('paramId_0', 'mean')         35259 non-null  float64\n",
      " 23  ('paramId_0', 'var')          35259 non-null  float64\n",
      " 24  ('pres', 'amin')              35259 non-null  float64\n",
      " 25  ('pres', 'amax')              35259 non-null  float64\n",
      " 26  ('pres', 'mean')              35259 non-null  float64\n",
      " 27  ('pres', 'var')               35259 non-null  float64\n",
      " 28  ('pres', 'count')             35259 non-null  float64\n",
      " 29  ('macro_season', '<lambda>')  35259 non-null  float64\n",
      " 30  ('month', '<lambda>')         35259 non-null  float64\n",
      "dtypes: float64(31)\n",
      "memory usage: 8.3 MB\n"
     ]
    }
   ],
   "source": [
    "tenday_full_test.info()\n",
    "tenday_imp_test = pd.DataFrame(imp.fit_transform(tenday_full_test))\n",
    "tenday_imp_test.columns = tenday_full_test.columns\n",
    "tenday_imp_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
      "0               0.0              0.42           0.10325         0.009705   \n",
      "1               0.0              0.22           0.07200         0.005263   \n",
      "2               0.0              0.43           0.05825         0.007605   \n",
      "3               0.0              0.32           0.03575         0.005051   \n",
      "4               0.0              0.69           0.11475         0.025159   \n",
      "\n",
      "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
      "0            8.0           48.0         21.625    124.189103   \n",
      "1           10.0           35.0         23.425     35.789103   \n",
      "2           13.0           55.0         27.250     77.935897   \n",
      "3            7.0           29.0         17.275     34.871154   \n",
      "4           18.0           50.0         27.250     39.987179   \n",
      "\n",
      "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('paramId_0', 'amax')  \\\n",
      "0         263.500000         304.299988  ...                   59.0   \n",
      "1         270.799988         360.100006  ...                   53.0   \n",
      "2         244.000000         334.700012  ...                   46.0   \n",
      "3         264.399994         348.000000  ...                    0.0   \n",
      "4         278.700012         407.299988  ...                   64.0   \n",
      "\n",
      "   ('paramId_0', 'mean')  ('paramId_0', 'var')  ('pres', 'amin')  \\\n",
      "0                  1.475             87.025000           34430.0   \n",
      "1                  3.200            144.420513           61430.0   \n",
      "2                  2.400             86.605128           65030.0   \n",
      "3                  0.000              0.000000           42970.0   \n",
      "4                 10.550            408.920513           41470.0   \n",
      "\n",
      "   ('pres', 'amax')  ('pres', 'mean')  ('pres', 'var')  ('pres', 'count')  \\\n",
      "0           34430.0      34430.000000     4.890605e+07                1.0   \n",
      "1           62960.0      62443.333333     7.702333e+05                3.0   \n",
      "2           75960.0      71433.333333     3.250763e+07                3.0   \n",
      "3           55790.0      50426.666667     4.890605e+07                0.0   \n",
      "4           76180.0      57815.000000     2.113109e+08               10.0   \n",
      "\n",
      "   ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
      "0                           0.0                    1.0  \n",
      "1                           0.0                    1.0  \n",
      "2                           0.0                    1.0  \n",
      "3                           0.0                    2.0  \n",
      "4                           0.0                    2.0  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>('cwat', 'amin')</th>\n",
       "      <th>('cwat', 'amax')</th>\n",
       "      <th>('cwat', 'mean')</th>\n",
       "      <th>('cwat', 'var')</th>\n",
       "      <th>('r', 'amin')</th>\n",
       "      <th>('r', 'amax')</th>\n",
       "      <th>('r', 'mean')</th>\n",
       "      <th>('r', 'var')</th>\n",
       "      <th>('tozne', 'amin')</th>\n",
       "      <th>('tozne', 'amax')</th>\n",
       "      <th>...</th>\n",
       "      <th>('paramId_0', 'amax')</th>\n",
       "      <th>('paramId_0', 'mean')</th>\n",
       "      <th>('paramId_0', 'var')</th>\n",
       "      <th>('pres', 'amin')</th>\n",
       "      <th>('pres', 'amax')</th>\n",
       "      <th>('pres', 'mean')</th>\n",
       "      <th>('pres', 'var')</th>\n",
       "      <th>('pres', 'count')</th>\n",
       "      <th>('macro_season', '&lt;lambda&gt;')</th>\n",
       "      <th>('month', '&lt;lambda&gt;')</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.040975</td>\n",
       "      <td>-0.169039</td>\n",
       "      <td>0.716210</td>\n",
       "      <td>-0.288309</td>\n",
       "      <td>-0.200135</td>\n",
       "      <td>1.282374</td>\n",
       "      <td>0.452033</td>\n",
       "      <td>1.718702</td>\n",
       "      <td>-0.761443</td>\n",
       "      <td>-0.971055</td>\n",
       "      <td>...</td>\n",
       "      <td>1.859557</td>\n",
       "      <td>0.134404</td>\n",
       "      <td>0.542619</td>\n",
       "      <td>-1.141225</td>\n",
       "      <td>-2.301097</td>\n",
       "      <td>-2.010614</td>\n",
       "      <td>-0.155729</td>\n",
       "      <td>-0.070656</td>\n",
       "      <td>-1.001419</td>\n",
       "      <td>-1.596821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.040975</td>\n",
       "      <td>-0.550471</td>\n",
       "      <td>0.245112</td>\n",
       "      <td>-0.383498</td>\n",
       "      <td>0.313999</td>\n",
       "      <td>0.103930</td>\n",
       "      <td>0.738453</td>\n",
       "      <td>-0.321883</td>\n",
       "      <td>-0.443504</td>\n",
       "      <td>0.461639</td>\n",
       "      <td>...</td>\n",
       "      <td>1.592859</td>\n",
       "      <td>0.883659</td>\n",
       "      <td>1.221350</td>\n",
       "      <td>2.250348</td>\n",
       "      <td>0.850770</td>\n",
       "      <td>1.589270</td>\n",
       "      <td>-1.073219</td>\n",
       "      <td>0.856715</td>\n",
       "      <td>-1.001419</td>\n",
       "      <td>-1.596821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.040975</td>\n",
       "      <td>-0.149967</td>\n",
       "      <td>0.037829</td>\n",
       "      <td>-0.333311</td>\n",
       "      <td>1.085200</td>\n",
       "      <td>1.916920</td>\n",
       "      <td>1.347096</td>\n",
       "      <td>0.651014</td>\n",
       "      <td>-1.610733</td>\n",
       "      <td>-0.190519</td>\n",
       "      <td>...</td>\n",
       "      <td>1.281711</td>\n",
       "      <td>0.536178</td>\n",
       "      <td>0.537654</td>\n",
       "      <td>2.702558</td>\n",
       "      <td>2.286952</td>\n",
       "      <td>2.744539</td>\n",
       "      <td>-0.468290</td>\n",
       "      <td>0.856715</td>\n",
       "      <td>-1.001419</td>\n",
       "      <td>-1.596821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.040975</td>\n",
       "      <td>-0.359755</td>\n",
       "      <td>-0.301362</td>\n",
       "      <td>-0.388038</td>\n",
       "      <td>-0.457203</td>\n",
       "      <td>-0.439967</td>\n",
       "      <td>-0.240149</td>\n",
       "      <td>-0.343073</td>\n",
       "      <td>-0.722245</td>\n",
       "      <td>0.150966</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.762973</td>\n",
       "      <td>-0.506263</td>\n",
       "      <td>-0.486495</td>\n",
       "      <td>-0.068483</td>\n",
       "      <td>0.058661</td>\n",
       "      <td>0.045055</td>\n",
       "      <td>-0.155729</td>\n",
       "      <td>-0.534341</td>\n",
       "      <td>-1.001419</td>\n",
       "      <td>-1.304275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.040975</td>\n",
       "      <td>0.345895</td>\n",
       "      <td>0.889574</td>\n",
       "      <td>0.042866</td>\n",
       "      <td>2.370536</td>\n",
       "      <td>1.463673</td>\n",
       "      <td>1.347096</td>\n",
       "      <td>-0.224977</td>\n",
       "      <td>-0.099431</td>\n",
       "      <td>1.673523</td>\n",
       "      <td>...</td>\n",
       "      <td>2.081805</td>\n",
       "      <td>4.076135</td>\n",
       "      <td>4.349194</td>\n",
       "      <td>-0.256903</td>\n",
       "      <td>2.311257</td>\n",
       "      <td>0.994501</td>\n",
       "      <td>2.939778</td>\n",
       "      <td>4.102513</td>\n",
       "      <td>-1.001419</td>\n",
       "      <td>-1.304275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
       "0         -0.040975         -0.169039          0.716210        -0.288309   \n",
       "1         -0.040975         -0.550471          0.245112        -0.383498   \n",
       "2         -0.040975         -0.149967          0.037829        -0.333311   \n",
       "3         -0.040975         -0.359755         -0.301362        -0.388038   \n",
       "4         -0.040975          0.345895          0.889574         0.042866   \n",
       "\n",
       "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
       "0      -0.200135       1.282374       0.452033      1.718702   \n",
       "1       0.313999       0.103930       0.738453     -0.321883   \n",
       "2       1.085200       1.916920       1.347096      0.651014   \n",
       "3      -0.457203      -0.439967      -0.240149     -0.343073   \n",
       "4       2.370536       1.463673       1.347096     -0.224977   \n",
       "\n",
       "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('paramId_0', 'amax')  \\\n",
       "0          -0.761443          -0.971055  ...               1.859557   \n",
       "1          -0.443504           0.461639  ...               1.592859   \n",
       "2          -1.610733          -0.190519  ...               1.281711   \n",
       "3          -0.722245           0.150966  ...              -0.762973   \n",
       "4          -0.099431           1.673523  ...               2.081805   \n",
       "\n",
       "   ('paramId_0', 'mean')  ('paramId_0', 'var')  ('pres', 'amin')  \\\n",
       "0               0.134404              0.542619         -1.141225   \n",
       "1               0.883659              1.221350          2.250348   \n",
       "2               0.536178              0.537654          2.702558   \n",
       "3              -0.506263             -0.486495         -0.068483   \n",
       "4               4.076135              4.349194         -0.256903   \n",
       "\n",
       "   ('pres', 'amax')  ('pres', 'mean')  ('pres', 'var')  ('pres', 'count')  \\\n",
       "0         -2.301097         -2.010614        -0.155729          -0.070656   \n",
       "1          0.850770          1.589270        -1.073219           0.856715   \n",
       "2          2.286952          2.744539        -0.468290           0.856715   \n",
       "3          0.058661          0.045055        -0.155729          -0.534341   \n",
       "4          2.311257          0.994501         2.939778           4.102513   \n",
       "\n",
       "   ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
       "0                     -1.001419              -1.596821  \n",
       "1                     -1.001419              -1.596821  \n",
       "2                     -1.001419              -1.596821  \n",
       "3                     -1.001419              -1.304275  \n",
       "4                     -1.001419              -1.304275  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(feature_imp_train_4_1.head())\n",
    "full_features_4_1_train = pd.DataFrame(scaler.fit_transform(feature_imp_train_4_1))\n",
    "full_features_4_1_train.columns = feature_imp_train_4_1.columns\n",
    "full_features_4_1_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>('cwat', 'amin')</th>\n",
       "      <th>('cwat', 'amax')</th>\n",
       "      <th>('cwat', 'mean')</th>\n",
       "      <th>('cwat', 'var')</th>\n",
       "      <th>('r', 'amin')</th>\n",
       "      <th>('r', 'amax')</th>\n",
       "      <th>('r', 'mean')</th>\n",
       "      <th>('r', 'var')</th>\n",
       "      <th>('tozne', 'amin')</th>\n",
       "      <th>('tozne', 'amax')</th>\n",
       "      <th>...</th>\n",
       "      <th>('paramId_0', 'amax')</th>\n",
       "      <th>('paramId_0', 'mean')</th>\n",
       "      <th>('paramId_0', 'var')</th>\n",
       "      <th>('pres', 'amin')</th>\n",
       "      <th>('pres', 'amax')</th>\n",
       "      <th>('pres', 'mean')</th>\n",
       "      <th>('pres', 'var')</th>\n",
       "      <th>('pres', 'count')</th>\n",
       "      <th>('macro_season', '&lt;lambda&gt;')</th>\n",
       "      <th>('month', '&lt;lambda&gt;')</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.040975</td>\n",
       "      <td>-0.703044</td>\n",
       "      <td>-0.403119</td>\n",
       "      <td>-0.444093</td>\n",
       "      <td>-0.971337</td>\n",
       "      <td>-0.802565</td>\n",
       "      <td>-1.218750</td>\n",
       "      <td>-0.334550</td>\n",
       "      <td>-1.488784</td>\n",
       "      <td>-1.348484</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.762973</td>\n",
       "      <td>-0.506263</td>\n",
       "      <td>-0.486495</td>\n",
       "      <td>-0.099886</td>\n",
       "      <td>-0.005415</td>\n",
       "      <td>-0.025195</td>\n",
       "      <td>-0.110560</td>\n",
       "      <td>-0.534341</td>\n",
       "      <td>-1.001419</td>\n",
       "      <td>-1.596821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.040975</td>\n",
       "      <td>1.566477</td>\n",
       "      <td>1.062938</td>\n",
       "      <td>0.446472</td>\n",
       "      <td>-0.200135</td>\n",
       "      <td>1.735621</td>\n",
       "      <td>1.028851</td>\n",
       "      <td>2.885974</td>\n",
       "      <td>-1.636865</td>\n",
       "      <td>-1.302269</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.762973</td>\n",
       "      <td>-0.506263</td>\n",
       "      <td>-0.486495</td>\n",
       "      <td>-0.099886</td>\n",
       "      <td>-0.005415</td>\n",
       "      <td>-0.025195</td>\n",
       "      <td>-0.110560</td>\n",
       "      <td>-0.534341</td>\n",
       "      <td>-1.001419</td>\n",
       "      <td>-1.596821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.040975</td>\n",
       "      <td>-0.073681</td>\n",
       "      <td>-0.116691</td>\n",
       "      <td>-0.243823</td>\n",
       "      <td>-0.457203</td>\n",
       "      <td>0.194580</td>\n",
       "      <td>-0.701603</td>\n",
       "      <td>0.401164</td>\n",
       "      <td>-1.044540</td>\n",
       "      <td>0.744070</td>\n",
       "      <td>...</td>\n",
       "      <td>0.259369</td>\n",
       "      <td>-0.245653</td>\n",
       "      <td>-0.330156</td>\n",
       "      <td>4.201131</td>\n",
       "      <td>2.397428</td>\n",
       "      <td>3.454749</td>\n",
       "      <td>-0.110560</td>\n",
       "      <td>-0.070656</td>\n",
       "      <td>-1.001419</td>\n",
       "      <td>-1.304275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.040975</td>\n",
       "      <td>-0.283468</td>\n",
       "      <td>0.701135</td>\n",
       "      <td>-0.340388</td>\n",
       "      <td>1.599334</td>\n",
       "      <td>0.829126</td>\n",
       "      <td>1.028851</td>\n",
       "      <td>-0.461731</td>\n",
       "      <td>-0.121208</td>\n",
       "      <td>1.432174</td>\n",
       "      <td>...</td>\n",
       "      <td>2.037355</td>\n",
       "      <td>3.902394</td>\n",
       "      <td>4.125177</td>\n",
       "      <td>-0.251879</td>\n",
       "      <td>2.493542</td>\n",
       "      <td>1.275858</td>\n",
       "      <td>3.468433</td>\n",
       "      <td>3.638828</td>\n",
       "      <td>-1.001419</td>\n",
       "      <td>-1.304275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.040975</td>\n",
       "      <td>1.890695</td>\n",
       "      <td>2.611909</td>\n",
       "      <td>3.529000</td>\n",
       "      <td>1.856402</td>\n",
       "      <td>1.735621</td>\n",
       "      <td>1.788660</td>\n",
       "      <td>1.524326</td>\n",
       "      <td>-0.665625</td>\n",
       "      <td>1.000825</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.274027</td>\n",
       "      <td>-0.310805</td>\n",
       "      <td>-0.437404</td>\n",
       "      <td>0.173952</td>\n",
       "      <td>1.575490</td>\n",
       "      <td>0.916754</td>\n",
       "      <td>4.688787</td>\n",
       "      <td>0.393030</td>\n",
       "      <td>-1.001419</td>\n",
       "      <td>-1.304275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
       "0         -0.040975         -0.703044         -0.403119        -0.444093   \n",
       "1         -0.040975          1.566477          1.062938         0.446472   \n",
       "2         -0.040975         -0.073681         -0.116691        -0.243823   \n",
       "3         -0.040975         -0.283468          0.701135        -0.340388   \n",
       "4         -0.040975          1.890695          2.611909         3.529000   \n",
       "\n",
       "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
       "0      -0.971337      -0.802565      -1.218750     -0.334550   \n",
       "1      -0.200135       1.735621       1.028851      2.885974   \n",
       "2      -0.457203       0.194580      -0.701603      0.401164   \n",
       "3       1.599334       0.829126       1.028851     -0.461731   \n",
       "4       1.856402       1.735621       1.788660      1.524326   \n",
       "\n",
       "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('paramId_0', 'amax')  \\\n",
       "0          -1.488784          -1.348484  ...              -0.762973   \n",
       "1          -1.636865          -1.302269  ...              -0.762973   \n",
       "2          -1.044540           0.744070  ...               0.259369   \n",
       "3          -0.121208           1.432174  ...               2.037355   \n",
       "4          -0.665625           1.000825  ...              -0.274027   \n",
       "\n",
       "   ('paramId_0', 'mean')  ('paramId_0', 'var')  ('pres', 'amin')  \\\n",
       "0              -0.506263             -0.486495         -0.099886   \n",
       "1              -0.506263             -0.486495         -0.099886   \n",
       "2              -0.245653             -0.330156          4.201131   \n",
       "3               3.902394              4.125177         -0.251879   \n",
       "4              -0.310805             -0.437404          0.173952   \n",
       "\n",
       "   ('pres', 'amax')  ('pres', 'mean')  ('pres', 'var')  ('pres', 'count')  \\\n",
       "0         -0.005415         -0.025195        -0.110560          -0.534341   \n",
       "1         -0.005415         -0.025195        -0.110560          -0.534341   \n",
       "2          2.397428          3.454749        -0.110560          -0.070656   \n",
       "3          2.493542          1.275858         3.468433           3.638828   \n",
       "4          1.575490          0.916754         4.688787           0.393030   \n",
       "\n",
       "   ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
       "0                     -1.001419              -1.596821  \n",
       "1                     -1.001419              -1.596821  \n",
       "2                     -1.001419              -1.304275  \n",
       "3                     -1.001419              -1.304275  \n",
       "4                     -1.001419              -1.304275  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tenday_full_features_test = pd.DataFrame(scaler.transform(tenday_imp_test))\n",
    "tenday_full_features_test.columns = tenday_imp_test.columns\n",
    "tenday_full_features_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.6596\n",
      "Test set accuracy: 0.6627\n",
      "Training Set Precision: 0.3366\n",
      "Training Set Recall: 0.7227\n",
      "Training Set F1 Score: 0.4592\n",
      "Test Set Precision: 0.2516\n",
      "Test Set Recall: 0.7024\n",
      "Test Set F1 Score: 0.3705\n"
     ]
    }
   ],
   "source": [
    "clf_log_10day_full = LogisticRegression(class_weight = \"balanced\", max_iter = 100000)\n",
    "clf_log_10day_full.fit(full_features_4_1_train, target_train_4_1)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_log_10day_full.score(full_features_4_1_train, \n",
    "                                                       target_train_4_1), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_log_10day_full.score(tenday_full_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_log = clf_log_10day_full.predict(full_features_4_1_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(target_train_4_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(target_train_4_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(target_train_4_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_log = clf_log_10day_full.predict(tenday_full_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_log), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_log), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_log), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.8016\n",
      "Test set accuracy: 0.8515\n",
      "Training Set Precision: 0.5285\n",
      "Training Set Recall: 0.0744\n",
      "Training Set F1 Score: 0.1304\n",
      "Test Set Precision: 0.3979\n",
      "Test Set Recall: 0.0985\n",
      "Test Set F1 Score: 0.158\n"
     ]
    }
   ],
   "source": [
    "clf_log_10day_full = LogisticRegression(max_iter = 100000)\n",
    "clf_log_10day_full.fit(full_features_4_1_train, target_train_4_1)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_log_10day_full.score(full_features_4_1_train, \n",
    "                                                       target_train_4_1), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_log_10day_full.score(tenday_full_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_log = clf_log_10day_full.predict(full_features_4_1_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(target_train_4_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(target_train_4_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(target_train_4_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_log = clf_log_10day_full.predict(tenday_full_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_log), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_log), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_log), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.6712\n",
      "Test set accuracy: 0.684\n",
      "Training Set Precision: 0.3406\n",
      "Training Set Recall: 0.6884\n",
      "Training Set F1 Score: 0.4557\n",
      "Test Set Precision: 0.2584\n",
      "Test Set Recall: 0.6608\n",
      "Test Set F1 Score: 0.3715\n"
     ]
    }
   ],
   "source": [
    "clf_sgd_10day_full = SGDClassifier(loss = \"log\", class_weight = \"balanced\", random_state = 0)\n",
    "clf_sgd_10day_full.fit(full_features_4_1_train, target_train_4_1)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_sgd_10day_full.score(full_features_4_1_train, \n",
    "                                                       target_train_4_1), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_sgd_10day_full.score(tenday_full_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_sgd = clf_sgd_10day_full.predict(full_features_4_1_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(target_train_4_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(target_train_4_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(target_train_4_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_sgd = clf_sgd_10day_full.predict(tenday_full_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_sgd), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_sgd), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_sgd), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.7981\n",
      "Test set accuracy: 0.8449\n",
      "Training Set Precision: 0.4851\n",
      "Training Set Recall: 0.1576\n",
      "Training Set F1 Score: 0.2379\n",
      "Test Set Precision: 0.3907\n",
      "Test Set Recall: 0.174\n",
      "Test Set F1 Score: 0.2408\n"
     ]
    }
   ],
   "source": [
    "clf_sgd_10day_full = SGDClassifier(loss = \"log\", random_state = 0)\n",
    "clf_sgd_10day_full.fit(full_features_4_1_train, target_train_4_1)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_sgd_10day_full.score(full_features_4_1_train, \n",
    "                                                       target_train_4_1), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_sgd_10day_full.score(tenday_full_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_sgd = clf_sgd_10day_full.predict(full_features_4_1_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(target_train_4_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(target_train_4_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(target_train_4_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_sgd = clf_sgd_10day_full.predict(tenday_full_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_sgd), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_sgd), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_sgd), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Part Parameter List "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    117335\n",
       "1     29333\n",
       "Name: fire, dtype: int64"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_part_train_4_1, target_train_4_1 = over_sampler_4_1.fit_resample(tenday_no_null_train, tenday_target_train)\n",
    "target_train_4_1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
      "0               0.0              0.42           0.10325         0.009705   \n",
      "1               0.0              0.22           0.07200         0.005263   \n",
      "2               0.0              0.43           0.05825         0.007605   \n",
      "3               0.0              0.32           0.03575         0.005051   \n",
      "4               0.0              0.69           0.11475         0.025159   \n",
      "\n",
      "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
      "0            8.0           48.0         21.625    124.189103   \n",
      "1           10.0           35.0         23.425     35.789103   \n",
      "2           13.0           55.0         27.250     77.935897   \n",
      "3            7.0           29.0         17.275     34.871154   \n",
      "4           18.0           50.0         27.250     39.987179   \n",
      "\n",
      "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('pwat', 'amax')  \\\n",
      "0         263.500000         304.299988  ...         28.500000   \n",
      "1         270.799988         360.100006  ...         18.299999   \n",
      "2         244.000000         334.700012  ...         22.600000   \n",
      "3         264.399994         348.000000  ...         20.100000   \n",
      "4         278.700012         407.299988  ...         28.000000   \n",
      "\n",
      "   ('pwat', 'mean')  ('pwat', 'var')  ('paramId_0', 'amin')  \\\n",
      "0           13.6700        37.680100                    0.0   \n",
      "1           13.1850         7.657718                    0.0   \n",
      "2           14.3550        16.905103                    0.0   \n",
      "3           11.1450        14.892795                    0.0   \n",
      "4           15.1125        15.711378                    0.0   \n",
      "\n",
      "   ('paramId_0', 'amax')  ('paramId_0', 'mean')  ('paramId_0', 'var')  \\\n",
      "0                   59.0                  1.475             87.025000   \n",
      "1                   53.0                  3.200            144.420513   \n",
      "2                   46.0                  2.400             86.605128   \n",
      "3                    0.0                  0.000              0.000000   \n",
      "4                   64.0                 10.550            408.920513   \n",
      "\n",
      "   ('pres', 'count')  ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
      "0                  1                             0                      1  \n",
      "1                  3                             0                      1  \n",
      "2                  3                             0                      1  \n",
      "3                  0                             0                      2  \n",
      "4                 10                             0                      2  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>('cwat', 'amin')</th>\n",
       "      <th>('cwat', 'amax')</th>\n",
       "      <th>('cwat', 'mean')</th>\n",
       "      <th>('cwat', 'var')</th>\n",
       "      <th>('r', 'amin')</th>\n",
       "      <th>('r', 'amax')</th>\n",
       "      <th>('r', 'mean')</th>\n",
       "      <th>('r', 'var')</th>\n",
       "      <th>('tozne', 'amin')</th>\n",
       "      <th>('tozne', 'amax')</th>\n",
       "      <th>...</th>\n",
       "      <th>('pwat', 'amax')</th>\n",
       "      <th>('pwat', 'mean')</th>\n",
       "      <th>('pwat', 'var')</th>\n",
       "      <th>('paramId_0', 'amin')</th>\n",
       "      <th>('paramId_0', 'amax')</th>\n",
       "      <th>('paramId_0', 'mean')</th>\n",
       "      <th>('paramId_0', 'var')</th>\n",
       "      <th>('pres', 'count')</th>\n",
       "      <th>('macro_season', '&lt;lambda&gt;')</th>\n",
       "      <th>('month', '&lt;lambda&gt;')</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.040975</td>\n",
       "      <td>-0.168893</td>\n",
       "      <td>0.716792</td>\n",
       "      <td>-0.288009</td>\n",
       "      <td>-0.201268</td>\n",
       "      <td>1.282547</td>\n",
       "      <td>0.451194</td>\n",
       "      <td>1.719151</td>\n",
       "      <td>-0.762231</td>\n",
       "      <td>-0.971225</td>\n",
       "      <td>...</td>\n",
       "      <td>0.812708</td>\n",
       "      <td>0.242401</td>\n",
       "      <td>1.059034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.857942</td>\n",
       "      <td>0.133474</td>\n",
       "      <td>0.540952</td>\n",
       "      <td>-0.071051</td>\n",
       "      <td>-1.001638</td>\n",
       "      <td>-1.598030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.040975</td>\n",
       "      <td>-0.550549</td>\n",
       "      <td>0.245452</td>\n",
       "      <td>-0.383291</td>\n",
       "      <td>0.312603</td>\n",
       "      <td>0.103549</td>\n",
       "      <td>0.737649</td>\n",
       "      <td>-0.321950</td>\n",
       "      <td>-0.444023</td>\n",
       "      <td>0.462410</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.383718</td>\n",
       "      <td>0.151094</td>\n",
       "      <td>-0.622044</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.591361</td>\n",
       "      <td>0.881744</td>\n",
       "      <td>1.218727</td>\n",
       "      <td>0.855250</td>\n",
       "      <td>-1.001638</td>\n",
       "      <td>-1.598030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.040975</td>\n",
       "      <td>-0.149811</td>\n",
       "      <td>0.038062</td>\n",
       "      <td>-0.333054</td>\n",
       "      <td>1.083410</td>\n",
       "      <td>1.917392</td>\n",
       "      <td>1.346367</td>\n",
       "      <td>0.651193</td>\n",
       "      <td>-1.612240</td>\n",
       "      <td>-0.190176</td>\n",
       "      <td>...</td>\n",
       "      <td>0.120658</td>\n",
       "      <td>0.371361</td>\n",
       "      <td>-0.104245</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.280350</td>\n",
       "      <td>0.534720</td>\n",
       "      <td>0.535994</td>\n",
       "      <td>0.855250</td>\n",
       "      <td>-1.001638</td>\n",
       "      <td>-1.598030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.040975</td>\n",
       "      <td>-0.359721</td>\n",
       "      <td>-0.301303</td>\n",
       "      <td>-0.387835</td>\n",
       "      <td>-0.458204</td>\n",
       "      <td>-0.440604</td>\n",
       "      <td>-0.241074</td>\n",
       "      <td>-0.343145</td>\n",
       "      <td>-0.723000</td>\n",
       "      <td>0.151532</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.172584</td>\n",
       "      <td>-0.232960</td>\n",
       "      <td>-0.216922</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.763436</td>\n",
       "      <td>-0.506352</td>\n",
       "      <td>-0.486713</td>\n",
       "      <td>-0.534201</td>\n",
       "      <td>-1.001638</td>\n",
       "      <td>-1.305326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.040975</td>\n",
       "      <td>0.346342</td>\n",
       "      <td>0.890246</td>\n",
       "      <td>0.043492</td>\n",
       "      <td>2.368087</td>\n",
       "      <td>1.463931</td>\n",
       "      <td>1.346367</td>\n",
       "      <td>-0.225019</td>\n",
       "      <td>-0.099660</td>\n",
       "      <td>1.675090</td>\n",
       "      <td>...</td>\n",
       "      <td>0.754060</td>\n",
       "      <td>0.513969</td>\n",
       "      <td>-0.171086</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.080093</td>\n",
       "      <td>4.070029</td>\n",
       "      <td>4.342167</td>\n",
       "      <td>4.097303</td>\n",
       "      <td>-1.001638</td>\n",
       "      <td>-1.305326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
       "0         -0.040975         -0.168893          0.716792        -0.288009   \n",
       "1         -0.040975         -0.550549          0.245452        -0.383291   \n",
       "2         -0.040975         -0.149811          0.038062        -0.333054   \n",
       "3         -0.040975         -0.359721         -0.301303        -0.387835   \n",
       "4         -0.040975          0.346342          0.890246         0.043492   \n",
       "\n",
       "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
       "0      -0.201268       1.282547       0.451194      1.719151   \n",
       "1       0.312603       0.103549       0.737649     -0.321950   \n",
       "2       1.083410       1.917392       1.346367      0.651193   \n",
       "3      -0.458204      -0.440604      -0.241074     -0.343145   \n",
       "4       2.368087       1.463931       1.346367     -0.225019   \n",
       "\n",
       "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('pwat', 'amax')  \\\n",
       "0          -0.762231          -0.971225  ...          0.812708   \n",
       "1          -0.444023           0.462410  ...         -0.383718   \n",
       "2          -1.612240          -0.190176  ...          0.120658   \n",
       "3          -0.723000           0.151532  ...         -0.172584   \n",
       "4          -0.099660           1.675090  ...          0.754060   \n",
       "\n",
       "   ('pwat', 'mean')  ('pwat', 'var')  ('paramId_0', 'amin')  \\\n",
       "0          0.242401         1.059034                    0.0   \n",
       "1          0.151094        -0.622044                    0.0   \n",
       "2          0.371361        -0.104245                    0.0   \n",
       "3         -0.232960        -0.216922                    0.0   \n",
       "4          0.513969        -0.171086                    0.0   \n",
       "\n",
       "   ('paramId_0', 'amax')  ('paramId_0', 'mean')  ('paramId_0', 'var')  \\\n",
       "0               1.857942               0.133474              0.540952   \n",
       "1               1.591361               0.881744              1.218727   \n",
       "2               1.280350               0.534720              0.535994   \n",
       "3              -0.763436              -0.506352             -0.486713   \n",
       "4               2.080093               4.070029              4.342167   \n",
       "\n",
       "   ('pres', 'count')  ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
       "0          -0.071051                     -1.001638              -1.598030  \n",
       "1           0.855250                     -1.001638              -1.598030  \n",
       "2           0.855250                     -1.001638              -1.598030  \n",
       "3          -0.534201                     -1.001638              -1.305326  \n",
       "4           4.097303                     -1.001638              -1.305326  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(feature_part_train_4_1.head())\n",
    "part_features_4_1_train = pd.DataFrame(scaler.fit_transform(feature_part_train_4_1))\n",
    "part_features_4_1_train.columns = feature_part_train_4_1.columns\n",
    "part_features_4_1_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
      "292               0.0              0.14           0.02900         0.002435   \n",
      "293               0.0              1.33           0.12625         0.043993   \n",
      "294               0.0              0.47           0.04800         0.011781   \n",
      "295               0.0              0.36           0.10225         0.007274   \n",
      "296               0.0              1.50           0.22900         0.187840   \n",
      "\n",
      "     ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
      "292            5.0           25.0         11.125     35.240385   \n",
      "293            8.0           53.0         25.250    174.756410   \n",
      "294            7.0           36.0         14.375     67.112179   \n",
      "295           15.0           43.0         25.250     29.730769   \n",
      "296           16.0           53.0         30.025    115.768590   \n",
      "\n",
      "     ('tozne', 'amin')  ('tozne', 'amax')  ...  ('pwat', 'amax')  \\\n",
      "292         246.800003         289.600006  ...         17.900000   \n",
      "293         243.399994         291.399994  ...         31.100000   \n",
      "294         257.000000         371.100006  ...         24.700001   \n",
      "295         278.200012         397.899994  ...         22.799999   \n",
      "296         265.700012         381.100006  ...         35.400002   \n",
      "\n",
      "     ('pwat', 'mean')  ('pwat', 'var')  ('paramId_0', 'amin')  \\\n",
      "292            8.0600        13.980923                    0.0   \n",
      "293           15.2250        54.741923                    0.0   \n",
      "294           10.2225        26.132044                    0.0   \n",
      "295           14.5500         7.674359                    0.0   \n",
      "296           18.6275        57.461017                    0.0   \n",
      "\n",
      "     ('paramId_0', 'amax')  ('paramId_0', 'mean')  ('paramId_0', 'var')  \\\n",
      "292                    0.0                   0.00              0.000000   \n",
      "293                    0.0                   0.00              0.000000   \n",
      "294                   23.0                   0.60             13.220513   \n",
      "295                   63.0                  10.15            389.976923   \n",
      "296                   11.0                   0.45              4.151282   \n",
      "\n",
      "     ('pres', 'count')  ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
      "292                  0                             0                      1  \n",
      "293                  0                             0                      1  \n",
      "294                  1                             0                      2  \n",
      "295                  9                             0                      2  \n",
      "296                  2                             0                      2  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>('cwat', 'amin')</th>\n",
       "      <th>('cwat', 'amax')</th>\n",
       "      <th>('cwat', 'mean')</th>\n",
       "      <th>('cwat', 'var')</th>\n",
       "      <th>('r', 'amin')</th>\n",
       "      <th>('r', 'amax')</th>\n",
       "      <th>('r', 'mean')</th>\n",
       "      <th>('r', 'var')</th>\n",
       "      <th>('tozne', 'amin')</th>\n",
       "      <th>('tozne', 'amax')</th>\n",
       "      <th>...</th>\n",
       "      <th>('pwat', 'amax')</th>\n",
       "      <th>('pwat', 'mean')</th>\n",
       "      <th>('pwat', 'var')</th>\n",
       "      <th>('paramId_0', 'amin')</th>\n",
       "      <th>('paramId_0', 'amax')</th>\n",
       "      <th>('paramId_0', 'mean')</th>\n",
       "      <th>('paramId_0', 'var')</th>\n",
       "      <th>('pres', 'count')</th>\n",
       "      <th>('macro_season', '&lt;lambda&gt;')</th>\n",
       "      <th>('month', '&lt;lambda&gt;')</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.041152</td>\n",
       "      <td>-0.702416</td>\n",
       "      <td>-0.403062</td>\n",
       "      <td>-0.443660</td>\n",
       "      <td>-0.972106</td>\n",
       "      <td>-0.802825</td>\n",
       "      <td>-1.219024</td>\n",
       "      <td>-0.334857</td>\n",
       "      <td>-1.489672</td>\n",
       "      <td>-1.347812</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.430005</td>\n",
       "      <td>-0.813672</td>\n",
       "      <td>-0.267856</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.762933</td>\n",
       "      <td>-0.505970</td>\n",
       "      <td>-0.486215</td>\n",
       "      <td>-0.534171</td>\n",
       "      <td>-1.002007</td>\n",
       "      <td>-1.597930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.041152</td>\n",
       "      <td>1.565330</td>\n",
       "      <td>1.062847</td>\n",
       "      <td>0.445669</td>\n",
       "      <td>-0.200132</td>\n",
       "      <td>1.736736</td>\n",
       "      <td>1.029579</td>\n",
       "      <td>2.886804</td>\n",
       "      <td>-1.637914</td>\n",
       "      <td>-1.301574</td>\n",
       "      <td>...</td>\n",
       "      <td>1.120129</td>\n",
       "      <td>0.537959</td>\n",
       "      <td>2.014389</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.762933</td>\n",
       "      <td>-0.505970</td>\n",
       "      <td>-0.486215</td>\n",
       "      <td>-0.534171</td>\n",
       "      <td>-1.002007</td>\n",
       "      <td>-1.597930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.041152</td>\n",
       "      <td>-0.073545</td>\n",
       "      <td>-0.116663</td>\n",
       "      <td>-0.243668</td>\n",
       "      <td>-0.457457</td>\n",
       "      <td>0.194860</td>\n",
       "      <td>-0.701647</td>\n",
       "      <td>0.401116</td>\n",
       "      <td>-1.044947</td>\n",
       "      <td>0.745730</td>\n",
       "      <td>...</td>\n",
       "      <td>0.368549</td>\n",
       "      <td>-0.405730</td>\n",
       "      <td>0.412496</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.259355</td>\n",
       "      <td>-0.245638</td>\n",
       "      <td>-0.330102</td>\n",
       "      <td>-0.070833</td>\n",
       "      <td>-1.002007</td>\n",
       "      <td>-1.305307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.041152</td>\n",
       "      <td>-0.283169</td>\n",
       "      <td>0.701080</td>\n",
       "      <td>-0.340098</td>\n",
       "      <td>1.601140</td>\n",
       "      <td>0.829750</td>\n",
       "      <td>1.029579</td>\n",
       "      <td>-0.462084</td>\n",
       "      <td>-0.120614</td>\n",
       "      <td>1.434158</td>\n",
       "      <td>...</td>\n",
       "      <td>0.145423</td>\n",
       "      <td>0.410625</td>\n",
       "      <td>-0.620967</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.037246</td>\n",
       "      <td>3.897989</td>\n",
       "      <td>4.118801</td>\n",
       "      <td>3.635867</td>\n",
       "      <td>-1.002007</td>\n",
       "      <td>-1.305307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.041152</td>\n",
       "      <td>1.889293</td>\n",
       "      <td>2.611661</td>\n",
       "      <td>3.523915</td>\n",
       "      <td>1.858464</td>\n",
       "      <td>1.736736</td>\n",
       "      <td>1.789726</td>\n",
       "      <td>1.524675</td>\n",
       "      <td>-0.665621</td>\n",
       "      <td>1.002606</td>\n",
       "      <td>...</td>\n",
       "      <td>1.625097</td>\n",
       "      <td>1.179819</td>\n",
       "      <td>2.166634</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.274013</td>\n",
       "      <td>-0.310721</td>\n",
       "      <td>-0.437195</td>\n",
       "      <td>0.392504</td>\n",
       "      <td>-1.002007</td>\n",
       "      <td>-1.305307</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
       "0         -0.041152         -0.702416         -0.403062        -0.443660   \n",
       "1         -0.041152          1.565330          1.062847         0.445669   \n",
       "2         -0.041152         -0.073545         -0.116663        -0.243668   \n",
       "3         -0.041152         -0.283169          0.701080        -0.340098   \n",
       "4         -0.041152          1.889293          2.611661         3.523915   \n",
       "\n",
       "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
       "0      -0.972106      -0.802825      -1.219024     -0.334857   \n",
       "1      -0.200132       1.736736       1.029579      2.886804   \n",
       "2      -0.457457       0.194860      -0.701647      0.401116   \n",
       "3       1.601140       0.829750       1.029579     -0.462084   \n",
       "4       1.858464       1.736736       1.789726      1.524675   \n",
       "\n",
       "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('pwat', 'amax')  \\\n",
       "0          -1.489672          -1.347812  ...         -0.430005   \n",
       "1          -1.637914          -1.301574  ...          1.120129   \n",
       "2          -1.044947           0.745730  ...          0.368549   \n",
       "3          -0.120614           1.434158  ...          0.145423   \n",
       "4          -0.665621           1.002606  ...          1.625097   \n",
       "\n",
       "   ('pwat', 'mean')  ('pwat', 'var')  ('paramId_0', 'amin')  \\\n",
       "0         -0.813672        -0.267856                    0.0   \n",
       "1          0.537959         2.014389                    0.0   \n",
       "2         -0.405730         0.412496                    0.0   \n",
       "3          0.410625        -0.620967                    0.0   \n",
       "4          1.179819         2.166634                    0.0   \n",
       "\n",
       "   ('paramId_0', 'amax')  ('paramId_0', 'mean')  ('paramId_0', 'var')  \\\n",
       "0              -0.762933              -0.505970             -0.486215   \n",
       "1              -0.762933              -0.505970             -0.486215   \n",
       "2               0.259355              -0.245638             -0.330102   \n",
       "3               2.037246               3.897989              4.118801   \n",
       "4              -0.274013              -0.310721             -0.437195   \n",
       "\n",
       "   ('pres', 'count')  ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
       "0          -0.534171                     -1.002007              -1.597930  \n",
       "1          -0.534171                     -1.002007              -1.597930  \n",
       "2          -0.070833                     -1.002007              -1.305307  \n",
       "3           3.635867                     -1.002007              -1.305307  \n",
       "4           0.392504                     -1.002007              -1.305307  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tenday_no_null_test.head())\n",
    "tenday_part_features_test = pd.DataFrame(scaler.transform(tenday_no_null_test))\n",
    "tenday_part_features_test.columns = tenday_no_null_test.columns\n",
    "tenday_part_features_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.6611\n",
      "Test set accuracy: 0.6749\n",
      "Training Set Precision: 0.3376\n",
      "Training Set Recall: 0.7221\n",
      "Training Set F1 Score: 0.4601\n",
      "Test Set Precision: 0.2564\n",
      "Test Set Recall: 0.6841\n",
      "Test Set F1 Score: 0.373\n"
     ]
    }
   ],
   "source": [
    "clf_log_10day_part = LogisticRegression(class_weight = \"balanced\")\n",
    "clf_log_10day_part.fit(part_features_4_1_train, target_train_4_1)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_log_10day_part.score(part_features_4_1_train, \n",
    "                                                       target_train_4_1), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_log_10day_part.score(tenday_part_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_log = clf_log_10day_part.predict(part_features_4_1_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(target_train_4_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(target_train_4_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(target_train_4_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_log = clf_log_10day_part.predict(tenday_part_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_log), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_log), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_log), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.801\n",
      "Test set accuracy: 0.8529\n",
      "Training Set Precision: 0.5174\n",
      "Training Set Recall: 0.0736\n",
      "Training Set F1 Score: 0.1288\n",
      "Test Set Precision: 0.4014\n",
      "Test Set Recall: 0.0825\n",
      "Test Set F1 Score: 0.1368\n"
     ]
    }
   ],
   "source": [
    "clf_log_10day_part = LogisticRegression()\n",
    "clf_log_10day_part.fit(part_features_4_1_train, target_train_4_1)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_log_10day_part.score(part_features_4_1_train, \n",
    "                                                       target_train_4_1), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_log_10day_part.score(tenday_part_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_log = clf_log_10day_part.predict(part_features_4_1_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(target_train_4_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(target_train_4_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(target_train_4_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_log = clf_log_10day_part.predict(tenday_part_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_log), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_log), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_log), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.6486\n",
      "Test set accuracy: 0.6665\n",
      "Training Set Precision: 0.3291\n",
      "Training Set Recall: 0.7288\n",
      "Training Set F1 Score: 0.4534\n",
      "Test Set Precision: 0.2516\n",
      "Test Set Recall: 0.6885\n",
      "Test Set F1 Score: 0.3685\n"
     ]
    }
   ],
   "source": [
    "clf_sgd_10day_part = SGDClassifier(loss = \"log\", class_weight = \"balanced\", random_state = 0)\n",
    "clf_sgd_10day_part.fit(part_features_4_1_train, target_train_4_1)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_sgd_10day_part.score(part_features_4_1_train, \n",
    "                                                       target_train_4_1), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_sgd_10day_part.score(tenday_part_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_sgd = clf_sgd_10day_part.predict(part_features_4_1_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(target_train_4_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(target_train_4_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(target_train_4_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_sgd = clf_sgd_10day_part.predict(tenday_part_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_sgd), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_sgd), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_sgd), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.7972\n",
      "Test set accuracy: 0.8493\n",
      "Training Set Precision: 0.4729\n",
      "Training Set Recall: 0.1214\n",
      "Training Set F1 Score: 0.1932\n",
      "Test Set Precision: 0.393\n",
      "Test Set Recall: 0.1216\n",
      "Test Set F1 Score: 0.1857\n"
     ]
    }
   ],
   "source": [
    "clf_sgd_10day_part = SGDClassifier(loss = \"log\", random_state = 0)\n",
    "clf_sgd_10day_part.fit(part_features_4_1_train, target_train_4_1)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_sgd_10day_part.score(part_features_4_1_train, \n",
    "                                                       target_train_4_1), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_sgd_10day_part.score(tenday_part_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_sgd = clf_sgd_10day_part.predict(part_features_4_1_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(target_train_4_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(target_train_4_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(target_train_4_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_sgd = clf_sgd_10day_part.predict(tenday_part_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_sgd), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_sgd), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_sgd), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Under Sampling Using Only 10-Day Average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1:1 Ratio Majority To Minority  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Full Parameter List "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    23701\n",
       "0    23701\n",
       "Name: fire, dtype: int64"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "under_sampler_1_1 = RandomUnderSampler(random_state = 0)\n",
    "feature_full_train_1_1, target_train_1_1 = under_sampler_1_1.fit_resample(tenday_full_train, tenday_target_train)\n",
    "target_train_1_1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 47402 entries, 0 to 47401\n",
      "Data columns (total 31 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   ('cwat', 'amin')              47402 non-null  float64\n",
      " 1   ('cwat', 'amax')              47402 non-null  float64\n",
      " 2   ('cwat', 'mean')              47402 non-null  float64\n",
      " 3   ('cwat', 'var')               47402 non-null  float64\n",
      " 4   ('r', 'amin')                 47402 non-null  float64\n",
      " 5   ('r', 'amax')                 47402 non-null  float64\n",
      " 6   ('r', 'mean')                 47402 non-null  float64\n",
      " 7   ('r', 'var')                  47402 non-null  float64\n",
      " 8   ('tozne', 'amin')             47402 non-null  float64\n",
      " 9   ('tozne', 'amax')             47402 non-null  float64\n",
      " 10  ('tozne', 'mean')             47402 non-null  float64\n",
      " 11  ('tozne', 'var')              47402 non-null  float64\n",
      " 12  ('gh', 'amin')                47402 non-null  float64\n",
      " 13  ('gh', 'amax')                47402 non-null  float64\n",
      " 14  ('gh', 'mean')                47402 non-null  float64\n",
      " 15  ('gh', 'var')                 47402 non-null  float64\n",
      " 16  ('pwat', 'amin')              47402 non-null  float64\n",
      " 17  ('pwat', 'amax')              47402 non-null  float64\n",
      " 18  ('pwat', 'mean')              47402 non-null  float64\n",
      " 19  ('pwat', 'var')               47402 non-null  float64\n",
      " 20  ('paramId_0', 'amin')         47402 non-null  float64\n",
      " 21  ('paramId_0', 'amax')         47402 non-null  float64\n",
      " 22  ('paramId_0', 'mean')         47402 non-null  float64\n",
      " 23  ('paramId_0', 'var')          47402 non-null  float64\n",
      " 24  ('pres', 'amin')              16638 non-null  float64\n",
      " 25  ('pres', 'amax')              16638 non-null  float64\n",
      " 26  ('pres', 'mean')              16638 non-null  float64\n",
      " 27  ('pres', 'var')               10102 non-null  float64\n",
      " 28  ('pres', 'count')             47402 non-null  int64  \n",
      " 29  ('macro_season', '<lambda>')  47402 non-null  int64  \n",
      " 30  ('month', '<lambda>')         47402 non-null  int64  \n",
      "dtypes: float64(28), int64(3)\n",
      "memory usage: 11.2 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 47402 entries, 0 to 47401\n",
      "Data columns (total 31 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   ('cwat', 'amin')              47402 non-null  float64\n",
      " 1   ('cwat', 'amax')              47402 non-null  float64\n",
      " 2   ('cwat', 'mean')              47402 non-null  float64\n",
      " 3   ('cwat', 'var')               47402 non-null  float64\n",
      " 4   ('r', 'amin')                 47402 non-null  float64\n",
      " 5   ('r', 'amax')                 47402 non-null  float64\n",
      " 6   ('r', 'mean')                 47402 non-null  float64\n",
      " 7   ('r', 'var')                  47402 non-null  float64\n",
      " 8   ('tozne', 'amin')             47402 non-null  float64\n",
      " 9   ('tozne', 'amax')             47402 non-null  float64\n",
      " 10  ('tozne', 'mean')             47402 non-null  float64\n",
      " 11  ('tozne', 'var')              47402 non-null  float64\n",
      " 12  ('gh', 'amin')                47402 non-null  float64\n",
      " 13  ('gh', 'amax')                47402 non-null  float64\n",
      " 14  ('gh', 'mean')                47402 non-null  float64\n",
      " 15  ('gh', 'var')                 47402 non-null  float64\n",
      " 16  ('pwat', 'amin')              47402 non-null  float64\n",
      " 17  ('pwat', 'amax')              47402 non-null  float64\n",
      " 18  ('pwat', 'mean')              47402 non-null  float64\n",
      " 19  ('pwat', 'var')               47402 non-null  float64\n",
      " 20  ('paramId_0', 'amin')         47402 non-null  float64\n",
      " 21  ('paramId_0', 'amax')         47402 non-null  float64\n",
      " 22  ('paramId_0', 'mean')         47402 non-null  float64\n",
      " 23  ('paramId_0', 'var')          47402 non-null  float64\n",
      " 24  ('pres', 'amin')              47402 non-null  float64\n",
      " 25  ('pres', 'amax')              47402 non-null  float64\n",
      " 26  ('pres', 'mean')              47402 non-null  float64\n",
      " 27  ('pres', 'var')               47402 non-null  float64\n",
      " 28  ('pres', 'count')             47402 non-null  float64\n",
      " 29  ('macro_season', '<lambda>')  47402 non-null  float64\n",
      " 30  ('month', '<lambda>')         47402 non-null  float64\n",
      "dtypes: float64(31)\n",
      "memory usage: 11.2 MB\n"
     ]
    }
   ],
   "source": [
    "feature_full_train_1_1.info()\n",
    "feature_imp_train_1_1 = pd.DataFrame(imp.fit_transform(feature_full_train_1_1))\n",
    "feature_imp_train_1_1.columns = feature_full_train_1_1.columns\n",
    "feature_imp_train_1_1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 35259 entries, 292 to 176294\n",
      "Data columns (total 31 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   ('cwat', 'amin')              35259 non-null  float64\n",
      " 1   ('cwat', 'amax')              35259 non-null  float64\n",
      " 2   ('cwat', 'mean')              35259 non-null  float64\n",
      " 3   ('cwat', 'var')               35259 non-null  float64\n",
      " 4   ('r', 'amin')                 35259 non-null  float64\n",
      " 5   ('r', 'amax')                 35259 non-null  float64\n",
      " 6   ('r', 'mean')                 35259 non-null  float64\n",
      " 7   ('r', 'var')                  35259 non-null  float64\n",
      " 8   ('tozne', 'amin')             35259 non-null  float64\n",
      " 9   ('tozne', 'amax')             35259 non-null  float64\n",
      " 10  ('tozne', 'mean')             35259 non-null  float64\n",
      " 11  ('tozne', 'var')              35259 non-null  float64\n",
      " 12  ('gh', 'amin')                35259 non-null  float64\n",
      " 13  ('gh', 'amax')                35259 non-null  float64\n",
      " 14  ('gh', 'mean')                35259 non-null  float64\n",
      " 15  ('gh', 'var')                 35259 non-null  float64\n",
      " 16  ('pwat', 'amin')              35259 non-null  float64\n",
      " 17  ('pwat', 'amax')              35259 non-null  float64\n",
      " 18  ('pwat', 'mean')              35259 non-null  float64\n",
      " 19  ('pwat', 'var')               35259 non-null  float64\n",
      " 20  ('paramId_0', 'amin')         35259 non-null  float64\n",
      " 21  ('paramId_0', 'amax')         35259 non-null  float64\n",
      " 22  ('paramId_0', 'mean')         35259 non-null  float64\n",
      " 23  ('paramId_0', 'var')          35259 non-null  float64\n",
      " 24  ('pres', 'amin')              15069 non-null  float64\n",
      " 25  ('pres', 'amax')              15069 non-null  float64\n",
      " 26  ('pres', 'mean')              15069 non-null  float64\n",
      " 27  ('pres', 'var')               9461 non-null   float64\n",
      " 28  ('pres', 'count')             35259 non-null  int64  \n",
      " 29  ('macro_season', '<lambda>')  35259 non-null  int64  \n",
      " 30  ('month', '<lambda>')         35259 non-null  int64  \n",
      "dtypes: float64(28), int64(3)\n",
      "memory usage: 8.6 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 35259 entries, 0 to 35258\n",
      "Data columns (total 31 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   ('cwat', 'amin')              35259 non-null  float64\n",
      " 1   ('cwat', 'amax')              35259 non-null  float64\n",
      " 2   ('cwat', 'mean')              35259 non-null  float64\n",
      " 3   ('cwat', 'var')               35259 non-null  float64\n",
      " 4   ('r', 'amin')                 35259 non-null  float64\n",
      " 5   ('r', 'amax')                 35259 non-null  float64\n",
      " 6   ('r', 'mean')                 35259 non-null  float64\n",
      " 7   ('r', 'var')                  35259 non-null  float64\n",
      " 8   ('tozne', 'amin')             35259 non-null  float64\n",
      " 9   ('tozne', 'amax')             35259 non-null  float64\n",
      " 10  ('tozne', 'mean')             35259 non-null  float64\n",
      " 11  ('tozne', 'var')              35259 non-null  float64\n",
      " 12  ('gh', 'amin')                35259 non-null  float64\n",
      " 13  ('gh', 'amax')                35259 non-null  float64\n",
      " 14  ('gh', 'mean')                35259 non-null  float64\n",
      " 15  ('gh', 'var')                 35259 non-null  float64\n",
      " 16  ('pwat', 'amin')              35259 non-null  float64\n",
      " 17  ('pwat', 'amax')              35259 non-null  float64\n",
      " 18  ('pwat', 'mean')              35259 non-null  float64\n",
      " 19  ('pwat', 'var')               35259 non-null  float64\n",
      " 20  ('paramId_0', 'amin')         35259 non-null  float64\n",
      " 21  ('paramId_0', 'amax')         35259 non-null  float64\n",
      " 22  ('paramId_0', 'mean')         35259 non-null  float64\n",
      " 23  ('paramId_0', 'var')          35259 non-null  float64\n",
      " 24  ('pres', 'amin')              35259 non-null  float64\n",
      " 25  ('pres', 'amax')              35259 non-null  float64\n",
      " 26  ('pres', 'mean')              35259 non-null  float64\n",
      " 27  ('pres', 'var')               35259 non-null  float64\n",
      " 28  ('pres', 'count')             35259 non-null  float64\n",
      " 29  ('macro_season', '<lambda>')  35259 non-null  float64\n",
      " 30  ('month', '<lambda>')         35259 non-null  float64\n",
      "dtypes: float64(31)\n",
      "memory usage: 8.3 MB\n"
     ]
    }
   ],
   "source": [
    "tenday_full_test.info()\n",
    "tenday_imp_test = pd.DataFrame(imp.fit_transform(tenday_full_test))\n",
    "tenday_imp_test.columns = tenday_full_test.columns\n",
    "tenday_imp_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
      "0               0.0              0.22           0.02125         0.002114   \n",
      "1               0.0              0.19           0.00950         0.001020   \n",
      "2               0.0              0.04           0.00225         0.000054   \n",
      "3               0.0              0.64           0.08150         0.014921   \n",
      "4               0.0              0.10           0.00400         0.000286   \n",
      "\n",
      "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
      "0           23.0           46.0          31.40     38.041026   \n",
      "1           10.0           30.0          17.75     26.397436   \n",
      "2            5.0           33.0          14.80     33.548718   \n",
      "3           10.0           31.0          17.45     22.510256   \n",
      "4            3.0           15.0           9.80      7.805128   \n",
      "\n",
      "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('paramId_0', 'amax')  \\\n",
      "0         311.200012         370.000000  ...                   68.0   \n",
      "1         309.799988         345.200012  ...                    0.0   \n",
      "2         255.600006         307.100006  ...                   38.0   \n",
      "3         278.399994         381.399994  ...                   37.0   \n",
      "4         288.100006         319.100006  ...                    0.0   \n",
      "\n",
      "   ('paramId_0', 'mean')  ('paramId_0', 'var')  ('pres', 'amin')  \\\n",
      "0                  4.475            257.537821           34770.0   \n",
      "1                  0.000              0.000000           41420.0   \n",
      "2                  0.950             36.100000           59460.0   \n",
      "3                  1.625             39.625000           43190.0   \n",
      "4                  0.000              0.000000           41420.0   \n",
      "\n",
      "   ('pres', 'amax')  ('pres', 'mean')  ('pres', 'var')  ('pres', 'count')  \\\n",
      "0           69680.0      47496.666667     3.717484e+08                3.0   \n",
      "1           53990.0      48854.166667     4.250867e+07                0.0   \n",
      "2           59460.0      59460.000000     4.250867e+07                1.0   \n",
      "3           43190.0      43190.000000     4.250867e+07                1.0   \n",
      "4           53990.0      48854.166667     4.250867e+07                0.0   \n",
      "\n",
      "   ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
      "0                           0.0                    2.0  \n",
      "1                           0.0                    6.0  \n",
      "2                           1.0                   12.0  \n",
      "3                           0.0                    4.0  \n",
      "4                           0.0                    6.0  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>('cwat', 'amin')</th>\n",
       "      <th>('cwat', 'amax')</th>\n",
       "      <th>('cwat', 'mean')</th>\n",
       "      <th>('cwat', 'var')</th>\n",
       "      <th>('r', 'amin')</th>\n",
       "      <th>('r', 'amax')</th>\n",
       "      <th>('r', 'mean')</th>\n",
       "      <th>('r', 'var')</th>\n",
       "      <th>('tozne', 'amin')</th>\n",
       "      <th>('tozne', 'amax')</th>\n",
       "      <th>...</th>\n",
       "      <th>('paramId_0', 'amax')</th>\n",
       "      <th>('paramId_0', 'mean')</th>\n",
       "      <th>('paramId_0', 'var')</th>\n",
       "      <th>('pres', 'amin')</th>\n",
       "      <th>('pres', 'amax')</th>\n",
       "      <th>('pres', 'mean')</th>\n",
       "      <th>('pres', 'var')</th>\n",
       "      <th>('pres', 'count')</th>\n",
       "      <th>('macro_season', '&lt;lambda&gt;')</th>\n",
       "      <th>('month', '&lt;lambda&gt;')</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.031515</td>\n",
       "      <td>-0.478358</td>\n",
       "      <td>-0.424763</td>\n",
       "      <td>-0.414106</td>\n",
       "      <td>3.911431</td>\n",
       "      <td>1.233643</td>\n",
       "      <td>2.210879</td>\n",
       "      <td>-0.213654</td>\n",
       "      <td>1.326818</td>\n",
       "      <td>0.826689</td>\n",
       "      <td>...</td>\n",
       "      <td>2.429934</td>\n",
       "      <td>1.638823</td>\n",
       "      <td>2.836338</td>\n",
       "      <td>-0.930159</td>\n",
       "      <td>1.802726</td>\n",
       "      <td>-0.123433</td>\n",
       "      <td>6.708396</td>\n",
       "      <td>1.015835</td>\n",
       "      <td>-1.089886</td>\n",
       "      <td>-1.499174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.031515</td>\n",
       "      <td>-0.537389</td>\n",
       "      <td>-0.618127</td>\n",
       "      <td>-0.439263</td>\n",
       "      <td>0.426191</td>\n",
       "      <td>-0.260262</td>\n",
       "      <td>-0.057979</td>\n",
       "      <td>-0.493341</td>\n",
       "      <td>1.262694</td>\n",
       "      <td>0.163356</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.708816</td>\n",
       "      <td>-0.477678</td>\n",
       "      <td>-0.453327</td>\n",
       "      <td>-0.073384</td>\n",
       "      <td>0.067066</td>\n",
       "      <td>0.050575</td>\n",
       "      <td>-0.154545</td>\n",
       "      <td>-0.500607</td>\n",
       "      <td>-1.089886</td>\n",
       "      <td>-0.214630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.031515</td>\n",
       "      <td>-0.832547</td>\n",
       "      <td>-0.737437</td>\n",
       "      <td>-0.461497</td>\n",
       "      <td>-0.914285</td>\n",
       "      <td>0.019845</td>\n",
       "      <td>-0.548318</td>\n",
       "      <td>-0.321563</td>\n",
       "      <td>-1.219759</td>\n",
       "      <td>-0.855715</td>\n",
       "      <td>...</td>\n",
       "      <td>1.045191</td>\n",
       "      <td>-0.028365</td>\n",
       "      <td>0.007797</td>\n",
       "      <td>2.250861</td>\n",
       "      <td>0.672169</td>\n",
       "      <td>1.410055</td>\n",
       "      <td>-0.154545</td>\n",
       "      <td>0.004873</td>\n",
       "      <td>0.917527</td>\n",
       "      <td>1.712185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.031515</td>\n",
       "      <td>0.348083</td>\n",
       "      <td>0.566744</td>\n",
       "      <td>-0.119478</td>\n",
       "      <td>0.426191</td>\n",
       "      <td>-0.166893</td>\n",
       "      <td>-0.107844</td>\n",
       "      <td>-0.586714</td>\n",
       "      <td>-0.175480</td>\n",
       "      <td>1.131608</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999033</td>\n",
       "      <td>0.290884</td>\n",
       "      <td>0.052824</td>\n",
       "      <td>0.154660</td>\n",
       "      <td>-1.127651</td>\n",
       "      <td>-0.675472</td>\n",
       "      <td>-0.154545</td>\n",
       "      <td>0.004873</td>\n",
       "      <td>-1.089886</td>\n",
       "      <td>-0.856902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.031515</td>\n",
       "      <td>-0.714484</td>\n",
       "      <td>-0.708638</td>\n",
       "      <td>-0.456151</td>\n",
       "      <td>-1.450476</td>\n",
       "      <td>-1.660798</td>\n",
       "      <td>-1.379402</td>\n",
       "      <td>-0.939942</td>\n",
       "      <td>0.268797</td>\n",
       "      <td>-0.534748</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.708816</td>\n",
       "      <td>-0.477678</td>\n",
       "      <td>-0.453327</td>\n",
       "      <td>-0.073384</td>\n",
       "      <td>0.067066</td>\n",
       "      <td>0.050575</td>\n",
       "      <td>-0.154545</td>\n",
       "      <td>-0.500607</td>\n",
       "      <td>-1.089886</td>\n",
       "      <td>-0.214630</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
       "0         -0.031515         -0.478358         -0.424763        -0.414106   \n",
       "1         -0.031515         -0.537389         -0.618127        -0.439263   \n",
       "2         -0.031515         -0.832547         -0.737437        -0.461497   \n",
       "3         -0.031515          0.348083          0.566744        -0.119478   \n",
       "4         -0.031515         -0.714484         -0.708638        -0.456151   \n",
       "\n",
       "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
       "0       3.911431       1.233643       2.210879     -0.213654   \n",
       "1       0.426191      -0.260262      -0.057979     -0.493341   \n",
       "2      -0.914285       0.019845      -0.548318     -0.321563   \n",
       "3       0.426191      -0.166893      -0.107844     -0.586714   \n",
       "4      -1.450476      -1.660798      -1.379402     -0.939942   \n",
       "\n",
       "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('paramId_0', 'amax')  \\\n",
       "0           1.326818           0.826689  ...               2.429934   \n",
       "1           1.262694           0.163356  ...              -0.708816   \n",
       "2          -1.219759          -0.855715  ...               1.045191   \n",
       "3          -0.175480           1.131608  ...               0.999033   \n",
       "4           0.268797          -0.534748  ...              -0.708816   \n",
       "\n",
       "   ('paramId_0', 'mean')  ('paramId_0', 'var')  ('pres', 'amin')  \\\n",
       "0               1.638823              2.836338         -0.930159   \n",
       "1              -0.477678             -0.453327         -0.073384   \n",
       "2              -0.028365              0.007797          2.250861   \n",
       "3               0.290884              0.052824          0.154660   \n",
       "4              -0.477678             -0.453327         -0.073384   \n",
       "\n",
       "   ('pres', 'amax')  ('pres', 'mean')  ('pres', 'var')  ('pres', 'count')  \\\n",
       "0          1.802726         -0.123433         6.708396           1.015835   \n",
       "1          0.067066          0.050575        -0.154545          -0.500607   \n",
       "2          0.672169          1.410055        -0.154545           0.004873   \n",
       "3         -1.127651         -0.675472        -0.154545           0.004873   \n",
       "4          0.067066          0.050575        -0.154545          -0.500607   \n",
       "\n",
       "   ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
       "0                     -1.089886              -1.499174  \n",
       "1                     -1.089886              -0.214630  \n",
       "2                      0.917527               1.712185  \n",
       "3                     -1.089886              -0.856902  \n",
       "4                     -1.089886              -0.214630  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(feature_imp_train_1_1.head())\n",
    "full_features_1_1_train = pd.DataFrame(scaler.fit_transform(feature_imp_train_1_1))\n",
    "full_features_1_1_train.columns = feature_imp_train_1_1.columns\n",
    "full_features_1_1_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>('cwat', 'amin')</th>\n",
       "      <th>('cwat', 'amax')</th>\n",
       "      <th>('cwat', 'mean')</th>\n",
       "      <th>('cwat', 'var')</th>\n",
       "      <th>('r', 'amin')</th>\n",
       "      <th>('r', 'amax')</th>\n",
       "      <th>('r', 'mean')</th>\n",
       "      <th>('r', 'var')</th>\n",
       "      <th>('tozne', 'amin')</th>\n",
       "      <th>('tozne', 'amax')</th>\n",
       "      <th>...</th>\n",
       "      <th>('paramId_0', 'amax')</th>\n",
       "      <th>('paramId_0', 'mean')</th>\n",
       "      <th>('paramId_0', 'var')</th>\n",
       "      <th>('pres', 'amin')</th>\n",
       "      <th>('pres', 'amax')</th>\n",
       "      <th>('pres', 'mean')</th>\n",
       "      <th>('pres', 'var')</th>\n",
       "      <th>('pres', 'count')</th>\n",
       "      <th>('macro_season', '&lt;lambda&gt;')</th>\n",
       "      <th>('month', '&lt;lambda&gt;')</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.031515</td>\n",
       "      <td>-0.635775</td>\n",
       "      <td>-0.297224</td>\n",
       "      <td>-0.406720</td>\n",
       "      <td>-0.914285</td>\n",
       "      <td>-0.727107</td>\n",
       "      <td>-1.159165</td>\n",
       "      <td>-0.280927</td>\n",
       "      <td>-1.622814</td>\n",
       "      <td>-1.323793</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.708816</td>\n",
       "      <td>-0.477678</td>\n",
       "      <td>-0.453327</td>\n",
       "      <td>0.094106</td>\n",
       "      <td>0.202025</td>\n",
       "      <td>0.182068</td>\n",
       "      <td>0.028205</td>\n",
       "      <td>-0.500607</td>\n",
       "      <td>-1.089886</td>\n",
       "      <td>-1.820310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.031515</td>\n",
       "      <td>1.705807</td>\n",
       "      <td>1.303174</td>\n",
       "      <td>0.549344</td>\n",
       "      <td>-0.109999</td>\n",
       "      <td>1.887226</td>\n",
       "      <td>1.188646</td>\n",
       "      <td>3.070347</td>\n",
       "      <td>-1.778541</td>\n",
       "      <td>-1.275648</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.708816</td>\n",
       "      <td>-0.477678</td>\n",
       "      <td>-0.453327</td>\n",
       "      <td>0.094106</td>\n",
       "      <td>0.202025</td>\n",
       "      <td>0.182068</td>\n",
       "      <td>0.028205</td>\n",
       "      <td>-0.500607</td>\n",
       "      <td>-1.089886</td>\n",
       "      <td>-1.820310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.031515</td>\n",
       "      <td>0.013571</td>\n",
       "      <td>0.015450</td>\n",
       "      <td>-0.191720</td>\n",
       "      <td>-0.378095</td>\n",
       "      <td>0.299952</td>\n",
       "      <td>-0.618960</td>\n",
       "      <td>0.484656</td>\n",
       "      <td>-1.155637</td>\n",
       "      <td>0.856111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.352820</td>\n",
       "      <td>-0.193901</td>\n",
       "      <td>-0.284454</td>\n",
       "      <td>4.505532</td>\n",
       "      <td>2.608054</td>\n",
       "      <td>3.653246</td>\n",
       "      <td>0.028205</td>\n",
       "      <td>0.004873</td>\n",
       "      <td>-1.089886</td>\n",
       "      <td>-1.499174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.031515</td>\n",
       "      <td>-0.202877</td>\n",
       "      <td>0.908217</td>\n",
       "      <td>-0.295387</td>\n",
       "      <td>1.766668</td>\n",
       "      <td>0.953536</td>\n",
       "      <td>1.188646</td>\n",
       "      <td>-0.413272</td>\n",
       "      <td>-0.184640</td>\n",
       "      <td>1.572938</td>\n",
       "      <td>...</td>\n",
       "      <td>2.199144</td>\n",
       "      <td>4.322878</td>\n",
       "      <td>4.528051</td>\n",
       "      <td>-0.061788</td>\n",
       "      <td>2.704295</td>\n",
       "      <td>1.479843</td>\n",
       "      <td>3.942252</td>\n",
       "      <td>4.048719</td>\n",
       "      <td>-1.089886</td>\n",
       "      <td>-1.499174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.031515</td>\n",
       "      <td>2.040319</td>\n",
       "      <td>2.994084</td>\n",
       "      <td>3.858583</td>\n",
       "      <td>2.034763</td>\n",
       "      <td>1.887226</td>\n",
       "      <td>1.982331</td>\n",
       "      <td>1.653417</td>\n",
       "      <td>-0.757161</td>\n",
       "      <td>1.123584</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.201077</td>\n",
       "      <td>-0.264846</td>\n",
       "      <td>-0.400300</td>\n",
       "      <td>0.374974</td>\n",
       "      <td>1.785026</td>\n",
       "      <td>1.121645</td>\n",
       "      <td>5.276853</td>\n",
       "      <td>0.510354</td>\n",
       "      <td>-1.089886</td>\n",
       "      <td>-1.499174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
       "0         -0.031515         -0.635775         -0.297224        -0.406720   \n",
       "1         -0.031515          1.705807          1.303174         0.549344   \n",
       "2         -0.031515          0.013571          0.015450        -0.191720   \n",
       "3         -0.031515         -0.202877          0.908217        -0.295387   \n",
       "4         -0.031515          2.040319          2.994084         3.858583   \n",
       "\n",
       "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
       "0      -0.914285      -0.727107      -1.159165     -0.280927   \n",
       "1      -0.109999       1.887226       1.188646      3.070347   \n",
       "2      -0.378095       0.299952      -0.618960      0.484656   \n",
       "3       1.766668       0.953536       1.188646     -0.413272   \n",
       "4       2.034763       1.887226       1.982331      1.653417   \n",
       "\n",
       "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('paramId_0', 'amax')  \\\n",
       "0          -1.622814          -1.323793  ...              -0.708816   \n",
       "1          -1.778541          -1.275648  ...              -0.708816   \n",
       "2          -1.155637           0.856111  ...               0.352820   \n",
       "3          -0.184640           1.572938  ...               2.199144   \n",
       "4          -0.757161           1.123584  ...              -0.201077   \n",
       "\n",
       "   ('paramId_0', 'mean')  ('paramId_0', 'var')  ('pres', 'amin')  \\\n",
       "0              -0.477678             -0.453327          0.094106   \n",
       "1              -0.477678             -0.453327          0.094106   \n",
       "2              -0.193901             -0.284454          4.505532   \n",
       "3               4.322878              4.528051         -0.061788   \n",
       "4              -0.264846             -0.400300          0.374974   \n",
       "\n",
       "   ('pres', 'amax')  ('pres', 'mean')  ('pres', 'var')  ('pres', 'count')  \\\n",
       "0          0.202025          0.182068         0.028205          -0.500607   \n",
       "1          0.202025          0.182068         0.028205          -0.500607   \n",
       "2          2.608054          3.653246         0.028205           0.004873   \n",
       "3          2.704295          1.479843         3.942252           4.048719   \n",
       "4          1.785026          1.121645         5.276853           0.510354   \n",
       "\n",
       "   ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
       "0                     -1.089886              -1.820310  \n",
       "1                     -1.089886              -1.820310  \n",
       "2                     -1.089886              -1.499174  \n",
       "3                     -1.089886              -1.499174  \n",
       "4                     -1.089886              -1.499174  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tenday_full_features_test = pd.DataFrame(scaler.transform(tenday_imp_test))\n",
    "tenday_full_features_test.columns = tenday_imp_test.columns\n",
    "tenday_full_features_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.6835\n",
      "Test set accuracy: 0.6669\n",
      "Training Set Precision: 0.6703\n",
      "Training Set Recall: 0.7222\n",
      "Training Set F1 Score: 0.6953\n",
      "Test Set Precision: 0.2534\n",
      "Test Set Recall: 0.6976\n",
      "Test Set F1 Score: 0.3718\n"
     ]
    }
   ],
   "source": [
    "clf_log_10day_full = LogisticRegression(max_iter = 1000000)\n",
    "clf_log_10day_full.fit(full_features_1_1_train, target_train_1_1)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_log_10day_full.score(full_features_1_1_train, \n",
    "                                                       target_train_1_1), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_log_10day_full.score(tenday_full_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_log = clf_log_10day_full.predict(full_features_1_1_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(target_train_1_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(target_train_1_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(target_train_1_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_log = clf_log_10day_full.predict(tenday_full_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_log), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_log), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_log), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.6715\n",
      "Test set accuracy: 0.6283\n",
      "Training Set Precision: 0.6485\n",
      "Training Set Recall: 0.7491\n",
      "Training Set F1 Score: 0.6952\n",
      "Test Set Precision: 0.235\n",
      "Test Set Recall: 0.7227\n",
      "Test Set F1 Score: 0.3547\n"
     ]
    }
   ],
   "source": [
    "clf_sgd_10day_full = SGDClassifier(loss = \"log\", random_state = 0)\n",
    "clf_sgd_10day_full.fit(full_features_1_1_train, target_train_1_1)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_sgd_10day_full.score(full_features_1_1_train, \n",
    "                                                       target_train_1_1), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_sgd_10day_full.score(tenday_full_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_sgd = clf_sgd_10day_full.predict(full_features_1_1_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(target_train_1_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(target_train_1_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(target_train_1_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_sgd = clf_sgd_10day_full.predict(tenday_full_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_sgd), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_sgd), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_sgd), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Part Parameter List "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    23701\n",
       "0    23701\n",
       "Name: fire, dtype: int64"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_part_train_1_1, target_train_1_1 = under_sampler_1_1.fit_resample(tenday_no_null_train, tenday_target_train)\n",
    "target_train_1_1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
      "0               0.0              0.66           0.14525         0.028179   \n",
      "1               0.0              0.13           0.02750         0.001804   \n",
      "2               0.0              0.03           0.00175         0.000035   \n",
      "3               0.0              0.82           0.08350         0.026639   \n",
      "4               0.0              0.62           0.03075         0.010899   \n",
      "\n",
      "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
      "0           12.0           30.0         20.575     28.045513   \n",
      "1            6.0           25.0         15.225     26.435256   \n",
      "2           14.0           36.0         22.225     28.383974   \n",
      "3           10.0           33.0         19.025     36.076282   \n",
      "4            8.0           40.0         16.500     53.025641   \n",
      "\n",
      "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('pwat', 'amax')  \\\n",
      "0         363.899994         429.100006  ...         19.100000   \n",
      "1         250.300003         311.000000  ...          9.500000   \n",
      "2         278.500000         308.100006  ...         37.099998   \n",
      "3         283.799988         376.799988  ...         14.200000   \n",
      "4         310.600006         368.500000  ...         22.900000   \n",
      "\n",
      "   ('pwat', 'mean')  ('pwat', 'var')  ('paramId_0', 'amin')  \\\n",
      "0           11.3350        11.886948                    0.0   \n",
      "1            5.7825         4.342506                    0.0   \n",
      "2           24.1050        29.122028                    0.0   \n",
      "3            7.0100         7.497846                    0.0   \n",
      "4           10.9700        18.449845                    0.0   \n",
      "\n",
      "   ('paramId_0', 'amax')  ('paramId_0', 'mean')  ('paramId_0', 'var')  \\\n",
      "0                   47.0                  5.975            188.589103   \n",
      "1                    0.0                  0.000              0.000000   \n",
      "2                    0.0                  0.000              0.000000   \n",
      "3                    0.0                  0.000              0.000000   \n",
      "4                   12.0                  0.300              3.600000   \n",
      "\n",
      "   ('pres', 'count')  ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
      "0                  6                             0                      5  \n",
      "1                  0                             1                     11  \n",
      "2                  0                             1                      9  \n",
      "3                  0                             0                      2  \n",
      "4                  0                             0                      4  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>('cwat', 'amin')</th>\n",
       "      <th>('cwat', 'amax')</th>\n",
       "      <th>('cwat', 'mean')</th>\n",
       "      <th>('cwat', 'var')</th>\n",
       "      <th>('r', 'amin')</th>\n",
       "      <th>('r', 'amax')</th>\n",
       "      <th>('r', 'mean')</th>\n",
       "      <th>('r', 'var')</th>\n",
       "      <th>('tozne', 'amin')</th>\n",
       "      <th>('tozne', 'amax')</th>\n",
       "      <th>...</th>\n",
       "      <th>('pwat', 'amax')</th>\n",
       "      <th>('pwat', 'mean')</th>\n",
       "      <th>('pwat', 'var')</th>\n",
       "      <th>('paramId_0', 'amin')</th>\n",
       "      <th>('paramId_0', 'amax')</th>\n",
       "      <th>('paramId_0', 'mean')</th>\n",
       "      <th>('paramId_0', 'var')</th>\n",
       "      <th>('pres', 'count')</th>\n",
       "      <th>('macro_season', '&lt;lambda&gt;')</th>\n",
       "      <th>('month', '&lt;lambda&gt;')</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.032523</td>\n",
       "      <td>0.387024</td>\n",
       "      <td>1.598506</td>\n",
       "      <td>0.185013</td>\n",
       "      <td>0.956767</td>\n",
       "      <td>-0.256437</td>\n",
       "      <td>0.416602</td>\n",
       "      <td>-0.449404</td>\n",
       "      <td>3.732772</td>\n",
       "      <td>2.399478</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.315513</td>\n",
       "      <td>-0.224167</td>\n",
       "      <td>-0.397848</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.458698</td>\n",
       "      <td>2.385414</td>\n",
       "      <td>1.982489</td>\n",
       "      <td>2.569801</td>\n",
       "      <td>-1.082917</td>\n",
       "      <td>-0.525873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.032523</td>\n",
       "      <td>-0.653222</td>\n",
       "      <td>-0.320893</td>\n",
       "      <td>-0.421579</td>\n",
       "      <td>-0.645763</td>\n",
       "      <td>-0.724440</td>\n",
       "      <td>-0.475116</td>\n",
       "      <td>-0.488313</td>\n",
       "      <td>-1.456546</td>\n",
       "      <td>-0.749712</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.492243</td>\n",
       "      <td>-1.305418</td>\n",
       "      <td>-0.835899</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.710802</td>\n",
       "      <td>-0.478394</td>\n",
       "      <td>-0.454123</td>\n",
       "      <td>-0.502204</td>\n",
       "      <td>0.923432</td>\n",
       "      <td>1.395587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.032523</td>\n",
       "      <td>-0.849495</td>\n",
       "      <td>-0.740634</td>\n",
       "      <td>-0.462252</td>\n",
       "      <td>1.490943</td>\n",
       "      <td>0.305166</td>\n",
       "      <td>0.691617</td>\n",
       "      <td>-0.441226</td>\n",
       "      <td>-0.168352</td>\n",
       "      <td>-0.827042</td>\n",
       "      <td>...</td>\n",
       "      <td>1.890856</td>\n",
       "      <td>2.262563</td>\n",
       "      <td>0.602867</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.710802</td>\n",
       "      <td>-0.478394</td>\n",
       "      <td>-0.454123</td>\n",
       "      <td>-0.502204</td>\n",
       "      <td>0.923432</td>\n",
       "      <td>0.755100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.032523</td>\n",
       "      <td>0.701061</td>\n",
       "      <td>0.591942</td>\n",
       "      <td>0.149579</td>\n",
       "      <td>0.422590</td>\n",
       "      <td>0.024365</td>\n",
       "      <td>0.158253</td>\n",
       "      <td>-0.255358</td>\n",
       "      <td>0.073754</td>\n",
       "      <td>1.004875</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.916136</td>\n",
       "      <td>-1.066384</td>\n",
       "      <td>-0.652692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.710802</td>\n",
       "      <td>-0.478394</td>\n",
       "      <td>-0.454123</td>\n",
       "      <td>-0.502204</td>\n",
       "      <td>-1.082917</td>\n",
       "      <td>-1.486602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.032523</td>\n",
       "      <td>0.308515</td>\n",
       "      <td>-0.267916</td>\n",
       "      <td>-0.212397</td>\n",
       "      <td>-0.111586</td>\n",
       "      <td>0.679569</td>\n",
       "      <td>-0.262604</td>\n",
       "      <td>0.154189</td>\n",
       "      <td>1.297996</td>\n",
       "      <td>0.783552</td>\n",
       "      <td>...</td>\n",
       "      <td>0.150276</td>\n",
       "      <td>-0.295244</td>\n",
       "      <td>-0.016789</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.156887</td>\n",
       "      <td>-0.334604</td>\n",
       "      <td>-0.407610</td>\n",
       "      <td>-0.502204</td>\n",
       "      <td>-1.082917</td>\n",
       "      <td>-0.846116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
       "0         -0.032523          0.387024          1.598506         0.185013   \n",
       "1         -0.032523         -0.653222         -0.320893        -0.421579   \n",
       "2         -0.032523         -0.849495         -0.740634        -0.462252   \n",
       "3         -0.032523          0.701061          0.591942         0.149579   \n",
       "4         -0.032523          0.308515         -0.267916        -0.212397   \n",
       "\n",
       "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
       "0       0.956767      -0.256437       0.416602     -0.449404   \n",
       "1      -0.645763      -0.724440      -0.475116     -0.488313   \n",
       "2       1.490943       0.305166       0.691617     -0.441226   \n",
       "3       0.422590       0.024365       0.158253     -0.255358   \n",
       "4      -0.111586       0.679569      -0.262604      0.154189   \n",
       "\n",
       "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('pwat', 'amax')  \\\n",
       "0           3.732772           2.399478  ...         -0.315513   \n",
       "1          -1.456546          -0.749712  ...         -1.492243   \n",
       "2          -0.168352          -0.827042  ...          1.890856   \n",
       "3           0.073754           1.004875  ...         -0.916136   \n",
       "4           1.297996           0.783552  ...          0.150276   \n",
       "\n",
       "   ('pwat', 'mean')  ('pwat', 'var')  ('paramId_0', 'amin')  \\\n",
       "0         -0.224167        -0.397848                    0.0   \n",
       "1         -1.305418        -0.835899                    0.0   \n",
       "2          2.262563         0.602867                    0.0   \n",
       "3         -1.066384        -0.652692                    0.0   \n",
       "4         -0.295244        -0.016789                    0.0   \n",
       "\n",
       "   ('paramId_0', 'amax')  ('paramId_0', 'mean')  ('paramId_0', 'var')  \\\n",
       "0               1.458698               2.385414              1.982489   \n",
       "1              -0.710802              -0.478394             -0.454123   \n",
       "2              -0.710802              -0.478394             -0.454123   \n",
       "3              -0.710802              -0.478394             -0.454123   \n",
       "4              -0.156887              -0.334604             -0.407610   \n",
       "\n",
       "   ('pres', 'count')  ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
       "0           2.569801                     -1.082917              -0.525873  \n",
       "1          -0.502204                      0.923432               1.395587  \n",
       "2          -0.502204                      0.923432               0.755100  \n",
       "3          -0.502204                     -1.082917              -1.486602  \n",
       "4          -0.502204                     -1.082917              -0.846116  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(feature_part_train_1_1.head())\n",
    "part_features_1_1_train = pd.DataFrame(scaler.fit_transform(feature_part_train_1_1))\n",
    "part_features_1_1_train.columns = feature_part_train_1_1.columns\n",
    "part_features_1_1_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
      "292               0.0              0.14           0.02900         0.002435   \n",
      "293               0.0              1.33           0.12625         0.043993   \n",
      "294               0.0              0.47           0.04800         0.011781   \n",
      "295               0.0              0.36           0.10225         0.007274   \n",
      "296               0.0              1.50           0.22900         0.187840   \n",
      "\n",
      "     ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
      "292            5.0           25.0         11.125     35.240385   \n",
      "293            8.0           53.0         25.250    174.756410   \n",
      "294            7.0           36.0         14.375     67.112179   \n",
      "295           15.0           43.0         25.250     29.730769   \n",
      "296           16.0           53.0         30.025    115.768590   \n",
      "\n",
      "     ('tozne', 'amin')  ('tozne', 'amax')  ...  ('pwat', 'amax')  \\\n",
      "292         246.800003         289.600006  ...         17.900000   \n",
      "293         243.399994         291.399994  ...         31.100000   \n",
      "294         257.000000         371.100006  ...         24.700001   \n",
      "295         278.200012         397.899994  ...         22.799999   \n",
      "296         265.700012         381.100006  ...         35.400002   \n",
      "\n",
      "     ('pwat', 'mean')  ('pwat', 'var')  ('paramId_0', 'amin')  \\\n",
      "292            8.0600        13.980923                    0.0   \n",
      "293           15.2250        54.741923                    0.0   \n",
      "294           10.2225        26.132044                    0.0   \n",
      "295           14.5500         7.674359                    0.0   \n",
      "296           18.6275        57.461017                    0.0   \n",
      "\n",
      "     ('paramId_0', 'amax')  ('paramId_0', 'mean')  ('paramId_0', 'var')  \\\n",
      "292                    0.0                   0.00              0.000000   \n",
      "293                    0.0                   0.00              0.000000   \n",
      "294                   23.0                   0.60             13.220513   \n",
      "295                   63.0                  10.15            389.976923   \n",
      "296                   11.0                   0.45              4.151282   \n",
      "\n",
      "     ('pres', 'count')  ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
      "292                  0                             0                      1  \n",
      "293                  0                             0                      1  \n",
      "294                  1                             0                      2  \n",
      "295                  9                             0                      2  \n",
      "296                  2                             0                      2  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>('cwat', 'amin')</th>\n",
       "      <th>('cwat', 'amax')</th>\n",
       "      <th>('cwat', 'mean')</th>\n",
       "      <th>('cwat', 'var')</th>\n",
       "      <th>('r', 'amin')</th>\n",
       "      <th>('r', 'amax')</th>\n",
       "      <th>('r', 'mean')</th>\n",
       "      <th>('r', 'var')</th>\n",
       "      <th>('tozne', 'amin')</th>\n",
       "      <th>('tozne', 'amax')</th>\n",
       "      <th>...</th>\n",
       "      <th>('pwat', 'amax')</th>\n",
       "      <th>('pwat', 'mean')</th>\n",
       "      <th>('pwat', 'var')</th>\n",
       "      <th>('paramId_0', 'amin')</th>\n",
       "      <th>('paramId_0', 'amax')</th>\n",
       "      <th>('paramId_0', 'mean')</th>\n",
       "      <th>('paramId_0', 'var')</th>\n",
       "      <th>('pres', 'count')</th>\n",
       "      <th>('macro_season', '&lt;lambda&gt;')</th>\n",
       "      <th>('month', '&lt;lambda&gt;')</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.032523</td>\n",
       "      <td>-0.633595</td>\n",
       "      <td>-0.296442</td>\n",
       "      <td>-0.407067</td>\n",
       "      <td>-0.912851</td>\n",
       "      <td>-0.724440</td>\n",
       "      <td>-1.158488</td>\n",
       "      <td>-0.275555</td>\n",
       "      <td>-1.616428</td>\n",
       "      <td>-1.320352</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.462604</td>\n",
       "      <td>-0.861915</td>\n",
       "      <td>-0.276267</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.710802</td>\n",
       "      <td>-0.478394</td>\n",
       "      <td>-0.454123</td>\n",
       "      <td>-0.502204</td>\n",
       "      <td>-1.082917</td>\n",
       "      <td>-1.806845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.032523</td>\n",
       "      <td>1.702052</td>\n",
       "      <td>1.288794</td>\n",
       "      <td>0.548704</td>\n",
       "      <td>-0.111586</td>\n",
       "      <td>1.896376</td>\n",
       "      <td>1.195813</td>\n",
       "      <td>3.095561</td>\n",
       "      <td>-1.771742</td>\n",
       "      <td>-1.272355</td>\n",
       "      <td>...</td>\n",
       "      <td>1.155400</td>\n",
       "      <td>0.533341</td>\n",
       "      <td>2.090429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.710802</td>\n",
       "      <td>-0.478394</td>\n",
       "      <td>-0.454123</td>\n",
       "      <td>-0.502204</td>\n",
       "      <td>-1.082917</td>\n",
       "      <td>-1.806845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.032523</td>\n",
       "      <td>0.014106</td>\n",
       "      <td>0.013270</td>\n",
       "      <td>-0.192133</td>\n",
       "      <td>-0.378675</td>\n",
       "      <td>0.305166</td>\n",
       "      <td>-0.616791</td>\n",
       "      <td>0.494561</td>\n",
       "      <td>-1.150486</td>\n",
       "      <td>0.852882</td>\n",
       "      <td>...</td>\n",
       "      <td>0.370913</td>\n",
       "      <td>-0.440807</td>\n",
       "      <td>0.429261</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.350868</td>\n",
       "      <td>-0.190815</td>\n",
       "      <td>-0.283311</td>\n",
       "      <td>0.009797</td>\n",
       "      <td>-1.082917</td>\n",
       "      <td>-1.486602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.032523</td>\n",
       "      <td>-0.201794</td>\n",
       "      <td>0.897579</td>\n",
       "      <td>-0.295768</td>\n",
       "      <td>1.758031</td>\n",
       "      <td>0.960370</td>\n",
       "      <td>1.195813</td>\n",
       "      <td>-0.408684</td>\n",
       "      <td>-0.182056</td>\n",
       "      <td>1.567516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.138019</td>\n",
       "      <td>0.401897</td>\n",
       "      <td>-0.642443</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.197251</td>\n",
       "      <td>4.386485</td>\n",
       "      <td>4.584463</td>\n",
       "      <td>4.105804</td>\n",
       "      <td>-1.082917</td>\n",
       "      <td>-1.486602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.032523</td>\n",
       "      <td>2.035716</td>\n",
       "      <td>2.963683</td>\n",
       "      <td>3.856926</td>\n",
       "      <td>2.025119</td>\n",
       "      <td>1.896376</td>\n",
       "      <td>1.991691</td>\n",
       "      <td>1.670242</td>\n",
       "      <td>-0.753064</td>\n",
       "      <td>1.119536</td>\n",
       "      <td>...</td>\n",
       "      <td>1.682477</td>\n",
       "      <td>1.195918</td>\n",
       "      <td>2.248307</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.203047</td>\n",
       "      <td>-0.262710</td>\n",
       "      <td>-0.400488</td>\n",
       "      <td>0.521798</td>\n",
       "      <td>-1.082917</td>\n",
       "      <td>-1.486602</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
       "0         -0.032523         -0.633595         -0.296442        -0.407067   \n",
       "1         -0.032523          1.702052          1.288794         0.548704   \n",
       "2         -0.032523          0.014106          0.013270        -0.192133   \n",
       "3         -0.032523         -0.201794          0.897579        -0.295768   \n",
       "4         -0.032523          2.035716          2.963683         3.856926   \n",
       "\n",
       "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
       "0      -0.912851      -0.724440      -1.158488     -0.275555   \n",
       "1      -0.111586       1.896376       1.195813      3.095561   \n",
       "2      -0.378675       0.305166      -0.616791      0.494561   \n",
       "3       1.758031       0.960370       1.195813     -0.408684   \n",
       "4       2.025119       1.896376       1.991691      1.670242   \n",
       "\n",
       "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('pwat', 'amax')  \\\n",
       "0          -1.616428          -1.320352  ...         -0.462604   \n",
       "1          -1.771742          -1.272355  ...          1.155400   \n",
       "2          -1.150486           0.852882  ...          0.370913   \n",
       "3          -0.182056           1.567516  ...          0.138019   \n",
       "4          -0.753064           1.119536  ...          1.682477   \n",
       "\n",
       "   ('pwat', 'mean')  ('pwat', 'var')  ('paramId_0', 'amin')  \\\n",
       "0         -0.861915        -0.276267                    0.0   \n",
       "1          0.533341         2.090429                    0.0   \n",
       "2         -0.440807         0.429261                    0.0   \n",
       "3          0.401897        -0.642443                    0.0   \n",
       "4          1.195918         2.248307                    0.0   \n",
       "\n",
       "   ('paramId_0', 'amax')  ('paramId_0', 'mean')  ('paramId_0', 'var')  \\\n",
       "0              -0.710802              -0.478394             -0.454123   \n",
       "1              -0.710802              -0.478394             -0.454123   \n",
       "2               0.350868              -0.190815             -0.283311   \n",
       "3               2.197251               4.386485              4.584463   \n",
       "4              -0.203047              -0.262710             -0.400488   \n",
       "\n",
       "   ('pres', 'count')  ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
       "0          -0.502204                     -1.082917              -1.806845  \n",
       "1          -0.502204                     -1.082917              -1.806845  \n",
       "2           0.009797                     -1.082917              -1.486602  \n",
       "3           4.105804                     -1.082917              -1.486602  \n",
       "4           0.521798                     -1.082917              -1.486602  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tenday_no_null_test.head())\n",
    "tenday_part_features_test = pd.DataFrame(scaler.transform(tenday_no_null_test))\n",
    "tenday_part_features_test.columns = tenday_no_null_test.columns\n",
    "tenday_part_features_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.6845\n",
      "Test set accuracy: 0.6645\n",
      "Training Set Precision: 0.6721\n",
      "Training Set Recall: 0.7206\n",
      "Training Set F1 Score: 0.6955\n",
      "Test Set Precision: 0.2522\n",
      "Test Set Recall: 0.6994\n",
      "Test Set F1 Score: 0.3708\n"
     ]
    }
   ],
   "source": [
    "clf_log_10day_part = LogisticRegression()\n",
    "clf_log_10day_part.fit(part_features_1_1_train, target_train_1_1)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_log_10day_part.score(part_features_1_1_train, \n",
    "                                                       target_train_1_1), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_log_10day_part.score(tenday_part_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_log = clf_log_10day_part.predict(part_features_1_1_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(target_train_1_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(target_train_1_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(target_train_1_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_log = clf_log_10day_part.predict(tenday_part_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_log), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_log), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_log), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.6814\n",
      "Test set accuracy: 0.6494\n",
      "Training Set Precision: 0.6716\n",
      "Training Set Recall: 0.7099\n",
      "Training Set F1 Score: 0.6902\n",
      "Test Set Precision: 0.2423\n",
      "Test Set Recall: 0.6962\n",
      "Test Set F1 Score: 0.3595\n"
     ]
    }
   ],
   "source": [
    "clf_sgd_10day_part = SGDClassifier(loss = \"log\", random_state = 0)\n",
    "clf_sgd_10day_part.fit(part_features_1_1_train, target_train_1_1)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_sgd_10day_part.score(part_features_1_1_train, \n",
    "                                                       target_train_1_1), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_sgd_10day_part.score(tenday_part_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_sgd = clf_sgd_10day_part.predict(part_features_1_1_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(target_train_1_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(target_train_1_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(target_train_1_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_sgd = clf_sgd_10day_part.predict(tenday_part_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_sgd), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_sgd), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_sgd), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2:1 Ratio No Fire:Fire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Full Parameter List "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    47402\n",
       "1    23701\n",
       "Name: fire, dtype: int64"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "under_sampler_2_1 = RandomUnderSampler(sampling_strategy = 0.5, random_state = 0)\n",
    "feature_full_train_2_1, target_train_2_1 = under_sampler_2_1.fit_resample(tenday_full_train, tenday_target_train)\n",
    "target_train_2_1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 71103 entries, 0 to 71102\n",
      "Data columns (total 31 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   ('cwat', 'amin')              71103 non-null  float64\n",
      " 1   ('cwat', 'amax')              71103 non-null  float64\n",
      " 2   ('cwat', 'mean')              71103 non-null  float64\n",
      " 3   ('cwat', 'var')               71103 non-null  float64\n",
      " 4   ('r', 'amin')                 71103 non-null  float64\n",
      " 5   ('r', 'amax')                 71103 non-null  float64\n",
      " 6   ('r', 'mean')                 71103 non-null  float64\n",
      " 7   ('r', 'var')                  71103 non-null  float64\n",
      " 8   ('tozne', 'amin')             71103 non-null  float64\n",
      " 9   ('tozne', 'amax')             71103 non-null  float64\n",
      " 10  ('tozne', 'mean')             71103 non-null  float64\n",
      " 11  ('tozne', 'var')              71103 non-null  float64\n",
      " 12  ('gh', 'amin')                71103 non-null  float64\n",
      " 13  ('gh', 'amax')                71103 non-null  float64\n",
      " 14  ('gh', 'mean')                71103 non-null  float64\n",
      " 15  ('gh', 'var')                 71103 non-null  float64\n",
      " 16  ('pwat', 'amin')              71103 non-null  float64\n",
      " 17  ('pwat', 'amax')              71103 non-null  float64\n",
      " 18  ('pwat', 'mean')              71103 non-null  float64\n",
      " 19  ('pwat', 'var')               71103 non-null  float64\n",
      " 20  ('paramId_0', 'amin')         71103 non-null  float64\n",
      " 21  ('paramId_0', 'amax')         71103 non-null  float64\n",
      " 22  ('paramId_0', 'mean')         71103 non-null  float64\n",
      " 23  ('paramId_0', 'var')          71103 non-null  float64\n",
      " 24  ('pres', 'amin')              26437 non-null  float64\n",
      " 25  ('pres', 'amax')              26437 non-null  float64\n",
      " 26  ('pres', 'mean')              26437 non-null  float64\n",
      " 27  ('pres', 'var')               16352 non-null  float64\n",
      " 28  ('pres', 'count')             71103 non-null  int64  \n",
      " 29  ('macro_season', '<lambda>')  71103 non-null  int64  \n",
      " 30  ('month', '<lambda>')         71103 non-null  int64  \n",
      "dtypes: float64(28), int64(3)\n",
      "memory usage: 16.8 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 71103 entries, 0 to 71102\n",
      "Data columns (total 31 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   ('cwat', 'amin')              71103 non-null  float64\n",
      " 1   ('cwat', 'amax')              71103 non-null  float64\n",
      " 2   ('cwat', 'mean')              71103 non-null  float64\n",
      " 3   ('cwat', 'var')               71103 non-null  float64\n",
      " 4   ('r', 'amin')                 71103 non-null  float64\n",
      " 5   ('r', 'amax')                 71103 non-null  float64\n",
      " 6   ('r', 'mean')                 71103 non-null  float64\n",
      " 7   ('r', 'var')                  71103 non-null  float64\n",
      " 8   ('tozne', 'amin')             71103 non-null  float64\n",
      " 9   ('tozne', 'amax')             71103 non-null  float64\n",
      " 10  ('tozne', 'mean')             71103 non-null  float64\n",
      " 11  ('tozne', 'var')              71103 non-null  float64\n",
      " 12  ('gh', 'amin')                71103 non-null  float64\n",
      " 13  ('gh', 'amax')                71103 non-null  float64\n",
      " 14  ('gh', 'mean')                71103 non-null  float64\n",
      " 15  ('gh', 'var')                 71103 non-null  float64\n",
      " 16  ('pwat', 'amin')              71103 non-null  float64\n",
      " 17  ('pwat', 'amax')              71103 non-null  float64\n",
      " 18  ('pwat', 'mean')              71103 non-null  float64\n",
      " 19  ('pwat', 'var')               71103 non-null  float64\n",
      " 20  ('paramId_0', 'amin')         71103 non-null  float64\n",
      " 21  ('paramId_0', 'amax')         71103 non-null  float64\n",
      " 22  ('paramId_0', 'mean')         71103 non-null  float64\n",
      " 23  ('paramId_0', 'var')          71103 non-null  float64\n",
      " 24  ('pres', 'amin')              71103 non-null  float64\n",
      " 25  ('pres', 'amax')              71103 non-null  float64\n",
      " 26  ('pres', 'mean')              71103 non-null  float64\n",
      " 27  ('pres', 'var')               71103 non-null  float64\n",
      " 28  ('pres', 'count')             71103 non-null  float64\n",
      " 29  ('macro_season', '<lambda>')  71103 non-null  float64\n",
      " 30  ('month', '<lambda>')         71103 non-null  float64\n",
      "dtypes: float64(31)\n",
      "memory usage: 16.8 MB\n"
     ]
    }
   ],
   "source": [
    "feature_full_train_2_1.info()\n",
    "feature_imp_train_2_1 = pd.DataFrame(imp.fit_transform(feature_full_train_2_1))\n",
    "feature_imp_train_2_1.columns = feature_full_train_2_1.columns\n",
    "feature_imp_train_2_1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 35259 entries, 292 to 176294\n",
      "Data columns (total 31 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   ('cwat', 'amin')              35259 non-null  float64\n",
      " 1   ('cwat', 'amax')              35259 non-null  float64\n",
      " 2   ('cwat', 'mean')              35259 non-null  float64\n",
      " 3   ('cwat', 'var')               35259 non-null  float64\n",
      " 4   ('r', 'amin')                 35259 non-null  float64\n",
      " 5   ('r', 'amax')                 35259 non-null  float64\n",
      " 6   ('r', 'mean')                 35259 non-null  float64\n",
      " 7   ('r', 'var')                  35259 non-null  float64\n",
      " 8   ('tozne', 'amin')             35259 non-null  float64\n",
      " 9   ('tozne', 'amax')             35259 non-null  float64\n",
      " 10  ('tozne', 'mean')             35259 non-null  float64\n",
      " 11  ('tozne', 'var')              35259 non-null  float64\n",
      " 12  ('gh', 'amin')                35259 non-null  float64\n",
      " 13  ('gh', 'amax')                35259 non-null  float64\n",
      " 14  ('gh', 'mean')                35259 non-null  float64\n",
      " 15  ('gh', 'var')                 35259 non-null  float64\n",
      " 16  ('pwat', 'amin')              35259 non-null  float64\n",
      " 17  ('pwat', 'amax')              35259 non-null  float64\n",
      " 18  ('pwat', 'mean')              35259 non-null  float64\n",
      " 19  ('pwat', 'var')               35259 non-null  float64\n",
      " 20  ('paramId_0', 'amin')         35259 non-null  float64\n",
      " 21  ('paramId_0', 'amax')         35259 non-null  float64\n",
      " 22  ('paramId_0', 'mean')         35259 non-null  float64\n",
      " 23  ('paramId_0', 'var')          35259 non-null  float64\n",
      " 24  ('pres', 'amin')              15069 non-null  float64\n",
      " 25  ('pres', 'amax')              15069 non-null  float64\n",
      " 26  ('pres', 'mean')              15069 non-null  float64\n",
      " 27  ('pres', 'var')               9461 non-null   float64\n",
      " 28  ('pres', 'count')             35259 non-null  int64  \n",
      " 29  ('macro_season', '<lambda>')  35259 non-null  int64  \n",
      " 30  ('month', '<lambda>')         35259 non-null  int64  \n",
      "dtypes: float64(28), int64(3)\n",
      "memory usage: 8.6 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 35259 entries, 0 to 35258\n",
      "Data columns (total 31 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   ('cwat', 'amin')              35259 non-null  float64\n",
      " 1   ('cwat', 'amax')              35259 non-null  float64\n",
      " 2   ('cwat', 'mean')              35259 non-null  float64\n",
      " 3   ('cwat', 'var')               35259 non-null  float64\n",
      " 4   ('r', 'amin')                 35259 non-null  float64\n",
      " 5   ('r', 'amax')                 35259 non-null  float64\n",
      " 6   ('r', 'mean')                 35259 non-null  float64\n",
      " 7   ('r', 'var')                  35259 non-null  float64\n",
      " 8   ('tozne', 'amin')             35259 non-null  float64\n",
      " 9   ('tozne', 'amax')             35259 non-null  float64\n",
      " 10  ('tozne', 'mean')             35259 non-null  float64\n",
      " 11  ('tozne', 'var')              35259 non-null  float64\n",
      " 12  ('gh', 'amin')                35259 non-null  float64\n",
      " 13  ('gh', 'amax')                35259 non-null  float64\n",
      " 14  ('gh', 'mean')                35259 non-null  float64\n",
      " 15  ('gh', 'var')                 35259 non-null  float64\n",
      " 16  ('pwat', 'amin')              35259 non-null  float64\n",
      " 17  ('pwat', 'amax')              35259 non-null  float64\n",
      " 18  ('pwat', 'mean')              35259 non-null  float64\n",
      " 19  ('pwat', 'var')               35259 non-null  float64\n",
      " 20  ('paramId_0', 'amin')         35259 non-null  float64\n",
      " 21  ('paramId_0', 'amax')         35259 non-null  float64\n",
      " 22  ('paramId_0', 'mean')         35259 non-null  float64\n",
      " 23  ('paramId_0', 'var')          35259 non-null  float64\n",
      " 24  ('pres', 'amin')              35259 non-null  float64\n",
      " 25  ('pres', 'amax')              35259 non-null  float64\n",
      " 26  ('pres', 'mean')              35259 non-null  float64\n",
      " 27  ('pres', 'var')               35259 non-null  float64\n",
      " 28  ('pres', 'count')             35259 non-null  float64\n",
      " 29  ('macro_season', '<lambda>')  35259 non-null  float64\n",
      " 30  ('month', '<lambda>')         35259 non-null  float64\n",
      "dtypes: float64(31)\n",
      "memory usage: 8.3 MB\n"
     ]
    }
   ],
   "source": [
    "tenday_full_test.info()\n",
    "tenday_imp_test = pd.DataFrame(imp.fit_transform(tenday_full_test))\n",
    "tenday_imp_test.columns = tenday_full_test.columns\n",
    "tenday_imp_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
      "0               0.0              1.57           0.15525         0.076405   \n",
      "1               0.0              0.14           0.03375         0.002004   \n",
      "2               0.0              0.25           0.01950         0.003784   \n",
      "3               0.0              0.03           0.00275         0.000056   \n",
      "4               0.0              0.62           0.07775         0.019705   \n",
      "\n",
      "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
      "0           12.0           55.0         28.825    151.942949   \n",
      "1            9.0           20.0         14.325      6.481410   \n",
      "2            3.0           18.0          9.725     16.666026   \n",
      "3            4.0           17.0         10.050     12.048718   \n",
      "4            6.0           49.0         19.850    121.310256   \n",
      "\n",
      "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('paramId_0', 'amax')  \\\n",
      "0         249.000000         359.299988  ...                   43.0   \n",
      "1         299.399994         324.899994  ...                    0.0   \n",
      "2         256.100006         336.799988  ...                    0.0   \n",
      "3         290.500000         352.200012  ...                    0.0   \n",
      "4         252.199997         357.399994  ...                   16.0   \n",
      "\n",
      "   ('paramId_0', 'mean')  ('paramId_0', 'var')  ('pres', 'amin')  \\\n",
      "0                  1.875             51.342949           45060.0   \n",
      "1                  0.000              0.000000           42350.0   \n",
      "2                  0.000              0.000000           42350.0   \n",
      "3                  0.000              0.000000           42350.0   \n",
      "4                  0.725              9.742949           41640.0   \n",
      "\n",
      "   ('pres', 'amax')  ('pres', 'mean')  ('pres', 'var')  ('pres', 'count')  \\\n",
      "0           49680.0      47370.000000       10672200.0                2.0   \n",
      "1           55330.0      49963.333333       45984050.0                0.0   \n",
      "2           55330.0      49963.333333       45984050.0                0.0   \n",
      "3           55330.0      49963.333333       45984050.0                0.0   \n",
      "4           65410.0      53525.000000      282506450.0                2.0   \n",
      "\n",
      "   ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
      "0                           0.0                    2.0  \n",
      "1                           1.0                    8.0  \n",
      "2                           1.0                   10.0  \n",
      "3                           0.0                    5.0  \n",
      "4                           0.0                    1.0  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>('cwat', 'amin')</th>\n",
       "      <th>('cwat', 'amax')</th>\n",
       "      <th>('cwat', 'mean')</th>\n",
       "      <th>('cwat', 'var')</th>\n",
       "      <th>('r', 'amin')</th>\n",
       "      <th>('r', 'amax')</th>\n",
       "      <th>('r', 'mean')</th>\n",
       "      <th>('r', 'var')</th>\n",
       "      <th>('tozne', 'amin')</th>\n",
       "      <th>('tozne', 'amax')</th>\n",
       "      <th>...</th>\n",
       "      <th>('paramId_0', 'amax')</th>\n",
       "      <th>('paramId_0', 'mean')</th>\n",
       "      <th>('paramId_0', 'var')</th>\n",
       "      <th>('pres', 'amin')</th>\n",
       "      <th>('pres', 'amax')</th>\n",
       "      <th>('pres', 'mean')</th>\n",
       "      <th>('pres', 'var')</th>\n",
       "      <th>('pres', 'count')</th>\n",
       "      <th>('macro_season', '&lt;lambda&gt;')</th>\n",
       "      <th>('month', '&lt;lambda&gt;')</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.038147</td>\n",
       "      <td>2.089264</td>\n",
       "      <td>1.602953</td>\n",
       "      <td>1.200025</td>\n",
       "      <td>0.883556</td>\n",
       "      <td>1.984670</td>\n",
       "      <td>1.674613</td>\n",
       "      <td>2.426307</td>\n",
       "      <td>-1.447555</td>\n",
       "      <td>0.480214</td>\n",
       "      <td>...</td>\n",
       "      <td>1.208868</td>\n",
       "      <td>0.352961</td>\n",
       "      <td>0.157845</td>\n",
       "      <td>0.269055</td>\n",
       "      <td>-0.552576</td>\n",
       "      <td>-0.280555</td>\n",
       "      <td>-0.849926</td>\n",
       "      <td>0.445618</td>\n",
       "      <td>-1.037129</td>\n",
       "      <td>-1.381954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.038147</td>\n",
       "      <td>-0.672070</td>\n",
       "      <td>-0.281976</td>\n",
       "      <td>-0.436079</td>\n",
       "      <td>0.100227</td>\n",
       "      <td>-1.227142</td>\n",
       "      <td>-0.671908</td>\n",
       "      <td>-0.985136</td>\n",
       "      <td>0.800084</td>\n",
       "      <td>-0.418021</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.737282</td>\n",
       "      <td>-0.492320</td>\n",
       "      <td>-0.469968</td>\n",
       "      <td>-0.074362</td>\n",
       "      <td>0.070383</td>\n",
       "      <td>0.051552</td>\n",
       "      <td>-0.155580</td>\n",
       "      <td>-0.519352</td>\n",
       "      <td>0.964200</td>\n",
       "      <td>0.439889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.038147</td>\n",
       "      <td>-0.459660</td>\n",
       "      <td>-0.503048</td>\n",
       "      <td>-0.396918</td>\n",
       "      <td>-1.466431</td>\n",
       "      <td>-1.410674</td>\n",
       "      <td>-1.416321</td>\n",
       "      <td>-0.746281</td>\n",
       "      <td>-1.130923</td>\n",
       "      <td>-0.107295</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.737282</td>\n",
       "      <td>-0.492320</td>\n",
       "      <td>-0.469968</td>\n",
       "      <td>-0.074362</td>\n",
       "      <td>0.070383</td>\n",
       "      <td>0.051552</td>\n",
       "      <td>-0.155580</td>\n",
       "      <td>-0.519352</td>\n",
       "      <td>0.964200</td>\n",
       "      <td>1.047170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.038147</td>\n",
       "      <td>-0.884481</td>\n",
       "      <td>-0.762904</td>\n",
       "      <td>-0.478898</td>\n",
       "      <td>-1.205321</td>\n",
       "      <td>-1.502440</td>\n",
       "      <td>-1.363727</td>\n",
       "      <td>-0.854569</td>\n",
       "      <td>0.403180</td>\n",
       "      <td>0.294823</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.737282</td>\n",
       "      <td>-0.492320</td>\n",
       "      <td>-0.469968</td>\n",
       "      <td>-0.074362</td>\n",
       "      <td>0.070383</td>\n",
       "      <td>0.051552</td>\n",
       "      <td>-0.155580</td>\n",
       "      <td>-0.519352</td>\n",
       "      <td>-1.037129</td>\n",
       "      <td>-0.471033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.038147</td>\n",
       "      <td>0.254811</td>\n",
       "      <td>0.400632</td>\n",
       "      <td>-0.046819</td>\n",
       "      <td>-0.683102</td>\n",
       "      <td>1.434074</td>\n",
       "      <td>0.222198</td>\n",
       "      <td>1.707892</td>\n",
       "      <td>-1.304848</td>\n",
       "      <td>0.430602</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013133</td>\n",
       "      <td>-0.165478</td>\n",
       "      <td>-0.350833</td>\n",
       "      <td>-0.164335</td>\n",
       "      <td>1.181785</td>\n",
       "      <td>0.507665</td>\n",
       "      <td>4.495220</td>\n",
       "      <td>0.445618</td>\n",
       "      <td>-1.037129</td>\n",
       "      <td>-1.685595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
       "0         -0.038147          2.089264          1.602953         1.200025   \n",
       "1         -0.038147         -0.672070         -0.281976        -0.436079   \n",
       "2         -0.038147         -0.459660         -0.503048        -0.396918   \n",
       "3         -0.038147         -0.884481         -0.762904        -0.478898   \n",
       "4         -0.038147          0.254811          0.400632        -0.046819   \n",
       "\n",
       "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
       "0       0.883556       1.984670       1.674613      2.426307   \n",
       "1       0.100227      -1.227142      -0.671908     -0.985136   \n",
       "2      -1.466431      -1.410674      -1.416321     -0.746281   \n",
       "3      -1.205321      -1.502440      -1.363727     -0.854569   \n",
       "4      -0.683102       1.434074       0.222198      1.707892   \n",
       "\n",
       "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('paramId_0', 'amax')  \\\n",
       "0          -1.447555           0.480214  ...               1.208868   \n",
       "1           0.800084          -0.418021  ...              -0.737282   \n",
       "2          -1.130923          -0.107295  ...              -0.737282   \n",
       "3           0.403180           0.294823  ...              -0.737282   \n",
       "4          -1.304848           0.430602  ...              -0.013133   \n",
       "\n",
       "   ('paramId_0', 'mean')  ('paramId_0', 'var')  ('pres', 'amin')  \\\n",
       "0               0.352961              0.157845          0.269055   \n",
       "1              -0.492320             -0.469968         -0.074362   \n",
       "2              -0.492320             -0.469968         -0.074362   \n",
       "3              -0.492320             -0.469968         -0.074362   \n",
       "4              -0.165478             -0.350833         -0.164335   \n",
       "\n",
       "   ('pres', 'amax')  ('pres', 'mean')  ('pres', 'var')  ('pres', 'count')  \\\n",
       "0         -0.552576         -0.280555        -0.849926           0.445618   \n",
       "1          0.070383          0.051552        -0.155580          -0.519352   \n",
       "2          0.070383          0.051552        -0.155580          -0.519352   \n",
       "3          0.070383          0.051552        -0.155580          -0.519352   \n",
       "4          1.181785          0.507665         4.495220           0.445618   \n",
       "\n",
       "   ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
       "0                     -1.037129              -1.381954  \n",
       "1                      0.964200               0.439889  \n",
       "2                      0.964200               1.047170  \n",
       "3                     -1.037129              -0.471033  \n",
       "4                     -1.037129              -1.685595  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(feature_imp_train_2_1.head())\n",
    "full_features_2_1_train = pd.DataFrame(scaler.fit_transform(feature_imp_train_2_1))\n",
    "full_features_2_1_train.columns = feature_imp_train_2_1.columns\n",
    "full_features_2_1_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>('cwat', 'amin')</th>\n",
       "      <th>('cwat', 'amax')</th>\n",
       "      <th>('cwat', 'mean')</th>\n",
       "      <th>('cwat', 'var')</th>\n",
       "      <th>('r', 'amin')</th>\n",
       "      <th>('r', 'amax')</th>\n",
       "      <th>('r', 'mean')</th>\n",
       "      <th>('r', 'var')</th>\n",
       "      <th>('tozne', 'amin')</th>\n",
       "      <th>('tozne', 'amax')</th>\n",
       "      <th>...</th>\n",
       "      <th>('paramId_0', 'amax')</th>\n",
       "      <th>('paramId_0', 'mean')</th>\n",
       "      <th>('paramId_0', 'var')</th>\n",
       "      <th>('pres', 'amin')</th>\n",
       "      <th>('pres', 'amax')</th>\n",
       "      <th>('pres', 'mean')</th>\n",
       "      <th>('pres', 'var')</th>\n",
       "      <th>('pres', 'count')</th>\n",
       "      <th>('macro_season', '&lt;lambda&gt;')</th>\n",
       "      <th>('month', '&lt;lambda&gt;')</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.038147</td>\n",
       "      <td>-0.672070</td>\n",
       "      <td>-0.355667</td>\n",
       "      <td>-0.426594</td>\n",
       "      <td>-0.944211</td>\n",
       "      <td>-0.768311</td>\n",
       "      <td>-1.189761</td>\n",
       "      <td>-0.310665</td>\n",
       "      <td>-1.545666</td>\n",
       "      <td>-1.339756</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.737282</td>\n",
       "      <td>-0.492320</td>\n",
       "      <td>-0.469968</td>\n",
       "      <td>-0.027475</td>\n",
       "      <td>0.057152</td>\n",
       "      <td>0.040880</td>\n",
       "      <td>-0.051527</td>\n",
       "      <td>-0.519352</td>\n",
       "      <td>-1.037129</td>\n",
       "      <td>-1.685595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.038147</td>\n",
       "      <td>1.625823</td>\n",
       "      <td>1.153052</td>\n",
       "      <td>0.487283</td>\n",
       "      <td>-0.160882</td>\n",
       "      <td>1.801138</td>\n",
       "      <td>1.096075</td>\n",
       "      <td>2.961341</td>\n",
       "      <td>-1.697293</td>\n",
       "      <td>-1.292755</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.737282</td>\n",
       "      <td>-0.492320</td>\n",
       "      <td>-0.469968</td>\n",
       "      <td>-0.027475</td>\n",
       "      <td>0.057152</td>\n",
       "      <td>0.040880</td>\n",
       "      <td>-0.051527</td>\n",
       "      <td>-0.519352</td>\n",
       "      <td>-1.037129</td>\n",
       "      <td>-1.685595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.038147</td>\n",
       "      <td>-0.034839</td>\n",
       "      <td>-0.060904</td>\n",
       "      <td>-0.221081</td>\n",
       "      <td>-0.421992</td>\n",
       "      <td>0.241115</td>\n",
       "      <td>-0.663816</td>\n",
       "      <td>0.436810</td>\n",
       "      <td>-1.090787</td>\n",
       "      <td>0.788329</td>\n",
       "      <td>...</td>\n",
       "      <td>0.303682</td>\n",
       "      <td>-0.221830</td>\n",
       "      <td>-0.308310</td>\n",
       "      <td>4.311498</td>\n",
       "      <td>2.455267</td>\n",
       "      <td>3.508793</td>\n",
       "      <td>-0.051527</td>\n",
       "      <td>-0.036867</td>\n",
       "      <td>-1.037129</td>\n",
       "      <td>-1.381954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.038147</td>\n",
       "      <td>-0.247250</td>\n",
       "      <td>0.780720</td>\n",
       "      <td>-0.320174</td>\n",
       "      <td>1.666885</td>\n",
       "      <td>0.883477</td>\n",
       "      <td>1.096075</td>\n",
       "      <td>-0.439880</td>\n",
       "      <td>-0.145351</td>\n",
       "      <td>1.488117</td>\n",
       "      <td>...</td>\n",
       "      <td>2.114054</td>\n",
       "      <td>4.083467</td>\n",
       "      <td>4.298607</td>\n",
       "      <td>-0.180809</td>\n",
       "      <td>2.551192</td>\n",
       "      <td>1.337435</td>\n",
       "      <td>3.640659</td>\n",
       "      <td>3.823014</td>\n",
       "      <td>-1.037129</td>\n",
       "      <td>-1.381954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.038147</td>\n",
       "      <td>1.954094</td>\n",
       "      <td>2.747096</td>\n",
       "      <td>3.650500</td>\n",
       "      <td>1.927995</td>\n",
       "      <td>1.801138</td>\n",
       "      <td>1.868808</td>\n",
       "      <td>1.577926</td>\n",
       "      <td>-0.702801</td>\n",
       "      <td>1.049444</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.239429</td>\n",
       "      <td>-0.289452</td>\n",
       "      <td>-0.419207</td>\n",
       "      <td>0.248780</td>\n",
       "      <td>1.634946</td>\n",
       "      <td>0.979573</td>\n",
       "      <td>4.899609</td>\n",
       "      <td>0.445618</td>\n",
       "      <td>-1.037129</td>\n",
       "      <td>-1.381954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
       "0         -0.038147         -0.672070         -0.355667        -0.426594   \n",
       "1         -0.038147          1.625823          1.153052         0.487283   \n",
       "2         -0.038147         -0.034839         -0.060904        -0.221081   \n",
       "3         -0.038147         -0.247250          0.780720        -0.320174   \n",
       "4         -0.038147          1.954094          2.747096         3.650500   \n",
       "\n",
       "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
       "0      -0.944211      -0.768311      -1.189761     -0.310665   \n",
       "1      -0.160882       1.801138       1.096075      2.961341   \n",
       "2      -0.421992       0.241115      -0.663816      0.436810   \n",
       "3       1.666885       0.883477       1.096075     -0.439880   \n",
       "4       1.927995       1.801138       1.868808      1.577926   \n",
       "\n",
       "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('paramId_0', 'amax')  \\\n",
       "0          -1.545666          -1.339756  ...              -0.737282   \n",
       "1          -1.697293          -1.292755  ...              -0.737282   \n",
       "2          -1.090787           0.788329  ...               0.303682   \n",
       "3          -0.145351           1.488117  ...               2.114054   \n",
       "4          -0.702801           1.049444  ...              -0.239429   \n",
       "\n",
       "   ('paramId_0', 'mean')  ('paramId_0', 'var')  ('pres', 'amin')  \\\n",
       "0              -0.492320             -0.469968         -0.027475   \n",
       "1              -0.492320             -0.469968         -0.027475   \n",
       "2              -0.221830             -0.308310          4.311498   \n",
       "3               4.083467              4.298607         -0.180809   \n",
       "4              -0.289452             -0.419207          0.248780   \n",
       "\n",
       "   ('pres', 'amax')  ('pres', 'mean')  ('pres', 'var')  ('pres', 'count')  \\\n",
       "0          0.057152          0.040880        -0.051527          -0.519352   \n",
       "1          0.057152          0.040880        -0.051527          -0.519352   \n",
       "2          2.455267          3.508793        -0.051527          -0.036867   \n",
       "3          2.551192          1.337435         3.640659           3.823014   \n",
       "4          1.634946          0.979573         4.899609           0.445618   \n",
       "\n",
       "   ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
       "0                     -1.037129              -1.685595  \n",
       "1                     -1.037129              -1.685595  \n",
       "2                     -1.037129              -1.381954  \n",
       "3                     -1.037129              -1.381954  \n",
       "4                     -1.037129              -1.381954  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tenday_full_features_test = pd.DataFrame(scaler.transform(tenday_imp_test))\n",
    "tenday_full_features_test.columns = tenday_imp_test.columns\n",
    "tenday_full_features_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.6726\n",
      "Test set accuracy: 0.6652\n",
      "Training Set Precision: 0.5063\n",
      "Training Set Recall: 0.7213\n",
      "Training Set F1 Score: 0.5949\n",
      "Test Set Precision: 0.2533\n",
      "Test Set Recall: 0.7026\n",
      "Test Set F1 Score: 0.3723\n"
     ]
    }
   ],
   "source": [
    "clf_log_10day_full = LogisticRegression(class_weight = \"balanced\", max_iter = 100000)\n",
    "clf_log_10day_full.fit(full_features_2_1_train, target_train_2_1)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_log_10day_full.score(full_features_2_1_train, \n",
    "                                                       target_train_2_1), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_log_10day_full.score(tenday_full_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_log = clf_log_10day_full.predict(full_features_2_1_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(target_train_2_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(target_train_2_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(target_train_2_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_log = clf_log_10day_full.predict(tenday_full_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_log), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_log), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_log), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.7191\n",
      "Test set accuracy: 0.7998\n",
      "Training Set Precision: 0.6202\n",
      "Training Set Recall: 0.406\n",
      "Training Set F1 Score: 0.4907\n",
      "Test Set Precision: 0.3315\n",
      "Test Set Recall: 0.4096\n",
      "Test Set F1 Score: 0.3664\n"
     ]
    }
   ],
   "source": [
    "clf_log_10day_full = LogisticRegression(max_iter = 100000)\n",
    "clf_log_10day_full.fit(full_features_2_1_train, target_train_2_1)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_log_10day_full.score(full_features_2_1_train, \n",
    "                                                       target_train_2_1), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_log_10day_full.score(tenday_full_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_log = clf_log_10day_full.predict(full_features_2_1_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(target_train_2_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(target_train_2_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(target_train_2_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_log = clf_log_10day_full.predict(tenday_full_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_log), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_log), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_log), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.6783\n",
      "Test set accuracy: 0.6922\n",
      "Training Set Precision: 0.5129\n",
      "Training Set Recall: 0.6915\n",
      "Training Set F1 Score: 0.589\n",
      "Test Set Precision: 0.2656\n",
      "Test Set Recall: 0.6675\n",
      "Test Set F1 Score: 0.38\n"
     ]
    }
   ],
   "source": [
    "clf_sgd_10day_full = SGDClassifier(loss = \"log\", class_weight = \"balanced\", random_state = 0)\n",
    "clf_sgd_10day_full.fit(full_features_2_1_train, target_train_2_1)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_sgd_10day_full.score(full_features_2_1_train, \n",
    "                                                       target_train_2_1), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_sgd_10day_full.score(tenday_full_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_sgd = clf_sgd_10day_full.predict(full_features_2_1_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(target_train_2_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(target_train_2_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(target_train_2_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_sgd = clf_sgd_10day_full.predict(tenday_full_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_sgd), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_sgd), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_sgd), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.713\n",
      "Test set accuracy: 0.7789\n",
      "Training Set Precision: 0.5911\n",
      "Training Set Recall: 0.4508\n",
      "Training Set F1 Score: 0.5115\n",
      "Test Set Precision: 0.3111\n",
      "Test Set Recall: 0.4646\n",
      "Test Set F1 Score: 0.3726\n"
     ]
    }
   ],
   "source": [
    "clf_sgd_10day_full = SGDClassifier(loss = \"log\", random_state = 0)\n",
    "clf_sgd_10day_full.fit(full_features_2_1_train, target_train_2_1)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_sgd_10day_full.score(full_features_2_1_train, \n",
    "                                                       target_train_2_1), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_sgd_10day_full.score(tenday_full_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_sgd = clf_sgd_10day_full.predict(full_features_2_1_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(target_train_2_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(target_train_2_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(target_train_2_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_sgd = clf_sgd_10day_full.predict(tenday_full_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_sgd), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_sgd), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_sgd), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Part Parameter List "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    47402\n",
       "1    23701\n",
       "Name: fire, dtype: int64"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_part_train_2_1, target_train_2_1 = under_sampler_2_1.fit_resample(tenday_no_null_train, tenday_target_train)\n",
    "target_train_2_1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
      "0               0.0              0.17           0.04900         0.002276   \n",
      "1               0.0              0.12           0.01700         0.001139   \n",
      "2               0.0              0.26           0.04925         0.003710   \n",
      "3               0.0              1.49           0.17000         0.098985   \n",
      "4               0.0              0.06           0.00400         0.000122   \n",
      "\n",
      "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
      "0           10.0           31.0         19.650     29.874359   \n",
      "1           10.0           34.0         20.800     33.958974   \n",
      "2           19.0           51.0         32.075     58.635256   \n",
      "3           13.0           67.0         35.425    235.122436   \n",
      "4            9.0           22.0         13.200     10.523077   \n",
      "\n",
      "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('pwat', 'amax')  \\\n",
      "0         278.100006         307.200012  ...         31.500000   \n",
      "1         281.799988         311.500000  ...         30.400000   \n",
      "2         299.500000         314.500000  ...         57.299999   \n",
      "3         289.000000         340.299988  ...         33.599998   \n",
      "4         244.000000         274.000000  ...         20.299999   \n",
      "\n",
      "   ('pwat', 'mean')  ('pwat', 'var')  ('paramId_0', 'amin')  \\\n",
      "0           19.7425        30.434301                    0.0   \n",
      "1           19.4650        23.003872                    0.0   \n",
      "2           39.6250        46.633205                    0.0   \n",
      "3           15.4575        54.532761                    0.0   \n",
      "4           12.4225         7.132557                    0.0   \n",
      "\n",
      "   ('paramId_0', 'amax')  ('paramId_0', 'mean')  ('paramId_0', 'var')  \\\n",
      "0                    0.0                  0.000              0.000000   \n",
      "1                    0.0                  0.000              0.000000   \n",
      "2                   42.0                  2.075             52.173718   \n",
      "3                   66.0                  8.650            281.925641   \n",
      "4                    0.0                  0.000              0.000000   \n",
      "\n",
      "   ('pres', 'count')  ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
      "0                  0                             1                      9  \n",
      "1                  0                             1                      9  \n",
      "2                  1                             1                      7  \n",
      "3                  9                             1                     12  \n",
      "4                  0                             1                     10  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>('cwat', 'amin')</th>\n",
       "      <th>('cwat', 'amax')</th>\n",
       "      <th>('cwat', 'mean')</th>\n",
       "      <th>('cwat', 'var')</th>\n",
       "      <th>('r', 'amin')</th>\n",
       "      <th>('r', 'amax')</th>\n",
       "      <th>('r', 'mean')</th>\n",
       "      <th>('r', 'var')</th>\n",
       "      <th>('tozne', 'amin')</th>\n",
       "      <th>('tozne', 'amax')</th>\n",
       "      <th>...</th>\n",
       "      <th>('pwat', 'amax')</th>\n",
       "      <th>('pwat', 'mean')</th>\n",
       "      <th>('pwat', 'var')</th>\n",
       "      <th>('paramId_0', 'amin')</th>\n",
       "      <th>('paramId_0', 'amax')</th>\n",
       "      <th>('paramId_0', 'mean')</th>\n",
       "      <th>('paramId_0', 'var')</th>\n",
       "      <th>('pres', 'count')</th>\n",
       "      <th>('macro_season', '&lt;lambda&gt;')</th>\n",
       "      <th>('month', '&lt;lambda&gt;')</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.036542</td>\n",
       "      <td>-0.612395</td>\n",
       "      <td>-0.042812</td>\n",
       "      <td>-0.430207</td>\n",
       "      <td>0.362621</td>\n",
       "      <td>-0.215795</td>\n",
       "      <td>0.192437</td>\n",
       "      <td>-0.436024</td>\n",
       "      <td>-0.152770</td>\n",
       "      <td>-0.874007</td>\n",
       "      <td>...</td>\n",
       "      <td>1.177876</td>\n",
       "      <td>1.392828</td>\n",
       "      <td>0.661637</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.735136</td>\n",
       "      <td>-0.490815</td>\n",
       "      <td>-0.468673</td>\n",
       "      <td>-0.516699</td>\n",
       "      <td>0.96119</td>\n",
       "      <td>0.744673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.036542</td>\n",
       "      <td>-0.709242</td>\n",
       "      <td>-0.541642</td>\n",
       "      <td>-0.455426</td>\n",
       "      <td>0.362621</td>\n",
       "      <td>0.059484</td>\n",
       "      <td>0.378603</td>\n",
       "      <td>-0.339697</td>\n",
       "      <td>0.011576</td>\n",
       "      <td>-0.761778</td>\n",
       "      <td>...</td>\n",
       "      <td>1.046658</td>\n",
       "      <td>1.339957</td>\n",
       "      <td>0.239326</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.735136</td>\n",
       "      <td>-0.490815</td>\n",
       "      <td>-0.468673</td>\n",
       "      <td>-0.516699</td>\n",
       "      <td>0.96119</td>\n",
       "      <td>0.744673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.036542</td>\n",
       "      <td>-0.438069</td>\n",
       "      <td>-0.038915</td>\n",
       "      <td>-0.398390</td>\n",
       "      <td>2.711884</td>\n",
       "      <td>1.619397</td>\n",
       "      <td>2.203839</td>\n",
       "      <td>0.242241</td>\n",
       "      <td>0.797776</td>\n",
       "      <td>-0.683479</td>\n",
       "      <td>...</td>\n",
       "      <td>4.255539</td>\n",
       "      <td>5.180989</td>\n",
       "      <td>1.582308</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.166082</td>\n",
       "      <td>0.447299</td>\n",
       "      <td>0.170256</td>\n",
       "      <td>-0.033916</td>\n",
       "      <td>0.96119</td>\n",
       "      <td>0.136307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.036542</td>\n",
       "      <td>1.944383</td>\n",
       "      <td>1.843386</td>\n",
       "      <td>1.715888</td>\n",
       "      <td>1.145709</td>\n",
       "      <td>3.087551</td>\n",
       "      <td>2.746148</td>\n",
       "      <td>4.404317</td>\n",
       "      <td>0.331387</td>\n",
       "      <td>-0.010107</td>\n",
       "      <td>...</td>\n",
       "      <td>1.428383</td>\n",
       "      <td>0.576418</td>\n",
       "      <td>2.031282</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.252492</td>\n",
       "      <td>3.419876</td>\n",
       "      <td>2.983841</td>\n",
       "      <td>3.828355</td>\n",
       "      <td>0.96119</td>\n",
       "      <td>1.657220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.036542</td>\n",
       "      <td>-0.825459</td>\n",
       "      <td>-0.744291</td>\n",
       "      <td>-0.478004</td>\n",
       "      <td>0.101592</td>\n",
       "      <td>-1.041631</td>\n",
       "      <td>-0.851710</td>\n",
       "      <td>-0.892383</td>\n",
       "      <td>-1.667425</td>\n",
       "      <td>-1.740518</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158164</td>\n",
       "      <td>-0.001833</td>\n",
       "      <td>-0.662727</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.735136</td>\n",
       "      <td>-0.490815</td>\n",
       "      <td>-0.468673</td>\n",
       "      <td>-0.516699</td>\n",
       "      <td>0.96119</td>\n",
       "      <td>1.048855</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
       "0         -0.036542         -0.612395         -0.042812        -0.430207   \n",
       "1         -0.036542         -0.709242         -0.541642        -0.455426   \n",
       "2         -0.036542         -0.438069         -0.038915        -0.398390   \n",
       "3         -0.036542          1.944383          1.843386         1.715888   \n",
       "4         -0.036542         -0.825459         -0.744291        -0.478004   \n",
       "\n",
       "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
       "0       0.362621      -0.215795       0.192437     -0.436024   \n",
       "1       0.362621       0.059484       0.378603     -0.339697   \n",
       "2       2.711884       1.619397       2.203839      0.242241   \n",
       "3       1.145709       3.087551       2.746148      4.404317   \n",
       "4       0.101592      -1.041631      -0.851710     -0.892383   \n",
       "\n",
       "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('pwat', 'amax')  \\\n",
       "0          -0.152770          -0.874007  ...          1.177876   \n",
       "1           0.011576          -0.761778  ...          1.046658   \n",
       "2           0.797776          -0.683479  ...          4.255539   \n",
       "3           0.331387          -0.010107  ...          1.428383   \n",
       "4          -1.667425          -1.740518  ...         -0.158164   \n",
       "\n",
       "   ('pwat', 'mean')  ('pwat', 'var')  ('paramId_0', 'amin')  \\\n",
       "0          1.392828         0.661637                    0.0   \n",
       "1          1.339957         0.239326                    0.0   \n",
       "2          5.180989         1.582308                    0.0   \n",
       "3          0.576418         2.031282                    0.0   \n",
       "4         -0.001833        -0.662727                    0.0   \n",
       "\n",
       "   ('paramId_0', 'amax')  ('paramId_0', 'mean')  ('paramId_0', 'var')  \\\n",
       "0              -0.735136              -0.490815             -0.468673   \n",
       "1              -0.735136              -0.490815             -0.468673   \n",
       "2               1.166082               0.447299              0.170256   \n",
       "3               2.252492               3.419876              2.983841   \n",
       "4              -0.735136              -0.490815             -0.468673   \n",
       "\n",
       "   ('pres', 'count')  ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
       "0          -0.516699                       0.96119               0.744673  \n",
       "1          -0.516699                       0.96119               0.744673  \n",
       "2          -0.033916                       0.96119               0.136307  \n",
       "3           3.828355                       0.96119               1.657220  \n",
       "4          -0.516699                       0.96119               1.048855  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(feature_part_train_2_1.head())\n",
    "part_features_2_1_train = pd.DataFrame(scaler.fit_transform(feature_part_train_2_1))\n",
    "part_features_2_1_train.columns = feature_part_train_2_1.columns\n",
    "part_features_2_1_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
      "292               0.0              0.14           0.02900         0.002435   \n",
      "293               0.0              1.33           0.12625         0.043993   \n",
      "294               0.0              0.47           0.04800         0.011781   \n",
      "295               0.0              0.36           0.10225         0.007274   \n",
      "296               0.0              1.50           0.22900         0.187840   \n",
      "\n",
      "     ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
      "292            5.0           25.0         11.125     35.240385   \n",
      "293            8.0           53.0         25.250    174.756410   \n",
      "294            7.0           36.0         14.375     67.112179   \n",
      "295           15.0           43.0         25.250     29.730769   \n",
      "296           16.0           53.0         30.025    115.768590   \n",
      "\n",
      "     ('tozne', 'amin')  ('tozne', 'amax')  ...  ('pwat', 'amax')  \\\n",
      "292         246.800003         289.600006  ...         17.900000   \n",
      "293         243.399994         291.399994  ...         31.100000   \n",
      "294         257.000000         371.100006  ...         24.700001   \n",
      "295         278.200012         397.899994  ...         22.799999   \n",
      "296         265.700012         381.100006  ...         35.400002   \n",
      "\n",
      "     ('pwat', 'mean')  ('pwat', 'var')  ('paramId_0', 'amin')  \\\n",
      "292            8.0600        13.980923                    0.0   \n",
      "293           15.2250        54.741923                    0.0   \n",
      "294           10.2225        26.132044                    0.0   \n",
      "295           14.5500         7.674359                    0.0   \n",
      "296           18.6275        57.461017                    0.0   \n",
      "\n",
      "     ('paramId_0', 'amax')  ('paramId_0', 'mean')  ('paramId_0', 'var')  \\\n",
      "292                    0.0                   0.00              0.000000   \n",
      "293                    0.0                   0.00              0.000000   \n",
      "294                   23.0                   0.60             13.220513   \n",
      "295                   63.0                  10.15            389.976923   \n",
      "296                   11.0                   0.45              4.151282   \n",
      "\n",
      "     ('pres', 'count')  ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
      "292                  0                             0                      1  \n",
      "293                  0                             0                      1  \n",
      "294                  1                             0                      2  \n",
      "295                  9                             0                      2  \n",
      "296                  2                             0                      2  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>('cwat', 'amin')</th>\n",
       "      <th>('cwat', 'amax')</th>\n",
       "      <th>('cwat', 'mean')</th>\n",
       "      <th>('cwat', 'var')</th>\n",
       "      <th>('r', 'amin')</th>\n",
       "      <th>('r', 'amax')</th>\n",
       "      <th>('r', 'mean')</th>\n",
       "      <th>('r', 'var')</th>\n",
       "      <th>('tozne', 'amin')</th>\n",
       "      <th>('tozne', 'amax')</th>\n",
       "      <th>...</th>\n",
       "      <th>('pwat', 'amax')</th>\n",
       "      <th>('pwat', 'mean')</th>\n",
       "      <th>('pwat', 'var')</th>\n",
       "      <th>('paramId_0', 'amin')</th>\n",
       "      <th>('paramId_0', 'amax')</th>\n",
       "      <th>('paramId_0', 'mean')</th>\n",
       "      <th>('paramId_0', 'var')</th>\n",
       "      <th>('pres', 'count')</th>\n",
       "      <th>('macro_season', '&lt;lambda&gt;')</th>\n",
       "      <th>('month', '&lt;lambda&gt;')</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.036542</td>\n",
       "      <td>-0.670503</td>\n",
       "      <td>-0.354581</td>\n",
       "      <td>-0.426679</td>\n",
       "      <td>-0.942526</td>\n",
       "      <td>-0.766353</td>\n",
       "      <td>-1.187618</td>\n",
       "      <td>-0.309477</td>\n",
       "      <td>-1.543055</td>\n",
       "      <td>-1.333362</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.444458</td>\n",
       "      <td>-0.833008</td>\n",
       "      <td>-0.273497</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.735136</td>\n",
       "      <td>-0.490815</td>\n",
       "      <td>-0.468673</td>\n",
       "      <td>-0.516699</td>\n",
       "      <td>-1.040377</td>\n",
       "      <td>-1.688788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.036542</td>\n",
       "      <td>1.634471</td>\n",
       "      <td>1.161393</td>\n",
       "      <td>0.495557</td>\n",
       "      <td>-0.159438</td>\n",
       "      <td>1.802916</td>\n",
       "      <td>1.098984</td>\n",
       "      <td>2.980712</td>\n",
       "      <td>-1.694076</td>\n",
       "      <td>-1.286383</td>\n",
       "      <td>...</td>\n",
       "      <td>1.130160</td>\n",
       "      <td>0.532121</td>\n",
       "      <td>2.043170</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.735136</td>\n",
       "      <td>-0.490815</td>\n",
       "      <td>-0.468673</td>\n",
       "      <td>-0.516699</td>\n",
       "      <td>-1.040377</td>\n",
       "      <td>-1.688788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.036542</td>\n",
       "      <td>-0.031309</td>\n",
       "      <td>-0.058401</td>\n",
       "      <td>-0.219287</td>\n",
       "      <td>-0.420467</td>\n",
       "      <td>0.243003</td>\n",
       "      <td>-0.661497</td>\n",
       "      <td>0.442151</td>\n",
       "      <td>-1.089991</td>\n",
       "      <td>0.793765</td>\n",
       "      <td>...</td>\n",
       "      <td>0.366709</td>\n",
       "      <td>-0.420993</td>\n",
       "      <td>0.417116</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.306007</td>\n",
       "      <td>-0.219553</td>\n",
       "      <td>-0.306772</td>\n",
       "      <td>-0.033916</td>\n",
       "      <td>-1.040377</td>\n",
       "      <td>-1.384606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.036542</td>\n",
       "      <td>-0.244373</td>\n",
       "      <td>0.787271</td>\n",
       "      <td>-0.319286</td>\n",
       "      <td>1.667767</td>\n",
       "      <td>0.885320</td>\n",
       "      <td>1.098984</td>\n",
       "      <td>-0.439410</td>\n",
       "      <td>-0.148327</td>\n",
       "      <td>1.493237</td>\n",
       "      <td>...</td>\n",
       "      <td>0.140059</td>\n",
       "      <td>0.403515</td>\n",
       "      <td>-0.631933</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.116691</td>\n",
       "      <td>4.098030</td>\n",
       "      <td>4.307057</td>\n",
       "      <td>3.828355</td>\n",
       "      <td>-1.040377</td>\n",
       "      <td>-1.384606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.036542</td>\n",
       "      <td>1.963753</td>\n",
       "      <td>2.763103</td>\n",
       "      <td>3.687707</td>\n",
       "      <td>1.928797</td>\n",
       "      <td>1.802916</td>\n",
       "      <td>1.871978</td>\n",
       "      <td>1.589609</td>\n",
       "      <td>-0.703553</td>\n",
       "      <td>1.054762</td>\n",
       "      <td>...</td>\n",
       "      <td>1.643104</td>\n",
       "      <td>1.180390</td>\n",
       "      <td>2.197711</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.237198</td>\n",
       "      <td>-0.287368</td>\n",
       "      <td>-0.417836</td>\n",
       "      <td>0.448868</td>\n",
       "      <td>-1.040377</td>\n",
       "      <td>-1.384606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
       "0         -0.036542         -0.670503         -0.354581        -0.426679   \n",
       "1         -0.036542          1.634471          1.161393         0.495557   \n",
       "2         -0.036542         -0.031309         -0.058401        -0.219287   \n",
       "3         -0.036542         -0.244373          0.787271        -0.319286   \n",
       "4         -0.036542          1.963753          2.763103         3.687707   \n",
       "\n",
       "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
       "0      -0.942526      -0.766353      -1.187618     -0.309477   \n",
       "1      -0.159438       1.802916       1.098984      2.980712   \n",
       "2      -0.420467       0.243003      -0.661497      0.442151   \n",
       "3       1.667767       0.885320       1.098984     -0.439410   \n",
       "4       1.928797       1.802916       1.871978      1.589609   \n",
       "\n",
       "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('pwat', 'amax')  \\\n",
       "0          -1.543055          -1.333362  ...         -0.444458   \n",
       "1          -1.694076          -1.286383  ...          1.130160   \n",
       "2          -1.089991           0.793765  ...          0.366709   \n",
       "3          -0.148327           1.493237  ...          0.140059   \n",
       "4          -0.703553           1.054762  ...          1.643104   \n",
       "\n",
       "   ('pwat', 'mean')  ('pwat', 'var')  ('paramId_0', 'amin')  \\\n",
       "0         -0.833008        -0.273497                    0.0   \n",
       "1          0.532121         2.043170                    0.0   \n",
       "2         -0.420993         0.417116                    0.0   \n",
       "3          0.403515        -0.631933                    0.0   \n",
       "4          1.180390         2.197711                    0.0   \n",
       "\n",
       "   ('paramId_0', 'amax')  ('paramId_0', 'mean')  ('paramId_0', 'var')  \\\n",
       "0              -0.735136              -0.490815             -0.468673   \n",
       "1              -0.735136              -0.490815             -0.468673   \n",
       "2               0.306007              -0.219553             -0.306772   \n",
       "3               2.116691               4.098030              4.307057   \n",
       "4              -0.237198              -0.287368             -0.417836   \n",
       "\n",
       "   ('pres', 'count')  ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
       "0          -0.516699                     -1.040377              -1.688788  \n",
       "1          -0.516699                     -1.040377              -1.688788  \n",
       "2          -0.033916                     -1.040377              -1.384606  \n",
       "3           3.828355                     -1.040377              -1.384606  \n",
       "4           0.448868                     -1.040377              -1.384606  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tenday_no_null_test.head())\n",
    "tenday_part_features_test = pd.DataFrame(scaler.transform(tenday_no_null_test))\n",
    "tenday_part_features_test.columns = tenday_no_null_test.columns\n",
    "tenday_part_features_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.6722\n",
      "Test set accuracy: 0.6641\n",
      "Training Set Precision: 0.5058\n",
      "Training Set Recall: 0.7218\n",
      "Training Set F1 Score: 0.5948\n",
      "Test Set Precision: 0.252\n",
      "Test Set Recall: 0.6996\n",
      "Test Set F1 Score: 0.3705\n"
     ]
    }
   ],
   "source": [
    "clf_log_10day_part = LogisticRegression(class_weight = \"balanced\")\n",
    "clf_log_10day_part.fit(part_features_2_1_train, target_train_2_1)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_log_10day_part.score(part_features_2_1_train, \n",
    "                                                       target_train_2_1), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_log_10day_part.score(tenday_part_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_log = clf_log_10day_part.predict(part_features_2_1_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(target_train_2_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(target_train_2_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(target_train_2_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_log = clf_log_10day_part.predict(tenday_part_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_log), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_log), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_log), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.7155\n",
      "Test set accuracy: 0.8002\n",
      "Training Set Precision: 0.6131\n",
      "Training Set Recall: 0.3972\n",
      "Training Set F1 Score: 0.4821\n",
      "Test Set Precision: 0.332\n",
      "Test Set Recall: 0.409\n",
      "Test Set F1 Score: 0.3665\n"
     ]
    }
   ],
   "source": [
    "clf_log_10day_part = LogisticRegression()\n",
    "clf_log_10day_part.fit(part_features_2_1_train, target_train_2_1)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_log_10day_part.score(part_features_2_1_train, \n",
    "                                                       target_train_2_1), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_log_10day_part.score(tenday_part_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_log = clf_log_10day_part.predict(part_features_2_1_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(target_train_2_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(target_train_2_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(target_train_2_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_log = clf_log_10day_part.predict(tenday_part_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_log), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_log), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_log), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.6505\n",
      "Test set accuracy: 0.6324\n",
      "Training Set Precision: 0.4843\n",
      "Training Set Recall: 0.7519\n",
      "Training Set F1 Score: 0.5892\n",
      "Test Set Precision: 0.2382\n",
      "Test Set Recall: 0.7287\n",
      "Test Set F1 Score: 0.3591\n"
     ]
    }
   ],
   "source": [
    "clf_sgd_10day_part = SGDClassifier(loss = \"log\", class_weight = \"balanced\", random_state = 0)\n",
    "clf_sgd_10day_part.fit(part_features_2_1_train, target_train_2_1)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_sgd_10day_part.score(part_features_2_1_train, \n",
    "                                                       target_train_2_1), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_sgd_10day_part.score(tenday_part_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_sgd = clf_sgd_10day_part.predict(part_features_2_1_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(target_train_2_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(target_train_2_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(target_train_2_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_sgd = clf_sgd_10day_part.predict(tenday_part_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_sgd), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_sgd), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_sgd), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.7128\n",
      "Test set accuracy: 0.7723\n",
      "Training Set Precision: 0.5817\n",
      "Training Set Recall: 0.4931\n",
      "Training Set F1 Score: 0.5338\n",
      "Test Set Precision: 0.3076\n",
      "Test Set Recall: 0.4887\n",
      "Test Set F1 Score: 0.3776\n"
     ]
    }
   ],
   "source": [
    "clf_sgd_10day_part = SGDClassifier(loss = \"log\", random_state = 0)\n",
    "clf_sgd_10day_part.fit(part_features_2_1_train, target_train_2_1)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_sgd_10day_part.score(part_features_2_1_train, \n",
    "                                                       target_train_2_1), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_sgd_10day_part.score(tenday_part_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_sgd = clf_sgd_10day_part.predict(part_features_2_1_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(target_train_2_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(target_train_2_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(target_train_2_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_sgd = clf_sgd_10day_part.predict(tenday_part_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_sgd), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_sgd), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_sgd), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3:1 Ratio No Fire:Fire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Full Parameter List "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    71103\n",
       "1    23701\n",
       "Name: fire, dtype: int64"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "under_sampler_3_1 = RandomUnderSampler(sampling_strategy = 0.3333333333333, random_state = 0)\n",
    "feature_full_train_3_1, target_train_3_1 = under_sampler_3_1.fit_resample(tenday_full_train, tenday_target_train)\n",
    "target_train_3_1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 94804 entries, 0 to 94803\n",
      "Data columns (total 31 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   ('cwat', 'amin')              94804 non-null  float64\n",
      " 1   ('cwat', 'amax')              94804 non-null  float64\n",
      " 2   ('cwat', 'mean')              94804 non-null  float64\n",
      " 3   ('cwat', 'var')               94804 non-null  float64\n",
      " 4   ('r', 'amin')                 94804 non-null  float64\n",
      " 5   ('r', 'amax')                 94804 non-null  float64\n",
      " 6   ('r', 'mean')                 94804 non-null  float64\n",
      " 7   ('r', 'var')                  94804 non-null  float64\n",
      " 8   ('tozne', 'amin')             94804 non-null  float64\n",
      " 9   ('tozne', 'amax')             94804 non-null  float64\n",
      " 10  ('tozne', 'mean')             94804 non-null  float64\n",
      " 11  ('tozne', 'var')              94804 non-null  float64\n",
      " 12  ('gh', 'amin')                94804 non-null  float64\n",
      " 13  ('gh', 'amax')                94804 non-null  float64\n",
      " 14  ('gh', 'mean')                94804 non-null  float64\n",
      " 15  ('gh', 'var')                 94804 non-null  float64\n",
      " 16  ('pwat', 'amin')              94804 non-null  float64\n",
      " 17  ('pwat', 'amax')              94804 non-null  float64\n",
      " 18  ('pwat', 'mean')              94804 non-null  float64\n",
      " 19  ('pwat', 'var')               94804 non-null  float64\n",
      " 20  ('paramId_0', 'amin')         94804 non-null  float64\n",
      " 21  ('paramId_0', 'amax')         94804 non-null  float64\n",
      " 22  ('paramId_0', 'mean')         94804 non-null  float64\n",
      " 23  ('paramId_0', 'var')          94804 non-null  float64\n",
      " 24  ('pres', 'amin')              36131 non-null  float64\n",
      " 25  ('pres', 'amax')              36131 non-null  float64\n",
      " 26  ('pres', 'mean')              36131 non-null  float64\n",
      " 27  ('pres', 'var')               22560 non-null  float64\n",
      " 28  ('pres', 'count')             94804 non-null  int64  \n",
      " 29  ('macro_season', '<lambda>')  94804 non-null  int64  \n",
      " 30  ('month', '<lambda>')         94804 non-null  int64  \n",
      "dtypes: float64(28), int64(3)\n",
      "memory usage: 22.4 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 94804 entries, 0 to 94803\n",
      "Data columns (total 31 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   ('cwat', 'amin')              94804 non-null  float64\n",
      " 1   ('cwat', 'amax')              94804 non-null  float64\n",
      " 2   ('cwat', 'mean')              94804 non-null  float64\n",
      " 3   ('cwat', 'var')               94804 non-null  float64\n",
      " 4   ('r', 'amin')                 94804 non-null  float64\n",
      " 5   ('r', 'amax')                 94804 non-null  float64\n",
      " 6   ('r', 'mean')                 94804 non-null  float64\n",
      " 7   ('r', 'var')                  94804 non-null  float64\n",
      " 8   ('tozne', 'amin')             94804 non-null  float64\n",
      " 9   ('tozne', 'amax')             94804 non-null  float64\n",
      " 10  ('tozne', 'mean')             94804 non-null  float64\n",
      " 11  ('tozne', 'var')              94804 non-null  float64\n",
      " 12  ('gh', 'amin')                94804 non-null  float64\n",
      " 13  ('gh', 'amax')                94804 non-null  float64\n",
      " 14  ('gh', 'mean')                94804 non-null  float64\n",
      " 15  ('gh', 'var')                 94804 non-null  float64\n",
      " 16  ('pwat', 'amin')              94804 non-null  float64\n",
      " 17  ('pwat', 'amax')              94804 non-null  float64\n",
      " 18  ('pwat', 'mean')              94804 non-null  float64\n",
      " 19  ('pwat', 'var')               94804 non-null  float64\n",
      " 20  ('paramId_0', 'amin')         94804 non-null  float64\n",
      " 21  ('paramId_0', 'amax')         94804 non-null  float64\n",
      " 22  ('paramId_0', 'mean')         94804 non-null  float64\n",
      " 23  ('paramId_0', 'var')          94804 non-null  float64\n",
      " 24  ('pres', 'amin')              94804 non-null  float64\n",
      " 25  ('pres', 'amax')              94804 non-null  float64\n",
      " 26  ('pres', 'mean')              94804 non-null  float64\n",
      " 27  ('pres', 'var')               94804 non-null  float64\n",
      " 28  ('pres', 'count')             94804 non-null  float64\n",
      " 29  ('macro_season', '<lambda>')  94804 non-null  float64\n",
      " 30  ('month', '<lambda>')         94804 non-null  float64\n",
      "dtypes: float64(31)\n",
      "memory usage: 22.4 MB\n"
     ]
    }
   ],
   "source": [
    "feature_full_train_3_1.info()\n",
    "feature_imp_train_3_1 = pd.DataFrame(imp.fit_transform(feature_full_train_3_1))\n",
    "feature_imp_train_3_1.columns = feature_full_train_3_1.columns\n",
    "feature_imp_train_3_1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 35259 entries, 292 to 176294\n",
      "Data columns (total 31 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   ('cwat', 'amin')              35259 non-null  float64\n",
      " 1   ('cwat', 'amax')              35259 non-null  float64\n",
      " 2   ('cwat', 'mean')              35259 non-null  float64\n",
      " 3   ('cwat', 'var')               35259 non-null  float64\n",
      " 4   ('r', 'amin')                 35259 non-null  float64\n",
      " 5   ('r', 'amax')                 35259 non-null  float64\n",
      " 6   ('r', 'mean')                 35259 non-null  float64\n",
      " 7   ('r', 'var')                  35259 non-null  float64\n",
      " 8   ('tozne', 'amin')             35259 non-null  float64\n",
      " 9   ('tozne', 'amax')             35259 non-null  float64\n",
      " 10  ('tozne', 'mean')             35259 non-null  float64\n",
      " 11  ('tozne', 'var')              35259 non-null  float64\n",
      " 12  ('gh', 'amin')                35259 non-null  float64\n",
      " 13  ('gh', 'amax')                35259 non-null  float64\n",
      " 14  ('gh', 'mean')                35259 non-null  float64\n",
      " 15  ('gh', 'var')                 35259 non-null  float64\n",
      " 16  ('pwat', 'amin')              35259 non-null  float64\n",
      " 17  ('pwat', 'amax')              35259 non-null  float64\n",
      " 18  ('pwat', 'mean')              35259 non-null  float64\n",
      " 19  ('pwat', 'var')               35259 non-null  float64\n",
      " 20  ('paramId_0', 'amin')         35259 non-null  float64\n",
      " 21  ('paramId_0', 'amax')         35259 non-null  float64\n",
      " 22  ('paramId_0', 'mean')         35259 non-null  float64\n",
      " 23  ('paramId_0', 'var')          35259 non-null  float64\n",
      " 24  ('pres', 'amin')              15069 non-null  float64\n",
      " 25  ('pres', 'amax')              15069 non-null  float64\n",
      " 26  ('pres', 'mean')              15069 non-null  float64\n",
      " 27  ('pres', 'var')               9461 non-null   float64\n",
      " 28  ('pres', 'count')             35259 non-null  int64  \n",
      " 29  ('macro_season', '<lambda>')  35259 non-null  int64  \n",
      " 30  ('month', '<lambda>')         35259 non-null  int64  \n",
      "dtypes: float64(28), int64(3)\n",
      "memory usage: 8.6 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 35259 entries, 0 to 35258\n",
      "Data columns (total 31 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   ('cwat', 'amin')              35259 non-null  float64\n",
      " 1   ('cwat', 'amax')              35259 non-null  float64\n",
      " 2   ('cwat', 'mean')              35259 non-null  float64\n",
      " 3   ('cwat', 'var')               35259 non-null  float64\n",
      " 4   ('r', 'amin')                 35259 non-null  float64\n",
      " 5   ('r', 'amax')                 35259 non-null  float64\n",
      " 6   ('r', 'mean')                 35259 non-null  float64\n",
      " 7   ('r', 'var')                  35259 non-null  float64\n",
      " 8   ('tozne', 'amin')             35259 non-null  float64\n",
      " 9   ('tozne', 'amax')             35259 non-null  float64\n",
      " 10  ('tozne', 'mean')             35259 non-null  float64\n",
      " 11  ('tozne', 'var')              35259 non-null  float64\n",
      " 12  ('gh', 'amin')                35259 non-null  float64\n",
      " 13  ('gh', 'amax')                35259 non-null  float64\n",
      " 14  ('gh', 'mean')                35259 non-null  float64\n",
      " 15  ('gh', 'var')                 35259 non-null  float64\n",
      " 16  ('pwat', 'amin')              35259 non-null  float64\n",
      " 17  ('pwat', 'amax')              35259 non-null  float64\n",
      " 18  ('pwat', 'mean')              35259 non-null  float64\n",
      " 19  ('pwat', 'var')               35259 non-null  float64\n",
      " 20  ('paramId_0', 'amin')         35259 non-null  float64\n",
      " 21  ('paramId_0', 'amax')         35259 non-null  float64\n",
      " 22  ('paramId_0', 'mean')         35259 non-null  float64\n",
      " 23  ('paramId_0', 'var')          35259 non-null  float64\n",
      " 24  ('pres', 'amin')              35259 non-null  float64\n",
      " 25  ('pres', 'amax')              35259 non-null  float64\n",
      " 26  ('pres', 'mean')              35259 non-null  float64\n",
      " 27  ('pres', 'var')               35259 non-null  float64\n",
      " 28  ('pres', 'count')             35259 non-null  float64\n",
      " 29  ('macro_season', '<lambda>')  35259 non-null  float64\n",
      " 30  ('month', '<lambda>')         35259 non-null  float64\n",
      "dtypes: float64(31)\n",
      "memory usage: 8.3 MB\n"
     ]
    }
   ],
   "source": [
    "tenday_full_test.info()\n",
    "tenday_imp_test = pd.DataFrame(imp.fit_transform(tenday_full_test))\n",
    "tenday_imp_test.columns = tenday_full_test.columns\n",
    "tenday_imp_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
      "0               0.0              0.94           0.09325         0.035956   \n",
      "1               0.0              0.15           0.03375         0.001229   \n",
      "2               0.0              1.00           0.09400         0.044732   \n",
      "3               0.0              0.22           0.03325         0.004069   \n",
      "4               0.0              0.00           0.00000         0.000000   \n",
      "\n",
      "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
      "0           11.0           44.0         23.025     68.896795   \n",
      "1           11.0           35.0         23.725     28.973718   \n",
      "2            6.0           55.0         18.925    180.840385   \n",
      "3            7.0           17.0         11.125      6.214744   \n",
      "4           10.0           23.0         17.250     13.987179   \n",
      "\n",
      "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('paramId_0', 'amax')  \\\n",
      "0         254.199997         351.600006  ...                   22.0   \n",
      "1         281.299988         303.200012  ...                   17.0   \n",
      "2         238.899994         388.399994  ...                    5.0   \n",
      "3         267.399994         309.600006  ...                    0.0   \n",
      "4         276.299988         320.000000  ...                    0.0   \n",
      "\n",
      "   ('paramId_0', 'mean')  ('paramId_0', 'var')  ('pres', 'amin')  \\\n",
      "0                  0.550             12.100000           62070.0   \n",
      "1                  0.425              7.225000           42800.0   \n",
      "2                  0.200              0.830769           42800.0   \n",
      "3                  0.000              0.000000           42800.0   \n",
      "4                  0.000              0.000000           42800.0   \n",
      "\n",
      "   ('pres', 'amax')  ('pres', 'mean')  ('pres', 'var')  ('pres', 'count')  \\\n",
      "0           62070.0      62070.000000     4.781473e+07                1.0   \n",
      "1           55580.0      50202.222222     4.781473e+07                0.0   \n",
      "2           55580.0      50202.222222     4.781473e+07                0.0   \n",
      "3           55580.0      50202.222222     4.781473e+07                0.0   \n",
      "4           55580.0      50202.222222     4.781473e+07                0.0   \n",
      "\n",
      "   ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
      "0                           0.0                    1.0  \n",
      "1                           1.0                    9.0  \n",
      "2                           0.0                    2.0  \n",
      "3                           1.0                   10.0  \n",
      "4                           1.0                    9.0  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>('cwat', 'amin')</th>\n",
       "      <th>('cwat', 'amax')</th>\n",
       "      <th>('cwat', 'mean')</th>\n",
       "      <th>('cwat', 'var')</th>\n",
       "      <th>('r', 'amin')</th>\n",
       "      <th>('r', 'amax')</th>\n",
       "      <th>('r', 'mean')</th>\n",
       "      <th>('r', 'var')</th>\n",
       "      <th>('tozne', 'amin')</th>\n",
       "      <th>('tozne', 'amax')</th>\n",
       "      <th>...</th>\n",
       "      <th>('paramId_0', 'amax')</th>\n",
       "      <th>('paramId_0', 'mean')</th>\n",
       "      <th>('paramId_0', 'var')</th>\n",
       "      <th>('pres', 'amin')</th>\n",
       "      <th>('pres', 'amax')</th>\n",
       "      <th>('pres', 'mean')</th>\n",
       "      <th>('pres', 'var')</th>\n",
       "      <th>('pres', 'count')</th>\n",
       "      <th>('macro_season', '&lt;lambda&gt;')</th>\n",
       "      <th>('month', '&lt;lambda&gt;')</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.042228</td>\n",
       "      <td>0.843153</td>\n",
       "      <td>0.598047</td>\n",
       "      <td>0.288640</td>\n",
       "      <td>0.592157</td>\n",
       "      <td>0.941862</td>\n",
       "      <td>0.700307</td>\n",
       "      <td>0.457549</td>\n",
       "      <td>-1.184721</td>\n",
       "      <td>0.263338</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232911</td>\n",
       "      <td>-0.256608</td>\n",
       "      <td>-0.333671</td>\n",
       "      <td>2.366474</td>\n",
       "      <td>0.781172</td>\n",
       "      <td>1.571377</td>\n",
       "      <td>-0.154864</td>\n",
       "      <td>-0.055399</td>\n",
       "      <td>-1.017430</td>\n",
       "      <td>-1.631946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.042228</td>\n",
       "      <td>-0.668272</td>\n",
       "      <td>-0.310919</td>\n",
       "      <td>-0.460649</td>\n",
       "      <td>0.592157</td>\n",
       "      <td>0.122138</td>\n",
       "      <td>0.812521</td>\n",
       "      <td>-0.471397</td>\n",
       "      <td>0.007457</td>\n",
       "      <td>-0.989670</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009109</td>\n",
       "      <td>-0.311730</td>\n",
       "      <td>-0.392144</td>\n",
       "      <td>-0.065329</td>\n",
       "      <td>0.064751</td>\n",
       "      <td>0.048447</td>\n",
       "      <td>-0.154864</td>\n",
       "      <td>-0.525930</td>\n",
       "      <td>0.982869</td>\n",
       "      <td>0.742677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.042228</td>\n",
       "      <td>0.957944</td>\n",
       "      <td>0.609505</td>\n",
       "      <td>0.478009</td>\n",
       "      <td>-0.701968</td>\n",
       "      <td>1.943747</td>\n",
       "      <td>0.043051</td>\n",
       "      <td>3.062298</td>\n",
       "      <td>-1.857796</td>\n",
       "      <td>1.216038</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.528013</td>\n",
       "      <td>-0.410948</td>\n",
       "      <td>-0.468840</td>\n",
       "      <td>-0.065329</td>\n",
       "      <td>0.064751</td>\n",
       "      <td>0.048447</td>\n",
       "      <td>-0.154864</td>\n",
       "      <td>-0.525930</td>\n",
       "      <td>-1.017430</td>\n",
       "      <td>-1.335118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.042228</td>\n",
       "      <td>-0.534348</td>\n",
       "      <td>-0.318557</td>\n",
       "      <td>-0.399382</td>\n",
       "      <td>-0.443143</td>\n",
       "      <td>-1.517311</td>\n",
       "      <td>-1.207338</td>\n",
       "      <td>-1.000962</td>\n",
       "      <td>-0.604029</td>\n",
       "      <td>-0.823983</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.751815</td>\n",
       "      <td>-0.499142</td>\n",
       "      <td>-0.478805</td>\n",
       "      <td>-0.065329</td>\n",
       "      <td>0.064751</td>\n",
       "      <td>0.048447</td>\n",
       "      <td>-0.154864</td>\n",
       "      <td>-0.525930</td>\n",
       "      <td>0.982869</td>\n",
       "      <td>1.039505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.042228</td>\n",
       "      <td>-0.955251</td>\n",
       "      <td>-0.826508</td>\n",
       "      <td>-0.487171</td>\n",
       "      <td>0.333332</td>\n",
       "      <td>-0.970828</td>\n",
       "      <td>-0.225462</td>\n",
       "      <td>-0.820109</td>\n",
       "      <td>-0.212502</td>\n",
       "      <td>-0.554742</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.751815</td>\n",
       "      <td>-0.499142</td>\n",
       "      <td>-0.478805</td>\n",
       "      <td>-0.065329</td>\n",
       "      <td>0.064751</td>\n",
       "      <td>0.048447</td>\n",
       "      <td>-0.154864</td>\n",
       "      <td>-0.525930</td>\n",
       "      <td>0.982869</td>\n",
       "      <td>0.742677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
       "0         -0.042228          0.843153          0.598047         0.288640   \n",
       "1         -0.042228         -0.668272         -0.310919        -0.460649   \n",
       "2         -0.042228          0.957944          0.609505         0.478009   \n",
       "3         -0.042228         -0.534348         -0.318557        -0.399382   \n",
       "4         -0.042228         -0.955251         -0.826508        -0.487171   \n",
       "\n",
       "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
       "0       0.592157       0.941862       0.700307      0.457549   \n",
       "1       0.592157       0.122138       0.812521     -0.471397   \n",
       "2      -0.701968       1.943747       0.043051      3.062298   \n",
       "3      -0.443143      -1.517311      -1.207338     -1.000962   \n",
       "4       0.333332      -0.970828      -0.225462     -0.820109   \n",
       "\n",
       "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('paramId_0', 'amax')  \\\n",
       "0          -1.184721           0.263338  ...               0.232911   \n",
       "1           0.007457          -0.989670  ...               0.009109   \n",
       "2          -1.857796           1.216038  ...              -0.528013   \n",
       "3          -0.604029          -0.823983  ...              -0.751815   \n",
       "4          -0.212502          -0.554742  ...              -0.751815   \n",
       "\n",
       "   ('paramId_0', 'mean')  ('paramId_0', 'var')  ('pres', 'amin')  \\\n",
       "0              -0.256608             -0.333671          2.366474   \n",
       "1              -0.311730             -0.392144         -0.065329   \n",
       "2              -0.410948             -0.468840         -0.065329   \n",
       "3              -0.499142             -0.478805         -0.065329   \n",
       "4              -0.499142             -0.478805         -0.065329   \n",
       "\n",
       "   ('pres', 'amax')  ('pres', 'mean')  ('pres', 'var')  ('pres', 'count')  \\\n",
       "0          0.781172          1.571377        -0.154864          -0.055399   \n",
       "1          0.064751          0.048447        -0.154864          -0.525930   \n",
       "2          0.064751          0.048447        -0.154864          -0.525930   \n",
       "3          0.064751          0.048447        -0.154864          -0.525930   \n",
       "4          0.064751          0.048447        -0.154864          -0.525930   \n",
       "\n",
       "   ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
       "0                     -1.017430              -1.631946  \n",
       "1                      0.982869               0.742677  \n",
       "2                     -1.017430              -1.335118  \n",
       "3                      0.982869               1.039505  \n",
       "4                      0.982869               0.742677  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(feature_imp_train_3_1.head())\n",
    "full_features_3_1_train = pd.DataFrame(scaler.fit_transform(feature_imp_train_3_1))\n",
    "full_features_3_1_train.columns = feature_imp_train_3_1.columns\n",
    "full_features_3_1_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>('cwat', 'amin')</th>\n",
       "      <th>('cwat', 'amax')</th>\n",
       "      <th>('cwat', 'mean')</th>\n",
       "      <th>('cwat', 'var')</th>\n",
       "      <th>('r', 'amin')</th>\n",
       "      <th>('r', 'amax')</th>\n",
       "      <th>('r', 'mean')</th>\n",
       "      <th>('r', 'var')</th>\n",
       "      <th>('tozne', 'amin')</th>\n",
       "      <th>('tozne', 'amax')</th>\n",
       "      <th>...</th>\n",
       "      <th>('paramId_0', 'amax')</th>\n",
       "      <th>('paramId_0', 'mean')</th>\n",
       "      <th>('paramId_0', 'var')</th>\n",
       "      <th>('pres', 'amin')</th>\n",
       "      <th>('pres', 'amax')</th>\n",
       "      <th>('pres', 'mean')</th>\n",
       "      <th>('pres', 'var')</th>\n",
       "      <th>('pres', 'count')</th>\n",
       "      <th>('macro_season', '&lt;lambda&gt;')</th>\n",
       "      <th>('month', '&lt;lambda&gt;')</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.042228</td>\n",
       "      <td>-0.687404</td>\n",
       "      <td>-0.383483</td>\n",
       "      <td>-0.434634</td>\n",
       "      <td>-0.960792</td>\n",
       "      <td>-0.788667</td>\n",
       "      <td>-1.207338</td>\n",
       "      <td>-0.325581</td>\n",
       "      <td>-1.510260</td>\n",
       "      <td>-1.341755</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.751815</td>\n",
       "      <td>-0.499142</td>\n",
       "      <td>-0.478805</td>\n",
       "      <td>-0.075425</td>\n",
       "      <td>0.023907</td>\n",
       "      <td>0.007098</td>\n",
       "      <td>-0.088189</td>\n",
       "      <td>-0.525930</td>\n",
       "      <td>-1.01743</td>\n",
       "      <td>-1.631946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.042228</td>\n",
       "      <td>1.589299</td>\n",
       "      <td>1.102180</td>\n",
       "      <td>0.462063</td>\n",
       "      <td>-0.184318</td>\n",
       "      <td>1.761586</td>\n",
       "      <td>1.056988</td>\n",
       "      <td>2.920733</td>\n",
       "      <td>-1.659833</td>\n",
       "      <td>-1.295156</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.751815</td>\n",
       "      <td>-0.499142</td>\n",
       "      <td>-0.478805</td>\n",
       "      <td>-0.075425</td>\n",
       "      <td>0.023907</td>\n",
       "      <td>0.007098</td>\n",
       "      <td>-0.088189</td>\n",
       "      <td>-0.525930</td>\n",
       "      <td>-1.01743</td>\n",
       "      <td>-1.631946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.042228</td>\n",
       "      <td>-0.056049</td>\n",
       "      <td>-0.093225</td>\n",
       "      <td>-0.232985</td>\n",
       "      <td>-0.443143</td>\n",
       "      <td>0.213218</td>\n",
       "      <td>-0.686342</td>\n",
       "      <td>0.416024</td>\n",
       "      <td>-1.061544</td>\n",
       "      <td>0.768166</td>\n",
       "      <td>...</td>\n",
       "      <td>0.277671</td>\n",
       "      <td>-0.234560</td>\n",
       "      <td>-0.320231</td>\n",
       "      <td>4.245537</td>\n",
       "      <td>2.424858</td>\n",
       "      <td>3.482132</td>\n",
       "      <td>-0.088189</td>\n",
       "      <td>-0.055399</td>\n",
       "      <td>-1.01743</td>\n",
       "      <td>-1.335118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.042228</td>\n",
       "      <td>-0.266501</td>\n",
       "      <td>0.735538</td>\n",
       "      <td>-0.330215</td>\n",
       "      <td>1.627457</td>\n",
       "      <td>0.850782</td>\n",
       "      <td>1.056988</td>\n",
       "      <td>-0.453781</td>\n",
       "      <td>-0.128917</td>\n",
       "      <td>1.461980</td>\n",
       "      <td>...</td>\n",
       "      <td>2.068080</td>\n",
       "      <td>3.976705</td>\n",
       "      <td>4.198800</td>\n",
       "      <td>-0.228122</td>\n",
       "      <td>2.520896</td>\n",
       "      <td>1.306315</td>\n",
       "      <td>3.529043</td>\n",
       "      <td>3.708847</td>\n",
       "      <td>-1.01743</td>\n",
       "      <td>-1.335118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.042228</td>\n",
       "      <td>1.914542</td>\n",
       "      <td>2.671864</td>\n",
       "      <td>3.565813</td>\n",
       "      <td>1.886282</td>\n",
       "      <td>1.761586</td>\n",
       "      <td>1.822451</td>\n",
       "      <td>1.548181</td>\n",
       "      <td>-0.678815</td>\n",
       "      <td>1.027052</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.259452</td>\n",
       "      <td>-0.300705</td>\n",
       "      <td>-0.429012</td>\n",
       "      <td>0.199683</td>\n",
       "      <td>1.603567</td>\n",
       "      <td>0.947719</td>\n",
       "      <td>4.762436</td>\n",
       "      <td>0.415132</td>\n",
       "      <td>-1.01743</td>\n",
       "      <td>-1.335118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
       "0         -0.042228         -0.687404         -0.383483        -0.434634   \n",
       "1         -0.042228          1.589299          1.102180         0.462063   \n",
       "2         -0.042228         -0.056049         -0.093225        -0.232985   \n",
       "3         -0.042228         -0.266501          0.735538        -0.330215   \n",
       "4         -0.042228          1.914542          2.671864         3.565813   \n",
       "\n",
       "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
       "0      -0.960792      -0.788667      -1.207338     -0.325581   \n",
       "1      -0.184318       1.761586       1.056988      2.920733   \n",
       "2      -0.443143       0.213218      -0.686342      0.416024   \n",
       "3       1.627457       0.850782       1.056988     -0.453781   \n",
       "4       1.886282       1.761586       1.822451      1.548181   \n",
       "\n",
       "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('paramId_0', 'amax')  \\\n",
       "0          -1.510260          -1.341755  ...              -0.751815   \n",
       "1          -1.659833          -1.295156  ...              -0.751815   \n",
       "2          -1.061544           0.768166  ...               0.277671   \n",
       "3          -0.128917           1.461980  ...               2.068080   \n",
       "4          -0.678815           1.027052  ...              -0.259452   \n",
       "\n",
       "   ('paramId_0', 'mean')  ('paramId_0', 'var')  ('pres', 'amin')  \\\n",
       "0              -0.499142             -0.478805         -0.075425   \n",
       "1              -0.499142             -0.478805         -0.075425   \n",
       "2              -0.234560             -0.320231          4.245537   \n",
       "3               3.976705              4.198800         -0.228122   \n",
       "4              -0.300705             -0.429012          0.199683   \n",
       "\n",
       "   ('pres', 'amax')  ('pres', 'mean')  ('pres', 'var')  ('pres', 'count')  \\\n",
       "0          0.023907          0.007098        -0.088189          -0.525930   \n",
       "1          0.023907          0.007098        -0.088189          -0.525930   \n",
       "2          2.424858          3.482132        -0.088189          -0.055399   \n",
       "3          2.520896          1.306315         3.529043           3.708847   \n",
       "4          1.603567          0.947719         4.762436           0.415132   \n",
       "\n",
       "   ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
       "0                      -1.01743              -1.631946  \n",
       "1                      -1.01743              -1.631946  \n",
       "2                      -1.01743              -1.335118  \n",
       "3                      -1.01743              -1.335118  \n",
       "4                      -1.01743              -1.335118  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tenday_full_features_test = pd.DataFrame(scaler.transform(tenday_imp_test))\n",
    "tenday_full_features_test.columns = tenday_imp_test.columns\n",
    "tenday_full_features_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.6632\n",
      "Test set accuracy: 0.6646\n",
      "Training Set Precision: 0.403\n",
      "Training Set Recall: 0.7217\n",
      "Training Set F1 Score: 0.5172\n",
      "Test Set Precision: 0.2529\n",
      "Test Set Recall: 0.7028\n",
      "Test Set F1 Score: 0.372\n"
     ]
    }
   ],
   "source": [
    "clf_log_10day_full = LogisticRegression(class_weight = \"balanced\", max_iter = 100000)\n",
    "clf_log_10day_full.fit(full_features_3_1_train, target_train_3_1)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_log_10day_full.score(full_features_3_1_train, \n",
    "                                                       target_train_3_1), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_log_10day_full.score(tenday_full_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_log = clf_log_10day_full.predict(full_features_3_1_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(target_train_3_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(target_train_3_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(target_train_3_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_log = clf_log_10day_full.predict(tenday_full_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_log), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_log), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_log), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.7597\n",
      "Test set accuracy: 0.8419\n",
      "Training Set Precision: 0.5608\n",
      "Training Set Recall: 0.1783\n",
      "Training Set F1 Score: 0.2706\n",
      "Test Set Precision: 0.3938\n",
      "Test Set Recall: 0.2208\n",
      "Test Set F1 Score: 0.2829\n"
     ]
    }
   ],
   "source": [
    "clf_log_10day_full = LogisticRegression(max_iter = 100000)\n",
    "clf_log_10day_full.fit(full_features_3_1_train, target_train_3_1)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_log_10day_full.score(full_features_3_1_train, \n",
    "                                                       target_train_3_1), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_log_10day_full.score(tenday_full_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_log = clf_log_10day_full.predict(full_features_3_1_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(target_train_3_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(target_train_3_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(target_train_3_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_log = clf_log_10day_full.predict(tenday_full_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_log), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_log), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_log), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.622\n",
      "Test set accuracy: 0.5895\n",
      "Training Set Precision: 0.3764\n",
      "Training Set Recall: 0.7792\n",
      "Training Set F1 Score: 0.5076\n",
      "Test Set Precision: 0.2238\n",
      "Test Set Recall: 0.7714\n",
      "Test Set F1 Score: 0.3469\n"
     ]
    }
   ],
   "source": [
    "clf_sgd_10day_full = SGDClassifier(loss = \"log\", class_weight = \"balanced\", random_state = 0)\n",
    "clf_sgd_10day_full.fit(full_features_3_1_train, target_train_3_1)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_sgd_10day_full.score(full_features_3_1_train, \n",
    "                                                       target_train_3_1), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_sgd_10day_full.score(tenday_full_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_sgd = clf_sgd_10day_full.predict(full_features_3_1_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(target_train_3_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(target_train_3_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(target_train_3_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_sgd = clf_sgd_10day_full.predict(tenday_full_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_sgd), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_sgd), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_sgd), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.7575\n",
      "Test set accuracy: 0.8451\n",
      "Training Set Precision: 0.5688\n",
      "Training Set Recall: 0.1235\n",
      "Training Set F1 Score: 0.2029\n",
      "Test Set Precision: 0.3894\n",
      "Test Set Recall: 0.1688\n",
      "Test Set F1 Score: 0.2355\n"
     ]
    }
   ],
   "source": [
    "clf_sgd_10day_full = SGDClassifier(loss = \"log\", random_state = 0)\n",
    "clf_sgd_10day_full.fit(full_features_3_1_train, target_train_3_1)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_sgd_10day_full.score(full_features_3_1_train, \n",
    "                                                       target_train_3_1), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_sgd_10day_full.score(tenday_full_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_sgd = clf_sgd_10day_full.predict(full_features_3_1_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(target_train_3_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(target_train_3_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(target_train_3_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_sgd = clf_sgd_10day_full.predict(tenday_full_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_sgd), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_sgd), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_sgd), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Part Parameter List "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    71103\n",
       "1    23701\n",
       "Name: fire, dtype: int64"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_part_train_3_1, target_train_3_1 = under_sampler_3_1.fit_resample(tenday_no_null_train, tenday_target_train)\n",
    "target_train_3_1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
      "0               0.0              0.51           0.02825         0.007420   \n",
      "1               0.0              1.70           0.08500         0.084662   \n",
      "2               0.0              0.06           0.00475         0.000169   \n",
      "3               0.0              1.04           0.15425         0.052025   \n",
      "4               0.0              0.23           0.02725         0.002410   \n",
      "\n",
      "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
      "0            8.0           40.0         20.850     64.746154   \n",
      "1            9.0           35.0         18.175     41.378846   \n",
      "2            7.0           24.0         14.275     34.460897   \n",
      "3           10.0           31.0         19.575     20.712179   \n",
      "4            9.0           29.0         17.600     27.476923   \n",
      "\n",
      "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('pwat', 'amax')  \\\n",
      "0         254.000000         283.600006  ...         16.200001   \n",
      "1         302.399994         411.100006  ...         19.200001   \n",
      "2         255.000000         270.799988  ...         11.400000   \n",
      "3         317.000000         391.100006  ...         10.800000   \n",
      "4         310.799988         390.700012  ...         16.400000   \n",
      "\n",
      "   ('pwat', 'mean')  ('pwat', 'var')  ('paramId_0', 'amin')  \\\n",
      "0            8.7275         9.816405                    0.0   \n",
      "1            9.9600        11.548103                    0.0   \n",
      "2            6.6550         7.401000                    0.0   \n",
      "3            6.2425         2.964558                    0.0   \n",
      "4            9.8175         9.560455                    0.0   \n",
      "\n",
      "   ('paramId_0', 'amax')  ('paramId_0', 'mean')  ('paramId_0', 'var')  \\\n",
      "0                    0.0                  0.000              0.000000   \n",
      "1                    0.0                  0.000              0.000000   \n",
      "2                    0.0                  0.000              0.000000   \n",
      "3                   41.0                  2.625             82.240385   \n",
      "4                    0.0                  0.000              0.000000   \n",
      "\n",
      "   ('pres', 'count')  ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
      "0                  0                             1                     10  \n",
      "1                  0                             0                      4  \n",
      "2                  0                             1                     10  \n",
      "3                  4                             0                      2  \n",
      "4                  0                             0                      5  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>('cwat', 'amin')</th>\n",
       "      <th>('cwat', 'amax')</th>\n",
       "      <th>('cwat', 'mean')</th>\n",
       "      <th>('cwat', 'var')</th>\n",
       "      <th>('r', 'amin')</th>\n",
       "      <th>('r', 'amax')</th>\n",
       "      <th>('r', 'mean')</th>\n",
       "      <th>('r', 'var')</th>\n",
       "      <th>('tozne', 'amin')</th>\n",
       "      <th>('tozne', 'amax')</th>\n",
       "      <th>...</th>\n",
       "      <th>('pwat', 'amax')</th>\n",
       "      <th>('pwat', 'mean')</th>\n",
       "      <th>('pwat', 'var')</th>\n",
       "      <th>('paramId_0', 'amin')</th>\n",
       "      <th>('paramId_0', 'amax')</th>\n",
       "      <th>('paramId_0', 'mean')</th>\n",
       "      <th>('paramId_0', 'var')</th>\n",
       "      <th>('pres', 'count')</th>\n",
       "      <th>('macro_season', '&lt;lambda&gt;')</th>\n",
       "      <th>('month', '&lt;lambda&gt;')</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.039715</td>\n",
       "      <td>0.018827</td>\n",
       "      <td>-0.397132</td>\n",
       "      <td>-0.329749</td>\n",
       "      <td>-0.184150</td>\n",
       "      <td>0.576961</td>\n",
       "      <td>0.350918</td>\n",
       "      <td>0.358951</td>\n",
       "      <td>-1.198251</td>\n",
       "      <td>-1.501276</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.635190</td>\n",
       "      <td>-0.694108</td>\n",
       "      <td>-0.503126</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.752444</td>\n",
       "      <td>-0.500559</td>\n",
       "      <td>-0.479637</td>\n",
       "      <td>-0.528274</td>\n",
       "      <td>0.982745</td>\n",
       "      <td>1.039209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.039715</td>\n",
       "      <td>2.300278</td>\n",
       "      <td>0.472055</td>\n",
       "      <td>1.350633</td>\n",
       "      <td>0.074785</td>\n",
       "      <td>0.121827</td>\n",
       "      <td>-0.077854</td>\n",
       "      <td>-0.182938</td>\n",
       "      <td>0.933936</td>\n",
       "      <td>1.798704</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.280891</td>\n",
       "      <td>-0.460477</td>\n",
       "      <td>-0.405949</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.752444</td>\n",
       "      <td>-0.500559</td>\n",
       "      <td>-0.479637</td>\n",
       "      <td>-0.528274</td>\n",
       "      <td>-1.017558</td>\n",
       "      <td>-0.740918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.039715</td>\n",
       "      <td>-0.843906</td>\n",
       "      <td>-0.757060</td>\n",
       "      <td>-0.487489</td>\n",
       "      <td>-0.443084</td>\n",
       "      <td>-0.879468</td>\n",
       "      <td>-0.702979</td>\n",
       "      <td>-0.343366</td>\n",
       "      <td>-1.154197</td>\n",
       "      <td>-1.832569</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.202069</td>\n",
       "      <td>-1.086968</td>\n",
       "      <td>-0.638670</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.752444</td>\n",
       "      <td>-0.500559</td>\n",
       "      <td>-0.479637</td>\n",
       "      <td>-0.528274</td>\n",
       "      <td>0.982745</td>\n",
       "      <td>1.039209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.039715</td>\n",
       "      <td>1.034935</td>\n",
       "      <td>1.532693</td>\n",
       "      <td>0.640630</td>\n",
       "      <td>0.333719</td>\n",
       "      <td>-0.242281</td>\n",
       "      <td>0.146550</td>\n",
       "      <td>-0.662199</td>\n",
       "      <td>1.577116</td>\n",
       "      <td>1.281060</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.272929</td>\n",
       "      <td>-1.165160</td>\n",
       "      <td>-0.887627</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.081847</td>\n",
       "      <td>0.659958</td>\n",
       "      <td>0.507163</td>\n",
       "      <td>1.361579</td>\n",
       "      <td>-1.017558</td>\n",
       "      <td>-1.334294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.039715</td>\n",
       "      <td>-0.517985</td>\n",
       "      <td>-0.412448</td>\n",
       "      <td>-0.438736</td>\n",
       "      <td>0.074785</td>\n",
       "      <td>-0.424334</td>\n",
       "      <td>-0.170020</td>\n",
       "      <td>-0.505324</td>\n",
       "      <td>1.303984</td>\n",
       "      <td>1.270707</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.611571</td>\n",
       "      <td>-0.487489</td>\n",
       "      <td>-0.517489</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.752444</td>\n",
       "      <td>-0.500559</td>\n",
       "      <td>-0.479637</td>\n",
       "      <td>-0.528274</td>\n",
       "      <td>-1.017558</td>\n",
       "      <td>-0.444231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
       "0         -0.039715          0.018827         -0.397132        -0.329749   \n",
       "1         -0.039715          2.300278          0.472055         1.350633   \n",
       "2         -0.039715         -0.843906         -0.757060        -0.487489   \n",
       "3         -0.039715          1.034935          1.532693         0.640630   \n",
       "4         -0.039715         -0.517985         -0.412448        -0.438736   \n",
       "\n",
       "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
       "0      -0.184150       0.576961       0.350918      0.358951   \n",
       "1       0.074785       0.121827      -0.077854     -0.182938   \n",
       "2      -0.443084      -0.879468      -0.702979     -0.343366   \n",
       "3       0.333719      -0.242281       0.146550     -0.662199   \n",
       "4       0.074785      -0.424334      -0.170020     -0.505324   \n",
       "\n",
       "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('pwat', 'amax')  \\\n",
       "0          -1.198251          -1.501276  ...         -0.635190   \n",
       "1           0.933936           1.798704  ...         -0.280891   \n",
       "2          -1.154197          -1.832569  ...         -1.202069   \n",
       "3           1.577116           1.281060  ...         -1.272929   \n",
       "4           1.303984           1.270707  ...         -0.611571   \n",
       "\n",
       "   ('pwat', 'mean')  ('pwat', 'var')  ('paramId_0', 'amin')  \\\n",
       "0         -0.694108        -0.503126                    0.0   \n",
       "1         -0.460477        -0.405949                    0.0   \n",
       "2         -1.086968        -0.638670                    0.0   \n",
       "3         -1.165160        -0.887627                    0.0   \n",
       "4         -0.487489        -0.517489                    0.0   \n",
       "\n",
       "   ('paramId_0', 'amax')  ('paramId_0', 'mean')  ('paramId_0', 'var')  \\\n",
       "0              -0.752444              -0.500559             -0.479637   \n",
       "1              -0.752444              -0.500559             -0.479637   \n",
       "2              -0.752444              -0.500559             -0.479637   \n",
       "3               1.081847               0.659958              0.507163   \n",
       "4              -0.752444              -0.500559             -0.479637   \n",
       "\n",
       "   ('pres', 'count')  ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
       "0          -0.528274                      0.982745               1.039209  \n",
       "1          -0.528274                     -1.017558              -0.740918  \n",
       "2          -0.528274                      0.982745               1.039209  \n",
       "3           1.361579                     -1.017558              -1.334294  \n",
       "4          -0.528274                     -1.017558              -0.444231  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(feature_part_train_3_1.head())\n",
    "part_features_3_1_train = pd.DataFrame(scaler.fit_transform(feature_part_train_3_1))\n",
    "part_features_3_1_train.columns = feature_part_train_3_1.columns\n",
    "part_features_3_1_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
      "292               0.0              0.14           0.02900         0.002435   \n",
      "293               0.0              1.33           0.12625         0.043993   \n",
      "294               0.0              0.47           0.04800         0.011781   \n",
      "295               0.0              0.36           0.10225         0.007274   \n",
      "296               0.0              1.50           0.22900         0.187840   \n",
      "\n",
      "     ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
      "292            5.0           25.0         11.125     35.240385   \n",
      "293            8.0           53.0         25.250    174.756410   \n",
      "294            7.0           36.0         14.375     67.112179   \n",
      "295           15.0           43.0         25.250     29.730769   \n",
      "296           16.0           53.0         30.025    115.768590   \n",
      "\n",
      "     ('tozne', 'amin')  ('tozne', 'amax')  ...  ('pwat', 'amax')  \\\n",
      "292         246.800003         289.600006  ...         17.900000   \n",
      "293         243.399994         291.399994  ...         31.100000   \n",
      "294         257.000000         371.100006  ...         24.700001   \n",
      "295         278.200012         397.899994  ...         22.799999   \n",
      "296         265.700012         381.100006  ...         35.400002   \n",
      "\n",
      "     ('pwat', 'mean')  ('pwat', 'var')  ('paramId_0', 'amin')  \\\n",
      "292            8.0600        13.980923                    0.0   \n",
      "293           15.2250        54.741923                    0.0   \n",
      "294           10.2225        26.132044                    0.0   \n",
      "295           14.5500         7.674359                    0.0   \n",
      "296           18.6275        57.461017                    0.0   \n",
      "\n",
      "     ('paramId_0', 'amax')  ('paramId_0', 'mean')  ('paramId_0', 'var')  \\\n",
      "292                    0.0                   0.00              0.000000   \n",
      "293                    0.0                   0.00              0.000000   \n",
      "294                   23.0                   0.60             13.220513   \n",
      "295                   63.0                  10.15            389.976923   \n",
      "296                   11.0                   0.45              4.151282   \n",
      "\n",
      "     ('pres', 'count')  ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
      "292                  0                             0                      1  \n",
      "293                  0                             0                      1  \n",
      "294                  1                             0                      2  \n",
      "295                  9                             0                      2  \n",
      "296                  2                             0                      2  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>('cwat', 'amin')</th>\n",
       "      <th>('cwat', 'amax')</th>\n",
       "      <th>('cwat', 'mean')</th>\n",
       "      <th>('cwat', 'var')</th>\n",
       "      <th>('r', 'amin')</th>\n",
       "      <th>('r', 'amax')</th>\n",
       "      <th>('r', 'mean')</th>\n",
       "      <th>('r', 'var')</th>\n",
       "      <th>('tozne', 'amin')</th>\n",
       "      <th>('tozne', 'amax')</th>\n",
       "      <th>...</th>\n",
       "      <th>('pwat', 'amax')</th>\n",
       "      <th>('pwat', 'mean')</th>\n",
       "      <th>('pwat', 'var')</th>\n",
       "      <th>('paramId_0', 'amin')</th>\n",
       "      <th>('paramId_0', 'amax')</th>\n",
       "      <th>('paramId_0', 'mean')</th>\n",
       "      <th>('paramId_0', 'var')</th>\n",
       "      <th>('pres', 'count')</th>\n",
       "      <th>('macro_season', '&lt;lambda&gt;')</th>\n",
       "      <th>('month', '&lt;lambda&gt;')</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.039715</td>\n",
       "      <td>-0.690531</td>\n",
       "      <td>-0.385645</td>\n",
       "      <td>-0.438199</td>\n",
       "      <td>-0.960952</td>\n",
       "      <td>-0.788442</td>\n",
       "      <td>-1.207888</td>\n",
       "      <td>-0.325289</td>\n",
       "      <td>-1.515435</td>\n",
       "      <td>-1.345983</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.434421</td>\n",
       "      <td>-0.820638</td>\n",
       "      <td>-0.269428</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.752444</td>\n",
       "      <td>-0.500559</td>\n",
       "      <td>-0.479637</td>\n",
       "      <td>-0.528274</td>\n",
       "      <td>-1.017558</td>\n",
       "      <td>-1.630982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.039715</td>\n",
       "      <td>1.590919</td>\n",
       "      <td>1.103843</td>\n",
       "      <td>0.465899</td>\n",
       "      <td>-0.184150</td>\n",
       "      <td>1.760310</td>\n",
       "      <td>1.056187</td>\n",
       "      <td>2.910093</td>\n",
       "      <td>-1.665217</td>\n",
       "      <td>-1.299395</td>\n",
       "      <td>...</td>\n",
       "      <td>1.124495</td>\n",
       "      <td>0.537547</td>\n",
       "      <td>2.017931</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.752444</td>\n",
       "      <td>-0.500559</td>\n",
       "      <td>-0.479637</td>\n",
       "      <td>-0.528274</td>\n",
       "      <td>-1.017558</td>\n",
       "      <td>-1.630982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.039715</td>\n",
       "      <td>-0.057860</td>\n",
       "      <td>-0.094639</td>\n",
       "      <td>-0.234885</td>\n",
       "      <td>-0.443084</td>\n",
       "      <td>0.212854</td>\n",
       "      <td>-0.686950</td>\n",
       "      <td>0.413819</td>\n",
       "      <td>-1.066090</td>\n",
       "      <td>0.763416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.368657</td>\n",
       "      <td>-0.410718</td>\n",
       "      <td>0.412448</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276549</td>\n",
       "      <td>-0.235298</td>\n",
       "      <td>-0.321005</td>\n",
       "      <td>-0.055811</td>\n",
       "      <td>-1.017558</td>\n",
       "      <td>-1.334294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.039715</td>\n",
       "      <td>-0.268751</td>\n",
       "      <td>0.736258</td>\n",
       "      <td>-0.332918</td>\n",
       "      <td>1.628390</td>\n",
       "      <td>0.850041</td>\n",
       "      <td>1.056187</td>\n",
       "      <td>-0.453058</td>\n",
       "      <td>-0.132157</td>\n",
       "      <td>1.457059</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144268</td>\n",
       "      <td>0.409595</td>\n",
       "      <td>-0.623330</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.066101</td>\n",
       "      <td>3.986773</td>\n",
       "      <td>4.199683</td>\n",
       "      <td>3.723895</td>\n",
       "      <td>-1.017558</td>\n",
       "      <td>-1.334294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.039715</td>\n",
       "      <td>1.916840</td>\n",
       "      <td>2.677570</td>\n",
       "      <td>3.595268</td>\n",
       "      <td>1.887324</td>\n",
       "      <td>1.760310</td>\n",
       "      <td>1.821565</td>\n",
       "      <td>1.542163</td>\n",
       "      <td>-0.682825</td>\n",
       "      <td>1.022238</td>\n",
       "      <td>...</td>\n",
       "      <td>1.632324</td>\n",
       "      <td>1.182520</td>\n",
       "      <td>2.170517</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.260317</td>\n",
       "      <td>-0.301613</td>\n",
       "      <td>-0.429826</td>\n",
       "      <td>0.416652</td>\n",
       "      <td>-1.017558</td>\n",
       "      <td>-1.334294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
       "0         -0.039715         -0.690531         -0.385645        -0.438199   \n",
       "1         -0.039715          1.590919          1.103843         0.465899   \n",
       "2         -0.039715         -0.057860         -0.094639        -0.234885   \n",
       "3         -0.039715         -0.268751          0.736258        -0.332918   \n",
       "4         -0.039715          1.916840          2.677570         3.595268   \n",
       "\n",
       "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
       "0      -0.960952      -0.788442      -1.207888     -0.325289   \n",
       "1      -0.184150       1.760310       1.056187      2.910093   \n",
       "2      -0.443084       0.212854      -0.686950      0.413819   \n",
       "3       1.628390       0.850041       1.056187     -0.453058   \n",
       "4       1.887324       1.760310       1.821565      1.542163   \n",
       "\n",
       "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('pwat', 'amax')  \\\n",
       "0          -1.515435          -1.345983  ...         -0.434421   \n",
       "1          -1.665217          -1.299395  ...          1.124495   \n",
       "2          -1.066090           0.763416  ...          0.368657   \n",
       "3          -0.132157           1.457059  ...          0.144268   \n",
       "4          -0.682825           1.022238  ...          1.632324   \n",
       "\n",
       "   ('pwat', 'mean')  ('pwat', 'var')  ('paramId_0', 'amin')  \\\n",
       "0         -0.820638        -0.269428                    0.0   \n",
       "1          0.537547         2.017931                    0.0   \n",
       "2         -0.410718         0.412448                    0.0   \n",
       "3          0.409595        -0.623330                    0.0   \n",
       "4          1.182520         2.170517                    0.0   \n",
       "\n",
       "   ('paramId_0', 'amax')  ('paramId_0', 'mean')  ('paramId_0', 'var')  \\\n",
       "0              -0.752444              -0.500559             -0.479637   \n",
       "1              -0.752444              -0.500559             -0.479637   \n",
       "2               0.276549              -0.235298             -0.321005   \n",
       "3               2.066101               3.986773              4.199683   \n",
       "4              -0.260317              -0.301613             -0.429826   \n",
       "\n",
       "   ('pres', 'count')  ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
       "0          -0.528274                     -1.017558              -1.630982  \n",
       "1          -0.528274                     -1.017558              -1.630982  \n",
       "2          -0.055811                     -1.017558              -1.334294  \n",
       "3           3.723895                     -1.017558              -1.334294  \n",
       "4           0.416652                     -1.017558              -1.334294  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tenday_no_null_test.head())\n",
    "tenday_part_features_test = pd.DataFrame(scaler.transform(tenday_no_null_test))\n",
    "tenday_part_features_test.columns = tenday_no_null_test.columns\n",
    "tenday_part_features_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.6643\n",
      "Test set accuracy: 0.6653\n",
      "Training Set Precision: 0.4039\n",
      "Training Set Recall: 0.7201\n",
      "Training Set F1 Score: 0.5175\n",
      "Test Set Precision: 0.2527\n",
      "Test Set Recall: 0.6988\n",
      "Test Set F1 Score: 0.3711\n"
     ]
    }
   ],
   "source": [
    "clf_log_10day_part = LogisticRegression(class_weight = \"balanced\")\n",
    "clf_log_10day_part.fit(part_features_3_1_train, target_train_3_1)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_log_10day_part.score(part_features_3_1_train, \n",
    "                                                       target_train_3_1), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_log_10day_part.score(tenday_part_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_log = clf_log_10day_part.predict(part_features_3_1_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(target_train_3_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(target_train_3_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(target_train_3_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_log = clf_log_10day_part.predict(tenday_part_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_log), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_log), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_log), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.7591\n",
      "Test set accuracy: 0.8406\n",
      "Training Set Precision: 0.5555\n",
      "Training Set Recall: 0.1821\n",
      "Training Set F1 Score: 0.2743\n",
      "Test Set Precision: 0.3864\n",
      "Test Set Recall: 0.2181\n",
      "Test Set F1 Score: 0.2789\n"
     ]
    }
   ],
   "source": [
    "clf_log_10day_part = LogisticRegression()\n",
    "clf_log_10day_part.fit(part_features_3_1_train, target_train_3_1)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_log_10day_part.score(part_features_3_1_train, \n",
    "                                                       target_train_3_1), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_log_10day_part.score(tenday_part_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_log = clf_log_10day_part.predict(part_features_3_1_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(target_train_3_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(target_train_3_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(target_train_3_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_log = clf_log_10day_part.predict(tenday_part_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_log), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_log), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_log), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.6313\n",
      "Test set accuracy: 0.6\n",
      "Training Set Precision: 0.3818\n",
      "Training Set Recall: 0.7668\n",
      "Training Set F1 Score: 0.5098\n",
      "Test Set Precision: 0.2272\n",
      "Test Set Recall: 0.7622\n",
      "Test Set F1 Score: 0.3501\n"
     ]
    }
   ],
   "source": [
    "clf_sgd_10day_part = SGDClassifier(loss = \"log\", class_weight = \"balanced\", random_state = 0)\n",
    "clf_sgd_10day_part.fit(part_features_3_1_train, target_train_3_1)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_sgd_10day_part.score(part_features_3_1_train, \n",
    "                                                       target_train_3_1), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_sgd_10day_part.score(tenday_part_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_sgd = clf_sgd_10day_part.predict(part_features_3_1_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(target_train_3_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(target_train_3_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(target_train_3_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_sgd = clf_sgd_10day_part.predict(tenday_part_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_sgd), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_sgd), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_sgd), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.7565\n",
      "Test set accuracy: 0.8357\n",
      "Training Set Precision: 0.5395\n",
      "Training Set Recall: 0.1765\n",
      "Training Set F1 Score: 0.266\n",
      "Test Set Precision: 0.3612\n",
      "Test Set Recall: 0.2117\n",
      "Test Set F1 Score: 0.267\n"
     ]
    }
   ],
   "source": [
    "clf_sgd_10day_part = SGDClassifier(loss = \"log\", random_state = 0)\n",
    "clf_sgd_10day_part.fit(part_features_3_1_train, target_train_3_1)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_sgd_10day_part.score(part_features_3_1_train, \n",
    "                                                       target_train_3_1), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_sgd_10day_part.score(tenday_part_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_sgd = clf_sgd_10day_part.predict(part_features_3_1_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(target_train_3_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(target_train_3_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(target_train_3_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_sgd = clf_sgd_10day_part.predict(tenday_part_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_sgd), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_sgd), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_sgd), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4:1 Ratio No Fire:Fire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Full Parameter List "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    94804\n",
       "1    23701\n",
       "Name: fire, dtype: int64"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "under_sampler_4_1 = RandomUnderSampler(sampling_strategy = 0.25, random_state = 0)\n",
    "feature_full_train_4_1, target_train_4_1 = under_sampler_4_1.fit_resample(tenday_full_train, tenday_target_train)\n",
    "target_train_4_1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 118505 entries, 0 to 118504\n",
      "Data columns (total 31 columns):\n",
      " #   Column                        Non-Null Count   Dtype  \n",
      "---  ------                        --------------   -----  \n",
      " 0   ('cwat', 'amin')              118505 non-null  float64\n",
      " 1   ('cwat', 'amax')              118505 non-null  float64\n",
      " 2   ('cwat', 'mean')              118505 non-null  float64\n",
      " 3   ('cwat', 'var')               118505 non-null  float64\n",
      " 4   ('r', 'amin')                 118505 non-null  float64\n",
      " 5   ('r', 'amax')                 118505 non-null  float64\n",
      " 6   ('r', 'mean')                 118505 non-null  float64\n",
      " 7   ('r', 'var')                  118505 non-null  float64\n",
      " 8   ('tozne', 'amin')             118505 non-null  float64\n",
      " 9   ('tozne', 'amax')             118505 non-null  float64\n",
      " 10  ('tozne', 'mean')             118505 non-null  float64\n",
      " 11  ('tozne', 'var')              118505 non-null  float64\n",
      " 12  ('gh', 'amin')                118505 non-null  float64\n",
      " 13  ('gh', 'amax')                118505 non-null  float64\n",
      " 14  ('gh', 'mean')                118505 non-null  float64\n",
      " 15  ('gh', 'var')                 118505 non-null  float64\n",
      " 16  ('pwat', 'amin')              118505 non-null  float64\n",
      " 17  ('pwat', 'amax')              118505 non-null  float64\n",
      " 18  ('pwat', 'mean')              118505 non-null  float64\n",
      " 19  ('pwat', 'var')               118505 non-null  float64\n",
      " 20  ('paramId_0', 'amin')         118505 non-null  float64\n",
      " 21  ('paramId_0', 'amax')         118505 non-null  float64\n",
      " 22  ('paramId_0', 'mean')         118505 non-null  float64\n",
      " 23  ('paramId_0', 'var')          118505 non-null  float64\n",
      " 24  ('pres', 'amin')              46083 non-null   float64\n",
      " 25  ('pres', 'amax')              46083 non-null   float64\n",
      " 26  ('pres', 'mean')              46083 non-null   float64\n",
      " 27  ('pres', 'var')               28885 non-null   float64\n",
      " 28  ('pres', 'count')             118505 non-null  int64  \n",
      " 29  ('macro_season', '<lambda>')  118505 non-null  int64  \n",
      " 30  ('month', '<lambda>')         118505 non-null  int64  \n",
      "dtypes: float64(28), int64(3)\n",
      "memory usage: 28.0 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 118505 entries, 0 to 118504\n",
      "Data columns (total 31 columns):\n",
      " #   Column                        Non-Null Count   Dtype  \n",
      "---  ------                        --------------   -----  \n",
      " 0   ('cwat', 'amin')              118505 non-null  float64\n",
      " 1   ('cwat', 'amax')              118505 non-null  float64\n",
      " 2   ('cwat', 'mean')              118505 non-null  float64\n",
      " 3   ('cwat', 'var')               118505 non-null  float64\n",
      " 4   ('r', 'amin')                 118505 non-null  float64\n",
      " 5   ('r', 'amax')                 118505 non-null  float64\n",
      " 6   ('r', 'mean')                 118505 non-null  float64\n",
      " 7   ('r', 'var')                  118505 non-null  float64\n",
      " 8   ('tozne', 'amin')             118505 non-null  float64\n",
      " 9   ('tozne', 'amax')             118505 non-null  float64\n",
      " 10  ('tozne', 'mean')             118505 non-null  float64\n",
      " 11  ('tozne', 'var')              118505 non-null  float64\n",
      " 12  ('gh', 'amin')                118505 non-null  float64\n",
      " 13  ('gh', 'amax')                118505 non-null  float64\n",
      " 14  ('gh', 'mean')                118505 non-null  float64\n",
      " 15  ('gh', 'var')                 118505 non-null  float64\n",
      " 16  ('pwat', 'amin')              118505 non-null  float64\n",
      " 17  ('pwat', 'amax')              118505 non-null  float64\n",
      " 18  ('pwat', 'mean')              118505 non-null  float64\n",
      " 19  ('pwat', 'var')               118505 non-null  float64\n",
      " 20  ('paramId_0', 'amin')         118505 non-null  float64\n",
      " 21  ('paramId_0', 'amax')         118505 non-null  float64\n",
      " 22  ('paramId_0', 'mean')         118505 non-null  float64\n",
      " 23  ('paramId_0', 'var')          118505 non-null  float64\n",
      " 24  ('pres', 'amin')              118505 non-null  float64\n",
      " 25  ('pres', 'amax')              118505 non-null  float64\n",
      " 26  ('pres', 'mean')              118505 non-null  float64\n",
      " 27  ('pres', 'var')               118505 non-null  float64\n",
      " 28  ('pres', 'count')             118505 non-null  float64\n",
      " 29  ('macro_season', '<lambda>')  118505 non-null  float64\n",
      " 30  ('month', '<lambda>')         118505 non-null  float64\n",
      "dtypes: float64(31)\n",
      "memory usage: 28.0 MB\n"
     ]
    }
   ],
   "source": [
    "feature_full_train_4_1.info()\n",
    "feature_imp_train_4_1 = pd.DataFrame(imp.fit_transform(feature_full_train_4_1))\n",
    "feature_imp_train_4_1.columns = feature_full_train_4_1.columns\n",
    "feature_imp_train_4_1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 35259 entries, 292 to 176294\n",
      "Data columns (total 31 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   ('cwat', 'amin')              35259 non-null  float64\n",
      " 1   ('cwat', 'amax')              35259 non-null  float64\n",
      " 2   ('cwat', 'mean')              35259 non-null  float64\n",
      " 3   ('cwat', 'var')               35259 non-null  float64\n",
      " 4   ('r', 'amin')                 35259 non-null  float64\n",
      " 5   ('r', 'amax')                 35259 non-null  float64\n",
      " 6   ('r', 'mean')                 35259 non-null  float64\n",
      " 7   ('r', 'var')                  35259 non-null  float64\n",
      " 8   ('tozne', 'amin')             35259 non-null  float64\n",
      " 9   ('tozne', 'amax')             35259 non-null  float64\n",
      " 10  ('tozne', 'mean')             35259 non-null  float64\n",
      " 11  ('tozne', 'var')              35259 non-null  float64\n",
      " 12  ('gh', 'amin')                35259 non-null  float64\n",
      " 13  ('gh', 'amax')                35259 non-null  float64\n",
      " 14  ('gh', 'mean')                35259 non-null  float64\n",
      " 15  ('gh', 'var')                 35259 non-null  float64\n",
      " 16  ('pwat', 'amin')              35259 non-null  float64\n",
      " 17  ('pwat', 'amax')              35259 non-null  float64\n",
      " 18  ('pwat', 'mean')              35259 non-null  float64\n",
      " 19  ('pwat', 'var')               35259 non-null  float64\n",
      " 20  ('paramId_0', 'amin')         35259 non-null  float64\n",
      " 21  ('paramId_0', 'amax')         35259 non-null  float64\n",
      " 22  ('paramId_0', 'mean')         35259 non-null  float64\n",
      " 23  ('paramId_0', 'var')          35259 non-null  float64\n",
      " 24  ('pres', 'amin')              15069 non-null  float64\n",
      " 25  ('pres', 'amax')              15069 non-null  float64\n",
      " 26  ('pres', 'mean')              15069 non-null  float64\n",
      " 27  ('pres', 'var')               9461 non-null   float64\n",
      " 28  ('pres', 'count')             35259 non-null  int64  \n",
      " 29  ('macro_season', '<lambda>')  35259 non-null  int64  \n",
      " 30  ('month', '<lambda>')         35259 non-null  int64  \n",
      "dtypes: float64(28), int64(3)\n",
      "memory usage: 8.6 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 35259 entries, 0 to 35258\n",
      "Data columns (total 31 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   ('cwat', 'amin')              35259 non-null  float64\n",
      " 1   ('cwat', 'amax')              35259 non-null  float64\n",
      " 2   ('cwat', 'mean')              35259 non-null  float64\n",
      " 3   ('cwat', 'var')               35259 non-null  float64\n",
      " 4   ('r', 'amin')                 35259 non-null  float64\n",
      " 5   ('r', 'amax')                 35259 non-null  float64\n",
      " 6   ('r', 'mean')                 35259 non-null  float64\n",
      " 7   ('r', 'var')                  35259 non-null  float64\n",
      " 8   ('tozne', 'amin')             35259 non-null  float64\n",
      " 9   ('tozne', 'amax')             35259 non-null  float64\n",
      " 10  ('tozne', 'mean')             35259 non-null  float64\n",
      " 11  ('tozne', 'var')              35259 non-null  float64\n",
      " 12  ('gh', 'amin')                35259 non-null  float64\n",
      " 13  ('gh', 'amax')                35259 non-null  float64\n",
      " 14  ('gh', 'mean')                35259 non-null  float64\n",
      " 15  ('gh', 'var')                 35259 non-null  float64\n",
      " 16  ('pwat', 'amin')              35259 non-null  float64\n",
      " 17  ('pwat', 'amax')              35259 non-null  float64\n",
      " 18  ('pwat', 'mean')              35259 non-null  float64\n",
      " 19  ('pwat', 'var')               35259 non-null  float64\n",
      " 20  ('paramId_0', 'amin')         35259 non-null  float64\n",
      " 21  ('paramId_0', 'amax')         35259 non-null  float64\n",
      " 22  ('paramId_0', 'mean')         35259 non-null  float64\n",
      " 23  ('paramId_0', 'var')          35259 non-null  float64\n",
      " 24  ('pres', 'amin')              35259 non-null  float64\n",
      " 25  ('pres', 'amax')              35259 non-null  float64\n",
      " 26  ('pres', 'mean')              35259 non-null  float64\n",
      " 27  ('pres', 'var')               35259 non-null  float64\n",
      " 28  ('pres', 'count')             35259 non-null  float64\n",
      " 29  ('macro_season', '<lambda>')  35259 non-null  float64\n",
      " 30  ('month', '<lambda>')         35259 non-null  float64\n",
      "dtypes: float64(31)\n",
      "memory usage: 8.3 MB\n"
     ]
    }
   ],
   "source": [
    "tenday_full_test.info()\n",
    "tenday_imp_test = pd.DataFrame(imp.fit_transform(tenday_full_test))\n",
    "tenday_imp_test.columns = tenday_full_test.columns\n",
    "tenday_imp_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
      "0               0.0              1.52           0.22700         0.136206   \n",
      "1               0.0              0.56           0.11325         0.024792   \n",
      "2               0.0              0.16           0.02075         0.001248   \n",
      "3               0.0              0.01           0.00025         0.000002   \n",
      "4               0.0              0.17           0.03400         0.003245   \n",
      "\n",
      "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
      "0            9.0           32.0         21.200     43.292308   \n",
      "1            7.0           53.0         28.575    208.045513   \n",
      "2            7.0           29.0         18.075     29.558333   \n",
      "3            7.0           22.0         12.875     12.727564   \n",
      "4            4.0           30.0         14.025     48.742949   \n",
      "\n",
      "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('paramId_0', 'amax')  \\\n",
      "0         319.500000         359.299988  ...                   44.0   \n",
      "1         273.799988         373.799988  ...                    0.0   \n",
      "2         273.299988         315.500000  ...                    0.0   \n",
      "3         299.600006         349.399994  ...                    0.0   \n",
      "4         260.299988         327.600006  ...                    0.0   \n",
      "\n",
      "   ('paramId_0', 'mean')  ('paramId_0', 'var')  ('pres', 'amin')  \\\n",
      "0                    3.7             97.958974           30300.0   \n",
      "1                    0.0              0.000000           42970.0   \n",
      "2                    0.0              0.000000           42970.0   \n",
      "3                    0.0              0.000000           42970.0   \n",
      "4                    0.0              0.000000           42970.0   \n",
      "\n",
      "   ('pres', 'amax')  ('pres', 'mean')  ('pres', 'var')  ('pres', 'count')  \\\n",
      "0           38340.0           35110.0       18032700.0                3.0   \n",
      "1           55780.0           50410.0       49512550.0                0.0   \n",
      "2           55780.0           50410.0       49512550.0                0.0   \n",
      "3           55780.0           50410.0       49512550.0                0.0   \n",
      "4           55780.0           50410.0       49512550.0                0.0   \n",
      "\n",
      "   ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
      "0                           0.0                    2.0  \n",
      "1                           1.0                   12.0  \n",
      "2                           0.0                    3.0  \n",
      "3                           1.0                    8.0  \n",
      "4                           0.0                    3.0  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>('cwat', 'amin')</th>\n",
       "      <th>('cwat', 'amax')</th>\n",
       "      <th>('cwat', 'mean')</th>\n",
       "      <th>('cwat', 'var')</th>\n",
       "      <th>('r', 'amin')</th>\n",
       "      <th>('r', 'amax')</th>\n",
       "      <th>('r', 'mean')</th>\n",
       "      <th>('r', 'var')</th>\n",
       "      <th>('tozne', 'amin')</th>\n",
       "      <th>('tozne', 'amax')</th>\n",
       "      <th>...</th>\n",
       "      <th>('paramId_0', 'amax')</th>\n",
       "      <th>('paramId_0', 'mean')</th>\n",
       "      <th>('paramId_0', 'var')</th>\n",
       "      <th>('pres', 'amin')</th>\n",
       "      <th>('pres', 'amax')</th>\n",
       "      <th>('pres', 'mean')</th>\n",
       "      <th>('pres', 'var')</th>\n",
       "      <th>('pres', 'count')</th>\n",
       "      <th>('macro_season', '&lt;lambda&gt;')</th>\n",
       "      <th>('month', '&lt;lambda&gt;')</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.040089</td>\n",
       "      <td>1.924487</td>\n",
       "      <td>2.583341</td>\n",
       "      <td>2.410399</td>\n",
       "      <td>0.056035</td>\n",
       "      <td>-0.168625</td>\n",
       "      <td>0.383726</td>\n",
       "      <td>-0.149346</td>\n",
       "      <td>1.677050</td>\n",
       "      <td>0.442997</td>\n",
       "      <td>...</td>\n",
       "      <td>1.192468</td>\n",
       "      <td>1.102572</td>\n",
       "      <td>0.673545</td>\n",
       "      <td>-1.659315</td>\n",
       "      <td>-1.869787</td>\n",
       "      <td>-1.922512</td>\n",
       "      <td>-0.750060</td>\n",
       "      <td>0.859064</td>\n",
       "      <td>-1.001495</td>\n",
       "      <td>-1.304467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.040089</td>\n",
       "      <td>0.097012</td>\n",
       "      <td>0.867488</td>\n",
       "      <td>0.033757</td>\n",
       "      <td>-0.457948</td>\n",
       "      <td>1.736764</td>\n",
       "      <td>1.558112</td>\n",
       "      <td>3.655181</td>\n",
       "      <td>-0.311430</td>\n",
       "      <td>0.815696</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.762987</td>\n",
       "      <td>-0.505987</td>\n",
       "      <td>-0.486585</td>\n",
       "      <td>-0.068735</td>\n",
       "      <td>0.057320</td>\n",
       "      <td>0.043451</td>\n",
       "      <td>-0.154301</td>\n",
       "      <td>-0.533938</td>\n",
       "      <td>0.998508</td>\n",
       "      <td>1.620874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.040089</td>\n",
       "      <td>-0.664435</td>\n",
       "      <td>-0.527821</td>\n",
       "      <td>-0.468464</td>\n",
       "      <td>-0.457948</td>\n",
       "      <td>-0.440824</td>\n",
       "      <td>-0.113895</td>\n",
       "      <td>-0.466495</td>\n",
       "      <td>-0.333185</td>\n",
       "      <td>-0.682809</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.762987</td>\n",
       "      <td>-0.505987</td>\n",
       "      <td>-0.486585</td>\n",
       "      <td>-0.068735</td>\n",
       "      <td>0.057320</td>\n",
       "      <td>0.043451</td>\n",
       "      <td>-0.154301</td>\n",
       "      <td>-0.533938</td>\n",
       "      <td>-1.001495</td>\n",
       "      <td>-1.011933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.040089</td>\n",
       "      <td>-0.949978</td>\n",
       "      <td>-0.837052</td>\n",
       "      <td>-0.495036</td>\n",
       "      <td>-0.457948</td>\n",
       "      <td>-1.075953</td>\n",
       "      <td>-0.941937</td>\n",
       "      <td>-0.855156</td>\n",
       "      <td>0.811170</td>\n",
       "      <td>0.188534</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.762987</td>\n",
       "      <td>-0.505987</td>\n",
       "      <td>-0.486585</td>\n",
       "      <td>-0.068735</td>\n",
       "      <td>0.057320</td>\n",
       "      <td>0.043451</td>\n",
       "      <td>-0.154301</td>\n",
       "      <td>-0.533938</td>\n",
       "      <td>0.998508</td>\n",
       "      <td>0.450738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.040089</td>\n",
       "      <td>-0.645399</td>\n",
       "      <td>-0.327953</td>\n",
       "      <td>-0.425865</td>\n",
       "      <td>-1.228923</td>\n",
       "      <td>-0.350091</td>\n",
       "      <td>-0.758812</td>\n",
       "      <td>-0.023478</td>\n",
       "      <td>-0.898836</td>\n",
       "      <td>-0.371799</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.762987</td>\n",
       "      <td>-0.505987</td>\n",
       "      <td>-0.486585</td>\n",
       "      <td>-0.068735</td>\n",
       "      <td>0.057320</td>\n",
       "      <td>0.043451</td>\n",
       "      <td>-0.154301</td>\n",
       "      <td>-0.533938</td>\n",
       "      <td>-1.001495</td>\n",
       "      <td>-1.011933</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
       "0         -0.040089          1.924487          2.583341         2.410399   \n",
       "1         -0.040089          0.097012          0.867488         0.033757   \n",
       "2         -0.040089         -0.664435         -0.527821        -0.468464   \n",
       "3         -0.040089         -0.949978         -0.837052        -0.495036   \n",
       "4         -0.040089         -0.645399         -0.327953        -0.425865   \n",
       "\n",
       "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
       "0       0.056035      -0.168625       0.383726     -0.149346   \n",
       "1      -0.457948       1.736764       1.558112      3.655181   \n",
       "2      -0.457948      -0.440824      -0.113895     -0.466495   \n",
       "3      -0.457948      -1.075953      -0.941937     -0.855156   \n",
       "4      -1.228923      -0.350091      -0.758812     -0.023478   \n",
       "\n",
       "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('paramId_0', 'amax')  \\\n",
       "0           1.677050           0.442997  ...               1.192468   \n",
       "1          -0.311430           0.815696  ...              -0.762987   \n",
       "2          -0.333185          -0.682809  ...              -0.762987   \n",
       "3           0.811170           0.188534  ...              -0.762987   \n",
       "4          -0.898836          -0.371799  ...              -0.762987   \n",
       "\n",
       "   ('paramId_0', 'mean')  ('paramId_0', 'var')  ('pres', 'amin')  \\\n",
       "0               1.102572              0.673545         -1.659315   \n",
       "1              -0.505987             -0.486585         -0.068735   \n",
       "2              -0.505987             -0.486585         -0.068735   \n",
       "3              -0.505987             -0.486585         -0.068735   \n",
       "4              -0.505987             -0.486585         -0.068735   \n",
       "\n",
       "   ('pres', 'amax')  ('pres', 'mean')  ('pres', 'var')  ('pres', 'count')  \\\n",
       "0         -1.869787         -1.922512        -0.750060           0.859064   \n",
       "1          0.057320          0.043451        -0.154301          -0.533938   \n",
       "2          0.057320          0.043451        -0.154301          -0.533938   \n",
       "3          0.057320          0.043451        -0.154301          -0.533938   \n",
       "4          0.057320          0.043451        -0.154301          -0.533938   \n",
       "\n",
       "   ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
       "0                     -1.001495              -1.304467  \n",
       "1                      0.998508               1.620874  \n",
       "2                     -1.001495              -1.011933  \n",
       "3                      0.998508               0.450738  \n",
       "4                     -1.001495              -1.011933  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(feature_imp_train_4_1.head())\n",
    "full_features_4_1_train = pd.DataFrame(scaler.fit_transform(feature_imp_train_4_1))\n",
    "full_features_4_1_train.columns = feature_imp_train_4_1.columns\n",
    "full_features_4_1_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>('cwat', 'amin')</th>\n",
       "      <th>('cwat', 'amax')</th>\n",
       "      <th>('cwat', 'mean')</th>\n",
       "      <th>('cwat', 'var')</th>\n",
       "      <th>('r', 'amin')</th>\n",
       "      <th>('r', 'amax')</th>\n",
       "      <th>('r', 'mean')</th>\n",
       "      <th>('r', 'var')</th>\n",
       "      <th>('tozne', 'amin')</th>\n",
       "      <th>('tozne', 'amax')</th>\n",
       "      <th>...</th>\n",
       "      <th>('paramId_0', 'amax')</th>\n",
       "      <th>('paramId_0', 'mean')</th>\n",
       "      <th>('paramId_0', 'var')</th>\n",
       "      <th>('pres', 'amin')</th>\n",
       "      <th>('pres', 'amax')</th>\n",
       "      <th>('pres', 'mean')</th>\n",
       "      <th>('pres', 'var')</th>\n",
       "      <th>('pres', 'count')</th>\n",
       "      <th>('macro_season', '&lt;lambda&gt;')</th>\n",
       "      <th>('month', '&lt;lambda&gt;')</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.040089</td>\n",
       "      <td>-0.702508</td>\n",
       "      <td>-0.403375</td>\n",
       "      <td>-0.443149</td>\n",
       "      <td>-0.971932</td>\n",
       "      <td>-0.803755</td>\n",
       "      <td>-1.220605</td>\n",
       "      <td>-0.335283</td>\n",
       "      <td>-1.486241</td>\n",
       "      <td>-1.348526</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.762987</td>\n",
       "      <td>-0.505987</td>\n",
       "      <td>-0.486585</td>\n",
       "      <td>-0.100119</td>\n",
       "      <td>-0.005665</td>\n",
       "      <td>-0.024651</td>\n",
       "      <td>-0.120931</td>\n",
       "      <td>-0.533938</td>\n",
       "      <td>-1.001495</td>\n",
       "      <td>-1.597001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.040089</td>\n",
       "      <td>1.562799</td>\n",
       "      <td>1.063586</td>\n",
       "      <td>0.443356</td>\n",
       "      <td>-0.200957</td>\n",
       "      <td>1.736764</td>\n",
       "      <td>1.028643</td>\n",
       "      <td>2.886460</td>\n",
       "      <td>-1.634181</td>\n",
       "      <td>-1.302260</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.762987</td>\n",
       "      <td>-0.505987</td>\n",
       "      <td>-0.486585</td>\n",
       "      <td>-0.100119</td>\n",
       "      <td>-0.005665</td>\n",
       "      <td>-0.024651</td>\n",
       "      <td>-0.120931</td>\n",
       "      <td>-0.533938</td>\n",
       "      <td>-1.001495</td>\n",
       "      <td>-1.597001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.040089</td>\n",
       "      <td>-0.074314</td>\n",
       "      <td>-0.116771</td>\n",
       "      <td>-0.243792</td>\n",
       "      <td>-0.457948</td>\n",
       "      <td>0.194306</td>\n",
       "      <td>-0.703079</td>\n",
       "      <td>0.400709</td>\n",
       "      <td>-1.042423</td>\n",
       "      <td>0.746297</td>\n",
       "      <td>...</td>\n",
       "      <td>0.259182</td>\n",
       "      <td>-0.245140</td>\n",
       "      <td>-0.330014</td>\n",
       "      <td>4.198339</td>\n",
       "      <td>2.397694</td>\n",
       "      <td>3.454976</td>\n",
       "      <td>-0.120931</td>\n",
       "      <td>-0.069604</td>\n",
       "      <td>-1.001495</td>\n",
       "      <td>-1.304467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.040089</td>\n",
       "      <td>-0.283712</td>\n",
       "      <td>0.701559</td>\n",
       "      <td>-0.339917</td>\n",
       "      <td>1.597985</td>\n",
       "      <td>0.829436</td>\n",
       "      <td>1.028643</td>\n",
       "      <td>-0.462513</td>\n",
       "      <td>-0.119978</td>\n",
       "      <td>1.435147</td>\n",
       "      <td>...</td>\n",
       "      <td>2.036869</td>\n",
       "      <td>3.906683</td>\n",
       "      <td>4.131917</td>\n",
       "      <td>-0.252022</td>\n",
       "      <td>2.493828</td>\n",
       "      <td>1.276283</td>\n",
       "      <td>3.432651</td>\n",
       "      <td>3.645066</td>\n",
       "      <td>-1.001495</td>\n",
       "      <td>-1.304467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.040089</td>\n",
       "      <td>1.886414</td>\n",
       "      <td>2.613510</td>\n",
       "      <td>3.511829</td>\n",
       "      <td>1.854976</td>\n",
       "      <td>1.736764</td>\n",
       "      <td>1.789008</td>\n",
       "      <td>1.524296</td>\n",
       "      <td>-0.663872</td>\n",
       "      <td>1.003331</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.274123</td>\n",
       "      <td>-0.310352</td>\n",
       "      <td>-0.437421</td>\n",
       "      <td>0.173556</td>\n",
       "      <td>1.575579</td>\n",
       "      <td>0.917213</td>\n",
       "      <td>4.644340</td>\n",
       "      <td>0.394730</td>\n",
       "      <td>-1.001495</td>\n",
       "      <td>-1.304467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
       "0         -0.040089         -0.702508         -0.403375        -0.443149   \n",
       "1         -0.040089          1.562799          1.063586         0.443356   \n",
       "2         -0.040089         -0.074314         -0.116771        -0.243792   \n",
       "3         -0.040089         -0.283712          0.701559        -0.339917   \n",
       "4         -0.040089          1.886414          2.613510         3.511829   \n",
       "\n",
       "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
       "0      -0.971932      -0.803755      -1.220605     -0.335283   \n",
       "1      -0.200957       1.736764       1.028643      2.886460   \n",
       "2      -0.457948       0.194306      -0.703079      0.400709   \n",
       "3       1.597985       0.829436       1.028643     -0.462513   \n",
       "4       1.854976       1.736764       1.789008      1.524296   \n",
       "\n",
       "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('paramId_0', 'amax')  \\\n",
       "0          -1.486241          -1.348526  ...              -0.762987   \n",
       "1          -1.634181          -1.302260  ...              -0.762987   \n",
       "2          -1.042423           0.746297  ...               0.259182   \n",
       "3          -0.119978           1.435147  ...               2.036869   \n",
       "4          -0.663872           1.003331  ...              -0.274123   \n",
       "\n",
       "   ('paramId_0', 'mean')  ('paramId_0', 'var')  ('pres', 'amin')  \\\n",
       "0              -0.505987             -0.486585         -0.100119   \n",
       "1              -0.505987             -0.486585         -0.100119   \n",
       "2              -0.245140             -0.330014          4.198339   \n",
       "3               3.906683              4.131917         -0.252022   \n",
       "4              -0.310352             -0.437421          0.173556   \n",
       "\n",
       "   ('pres', 'amax')  ('pres', 'mean')  ('pres', 'var')  ('pres', 'count')  \\\n",
       "0         -0.005665         -0.024651        -0.120931          -0.533938   \n",
       "1         -0.005665         -0.024651        -0.120931          -0.533938   \n",
       "2          2.397694          3.454976        -0.120931          -0.069604   \n",
       "3          2.493828          1.276283         3.432651           3.645066   \n",
       "4          1.575579          0.917213         4.644340           0.394730   \n",
       "\n",
       "   ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
       "0                     -1.001495              -1.597001  \n",
       "1                     -1.001495              -1.597001  \n",
       "2                     -1.001495              -1.304467  \n",
       "3                     -1.001495              -1.304467  \n",
       "4                     -1.001495              -1.304467  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tenday_full_features_test = pd.DataFrame(scaler.transform(tenday_imp_test))\n",
    "tenday_full_features_test.columns = tenday_imp_test.columns\n",
    "tenday_full_features_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.6606\n",
      "Test set accuracy: 0.6631\n",
      "Training Set Precision: 0.3372\n",
      "Training Set Recall: 0.7218\n",
      "Training Set F1 Score: 0.4597\n",
      "Test Set Precision: 0.2525\n",
      "Test Set Recall: 0.706\n",
      "Test Set F1 Score: 0.372\n"
     ]
    }
   ],
   "source": [
    "clf_log_10day_full = LogisticRegression(class_weight = \"balanced\", max_iter = 100000)\n",
    "clf_log_10day_full.fit(full_features_4_1_train, target_train_4_1)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_log_10day_full.score(full_features_4_1_train, \n",
    "                                                       target_train_4_1), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_log_10day_full.score(tenday_full_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_log = clf_log_10day_full.predict(full_features_4_1_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(target_train_4_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(target_train_4_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(target_train_4_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_log = clf_log_10day_full.predict(tenday_full_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_log), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_log), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_log), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.8014\n",
      "Test set accuracy: 0.8514\n",
      "Training Set Precision: 0.5241\n",
      "Training Set Recall: 0.0752\n",
      "Training Set F1 Score: 0.1315\n",
      "Test Set Precision: 0.3965\n",
      "Test Set Recall: 0.0991\n",
      "Test Set F1 Score: 0.1586\n"
     ]
    }
   ],
   "source": [
    "clf_log_10day_full = LogisticRegression(max_iter = 100000)\n",
    "clf_log_10day_full.fit(full_features_4_1_train, target_train_4_1)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_log_10day_full.score(full_features_4_1_train, \n",
    "                                                       target_train_4_1), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_log_10day_full.score(tenday_full_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_log = clf_log_10day_full.predict(full_features_4_1_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(target_train_4_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(target_train_4_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(target_train_4_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_log = clf_log_10day_full.predict(tenday_full_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_log), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_log), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_log), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.62\n",
      "Test set accuracy: 0.6268\n",
      "Training Set Precision: 0.3157\n",
      "Training Set Recall: 0.7713\n",
      "Training Set F1 Score: 0.4481\n",
      "Test Set Precision: 0.2375\n",
      "Test Set Recall: 0.7419\n",
      "Test Set F1 Score: 0.3598\n"
     ]
    }
   ],
   "source": [
    "clf_sgd_10day_full = SGDClassifier(loss = \"log\", class_weight = \"balanced\", random_state = 0)\n",
    "clf_sgd_10day_full.fit(full_features_4_1_train, target_train_4_1)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_sgd_10day_full.score(full_features_4_1_train, \n",
    "                                                       target_train_4_1), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_sgd_10day_full.score(tenday_full_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_sgd = clf_sgd_10day_full.predict(full_features_4_1_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(target_train_4_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(target_train_4_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(target_train_4_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_sgd = clf_sgd_10day_full.predict(tenday_full_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_sgd), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_sgd), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_sgd), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.7974\n",
      "Test set accuracy: 0.8476\n",
      "Training Set Precision: 0.4565\n",
      "Training Set Recall: 0.0683\n",
      "Training Set F1 Score: 0.1188\n",
      "Test Set Precision: 0.331\n",
      "Test Set Recall: 0.0767\n",
      "Test Set F1 Score: 0.1245\n"
     ]
    }
   ],
   "source": [
    "clf_sgd_10day_full = SGDClassifier(loss = \"log\", random_state = 0)\n",
    "clf_sgd_10day_full.fit(full_features_4_1_train, target_train_4_1)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_sgd_10day_full.score(full_features_4_1_train, \n",
    "                                                       target_train_4_1), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_sgd_10day_full.score(tenday_full_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_sgd = clf_sgd_10day_full.predict(full_features_4_1_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(target_train_4_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(target_train_4_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(target_train_4_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_sgd = clf_sgd_10day_full.predict(tenday_full_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_sgd), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_sgd), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_sgd), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Part Parameter List "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    94804\n",
       "1    23701\n",
       "Name: fire, dtype: int64"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_part_train_4_1, target_train_4_1 = under_sampler_4_1.fit_resample(tenday_no_null_train, tenday_target_train)\n",
    "target_train_4_1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
      "0               0.0              0.11           0.01700         0.000863   \n",
      "1               0.0              0.86           0.07200         0.019970   \n",
      "2               0.0              0.08           0.01550         0.000574   \n",
      "3               0.0              1.54           0.15025         0.087110   \n",
      "4               0.0              0.15           0.03075         0.002187   \n",
      "\n",
      "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
      "0           12.0           40.0         19.425     39.378846   \n",
      "1            8.0           33.0         16.775     49.768590   \n",
      "2            6.0           28.0         13.700     29.138462   \n",
      "3           17.0           51.0         32.700     93.856410   \n",
      "4            9.0           29.0         17.250     26.192308   \n",
      "\n",
      "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('pwat', 'amax')  \\\n",
      "0         306.000000         355.500000  ...         35.000000   \n",
      "1         286.000000         378.600006  ...         13.100000   \n",
      "2         258.399994         283.100006  ...         20.000000   \n",
      "3         293.799988         434.000000  ...         28.200001   \n",
      "4         277.399994         323.500000  ...         27.200001   \n",
      "\n",
      "   ('pwat', 'mean')  ('pwat', 'var')  ('paramId_0', 'amin')  \\\n",
      "0           17.1875        29.627275                    0.0   \n",
      "1            5.3750         7.205513                    0.0   \n",
      "2           10.9025        13.779737                    0.0   \n",
      "3           15.3975        19.121789                    0.0   \n",
      "4           17.2350        14.261820                    0.0   \n",
      "\n",
      "   ('paramId_0', 'amax')  ('paramId_0', 'mean')  ('paramId_0', 'var')  \\\n",
      "0                    0.0                   0.00              0.000000   \n",
      "1                   33.0                   0.90             27.323077   \n",
      "2                    0.0                   0.00              0.000000   \n",
      "3                   54.0                   5.85            197.925641   \n",
      "4                    0.0                   0.00              0.000000   \n",
      "\n",
      "   ('pres', 'count')  ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
      "0                  0                             0                      6  \n",
      "1                  1                             0                      2  \n",
      "2                  0                             1                     11  \n",
      "3                  5                             0                      2  \n",
      "4                  0                             1                      8  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>('cwat', 'amin')</th>\n",
       "      <th>('cwat', 'amax')</th>\n",
       "      <th>('cwat', 'mean')</th>\n",
       "      <th>('cwat', 'var')</th>\n",
       "      <th>('r', 'amin')</th>\n",
       "      <th>('r', 'amax')</th>\n",
       "      <th>('r', 'mean')</th>\n",
       "      <th>('r', 'var')</th>\n",
       "      <th>('tozne', 'amin')</th>\n",
       "      <th>('tozne', 'amax')</th>\n",
       "      <th>...</th>\n",
       "      <th>('pwat', 'amax')</th>\n",
       "      <th>('pwat', 'mean')</th>\n",
       "      <th>('pwat', 'var')</th>\n",
       "      <th>('paramId_0', 'amin')</th>\n",
       "      <th>('paramId_0', 'amax')</th>\n",
       "      <th>('paramId_0', 'mean')</th>\n",
       "      <th>('paramId_0', 'var')</th>\n",
       "      <th>('pres', 'count')</th>\n",
       "      <th>('macro_season', '&lt;lambda&gt;')</th>\n",
       "      <th>('month', '&lt;lambda&gt;')</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.04132</td>\n",
       "      <td>-0.760368</td>\n",
       "      <td>-0.584011</td>\n",
       "      <td>-0.476601</td>\n",
       "      <td>0.832052</td>\n",
       "      <td>0.558054</td>\n",
       "      <td>0.102547</td>\n",
       "      <td>-0.239541</td>\n",
       "      <td>1.091792</td>\n",
       "      <td>0.343228</td>\n",
       "      <td>...</td>\n",
       "      <td>1.578408</td>\n",
       "      <td>0.908579</td>\n",
       "      <td>0.610296</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.764217</td>\n",
       "      <td>-0.505808</td>\n",
       "      <td>-0.486464</td>\n",
       "      <td>-0.534247</td>\n",
       "      <td>-1.00016</td>\n",
       "      <td>-0.132531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.04132</td>\n",
       "      <td>0.667146</td>\n",
       "      <td>0.243302</td>\n",
       "      <td>-0.069270</td>\n",
       "      <td>-0.198168</td>\n",
       "      <td>-0.077091</td>\n",
       "      <td>-0.319200</td>\n",
       "      <td>0.000580</td>\n",
       "      <td>0.219728</td>\n",
       "      <td>0.935750</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.991185</td>\n",
       "      <td>-1.317159</td>\n",
       "      <td>-0.647389</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.702473</td>\n",
       "      <td>-0.116216</td>\n",
       "      <td>-0.164063</td>\n",
       "      <td>-0.071820</td>\n",
       "      <td>-1.00016</td>\n",
       "      <td>-1.302343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.04132</td>\n",
       "      <td>-0.817469</td>\n",
       "      <td>-0.606574</td>\n",
       "      <td>-0.482750</td>\n",
       "      <td>-0.713278</td>\n",
       "      <td>-0.530766</td>\n",
       "      <td>-0.808587</td>\n",
       "      <td>-0.476211</td>\n",
       "      <td>-0.983720</td>\n",
       "      <td>-1.513854</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.181587</td>\n",
       "      <td>-0.275655</td>\n",
       "      <td>-0.278626</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.764217</td>\n",
       "      <td>-0.505808</td>\n",
       "      <td>-0.486464</td>\n",
       "      <td>-0.534247</td>\n",
       "      <td>0.99984</td>\n",
       "      <td>1.329733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.04132</td>\n",
       "      <td>1.961426</td>\n",
       "      <td>1.420343</td>\n",
       "      <td>1.361996</td>\n",
       "      <td>2.119827</td>\n",
       "      <td>1.556139</td>\n",
       "      <td>2.215265</td>\n",
       "      <td>1.019510</td>\n",
       "      <td>0.559832</td>\n",
       "      <td>2.356777</td>\n",
       "      <td>...</td>\n",
       "      <td>0.780544</td>\n",
       "      <td>0.571303</td>\n",
       "      <td>0.021021</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.635821</td>\n",
       "      <td>2.026539</td>\n",
       "      <td>1.848980</td>\n",
       "      <td>1.777890</td>\n",
       "      <td>-1.00016</td>\n",
       "      <td>-1.302343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.04132</td>\n",
       "      <td>-0.684234</td>\n",
       "      <td>-0.377183</td>\n",
       "      <td>-0.448375</td>\n",
       "      <td>0.059387</td>\n",
       "      <td>-0.440031</td>\n",
       "      <td>-0.243604</td>\n",
       "      <td>-0.544301</td>\n",
       "      <td>-0.155260</td>\n",
       "      <td>-0.477582</td>\n",
       "      <td>...</td>\n",
       "      <td>0.663211</td>\n",
       "      <td>0.917529</td>\n",
       "      <td>-0.251585</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.764217</td>\n",
       "      <td>-0.505808</td>\n",
       "      <td>-0.486464</td>\n",
       "      <td>-0.534247</td>\n",
       "      <td>0.99984</td>\n",
       "      <td>0.452375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
       "0          -0.04132         -0.760368         -0.584011        -0.476601   \n",
       "1          -0.04132          0.667146          0.243302        -0.069270   \n",
       "2          -0.04132         -0.817469         -0.606574        -0.482750   \n",
       "3          -0.04132          1.961426          1.420343         1.361996   \n",
       "4          -0.04132         -0.684234         -0.377183        -0.448375   \n",
       "\n",
       "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
       "0       0.832052       0.558054       0.102547     -0.239541   \n",
       "1      -0.198168      -0.077091      -0.319200      0.000580   \n",
       "2      -0.713278      -0.530766      -0.808587     -0.476211   \n",
       "3       2.119827       1.556139       2.215265      1.019510   \n",
       "4       0.059387      -0.440031      -0.243604     -0.544301   \n",
       "\n",
       "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('pwat', 'amax')  \\\n",
       "0           1.091792           0.343228  ...          1.578408   \n",
       "1           0.219728           0.935750  ...         -0.991185   \n",
       "2          -0.983720          -1.513854  ...         -0.181587   \n",
       "3           0.559832           2.356777  ...          0.780544   \n",
       "4          -0.155260          -0.477582  ...          0.663211   \n",
       "\n",
       "   ('pwat', 'mean')  ('pwat', 'var')  ('paramId_0', 'amin')  \\\n",
       "0          0.908579         0.610296                    0.0   \n",
       "1         -1.317159        -0.647389                    0.0   \n",
       "2         -0.275655        -0.278626                    0.0   \n",
       "3          0.571303         0.021021                    0.0   \n",
       "4          0.917529        -0.251585                    0.0   \n",
       "\n",
       "   ('paramId_0', 'amax')  ('paramId_0', 'mean')  ('paramId_0', 'var')  \\\n",
       "0              -0.764217              -0.505808             -0.486464   \n",
       "1               0.702473              -0.116216             -0.164063   \n",
       "2              -0.764217              -0.505808             -0.486464   \n",
       "3               1.635821               2.026539              1.848980   \n",
       "4              -0.764217              -0.505808             -0.486464   \n",
       "\n",
       "   ('pres', 'count')  ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
       "0          -0.534247                      -1.00016              -0.132531  \n",
       "1          -0.071820                      -1.00016              -1.302343  \n",
       "2          -0.534247                       0.99984               1.329733  \n",
       "3           1.777890                      -1.00016              -1.302343  \n",
       "4          -0.534247                       0.99984               0.452375  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(feature_part_train_4_1.head())\n",
    "part_features_4_1_train = pd.DataFrame(scaler.fit_transform(feature_part_train_4_1))\n",
    "part_features_4_1_train.columns = feature_part_train_4_1.columns\n",
    "part_features_4_1_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
      "292               0.0              0.14           0.02900         0.002435   \n",
      "293               0.0              1.33           0.12625         0.043993   \n",
      "294               0.0              0.47           0.04800         0.011781   \n",
      "295               0.0              0.36           0.10225         0.007274   \n",
      "296               0.0              1.50           0.22900         0.187840   \n",
      "\n",
      "     ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
      "292            5.0           25.0         11.125     35.240385   \n",
      "293            8.0           53.0         25.250    174.756410   \n",
      "294            7.0           36.0         14.375     67.112179   \n",
      "295           15.0           43.0         25.250     29.730769   \n",
      "296           16.0           53.0         30.025    115.768590   \n",
      "\n",
      "     ('tozne', 'amin')  ('tozne', 'amax')  ...  ('pwat', 'amax')  \\\n",
      "292         246.800003         289.600006  ...         17.900000   \n",
      "293         243.399994         291.399994  ...         31.100000   \n",
      "294         257.000000         371.100006  ...         24.700001   \n",
      "295         278.200012         397.899994  ...         22.799999   \n",
      "296         265.700012         381.100006  ...         35.400002   \n",
      "\n",
      "     ('pwat', 'mean')  ('pwat', 'var')  ('paramId_0', 'amin')  \\\n",
      "292            8.0600        13.980923                    0.0   \n",
      "293           15.2250        54.741923                    0.0   \n",
      "294           10.2225        26.132044                    0.0   \n",
      "295           14.5500         7.674359                    0.0   \n",
      "296           18.6275        57.461017                    0.0   \n",
      "\n",
      "     ('paramId_0', 'amax')  ('paramId_0', 'mean')  ('paramId_0', 'var')  \\\n",
      "292                    0.0                   0.00              0.000000   \n",
      "293                    0.0                   0.00              0.000000   \n",
      "294                   23.0                   0.60             13.220513   \n",
      "295                   63.0                  10.15            389.976923   \n",
      "296                   11.0                   0.45              4.151282   \n",
      "\n",
      "     ('pres', 'count')  ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
      "292                  0                             0                      1  \n",
      "293                  0                             0                      1  \n",
      "294                  1                             0                      2  \n",
      "295                  9                             0                      2  \n",
      "296                  2                             0                      2  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>('cwat', 'amin')</th>\n",
       "      <th>('cwat', 'amax')</th>\n",
       "      <th>('cwat', 'mean')</th>\n",
       "      <th>('cwat', 'var')</th>\n",
       "      <th>('r', 'amin')</th>\n",
       "      <th>('r', 'amax')</th>\n",
       "      <th>('r', 'mean')</th>\n",
       "      <th>('r', 'var')</th>\n",
       "      <th>('tozne', 'amin')</th>\n",
       "      <th>('tozne', 'amax')</th>\n",
       "      <th>...</th>\n",
       "      <th>('pwat', 'amax')</th>\n",
       "      <th>('pwat', 'mean')</th>\n",
       "      <th>('pwat', 'var')</th>\n",
       "      <th>('paramId_0', 'amin')</th>\n",
       "      <th>('paramId_0', 'amax')</th>\n",
       "      <th>('paramId_0', 'mean')</th>\n",
       "      <th>('paramId_0', 'var')</th>\n",
       "      <th>('pres', 'count')</th>\n",
       "      <th>('macro_season', '&lt;lambda&gt;')</th>\n",
       "      <th>('month', '&lt;lambda&gt;')</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.04132</td>\n",
       "      <td>-0.703268</td>\n",
       "      <td>-0.403506</td>\n",
       "      <td>-0.443083</td>\n",
       "      <td>-0.970833</td>\n",
       "      <td>-0.802971</td>\n",
       "      <td>-1.218398</td>\n",
       "      <td>-0.335187</td>\n",
       "      <td>-1.489517</td>\n",
       "      <td>-1.347127</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.427987</td>\n",
       "      <td>-0.811245</td>\n",
       "      <td>-0.267342</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.764217</td>\n",
       "      <td>-0.505808</td>\n",
       "      <td>-0.486464</td>\n",
       "      <td>-0.534247</td>\n",
       "      <td>-1.00016</td>\n",
       "      <td>-1.594796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.04132</td>\n",
       "      <td>1.561722</td>\n",
       "      <td>1.059334</td>\n",
       "      <td>0.442844</td>\n",
       "      <td>-0.198168</td>\n",
       "      <td>1.737609</td>\n",
       "      <td>1.029597</td>\n",
       "      <td>2.889221</td>\n",
       "      <td>-1.637768</td>\n",
       "      <td>-1.300956</td>\n",
       "      <td>...</td>\n",
       "      <td>1.120809</td>\n",
       "      <td>0.538801</td>\n",
       "      <td>2.019032</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.764217</td>\n",
       "      <td>-0.505808</td>\n",
       "      <td>-0.486464</td>\n",
       "      <td>-0.534247</td>\n",
       "      <td>-1.00016</td>\n",
       "      <td>-1.594796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.04132</td>\n",
       "      <td>-0.075161</td>\n",
       "      <td>-0.117707</td>\n",
       "      <td>-0.243856</td>\n",
       "      <td>-0.455723</td>\n",
       "      <td>0.195114</td>\n",
       "      <td>-0.701161</td>\n",
       "      <td>0.401414</td>\n",
       "      <td>-1.044765</td>\n",
       "      <td>0.743373</td>\n",
       "      <td>...</td>\n",
       "      <td>0.369878</td>\n",
       "      <td>-0.403782</td>\n",
       "      <td>0.414241</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.258021</td>\n",
       "      <td>-0.246080</td>\n",
       "      <td>-0.330467</td>\n",
       "      <td>-0.071820</td>\n",
       "      <td>-1.00016</td>\n",
       "      <td>-1.302343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.04132</td>\n",
       "      <td>-0.284530</td>\n",
       "      <td>0.698324</td>\n",
       "      <td>-0.339918</td>\n",
       "      <td>1.604717</td>\n",
       "      <td>0.830259</td>\n",
       "      <td>1.029597</td>\n",
       "      <td>-0.462522</td>\n",
       "      <td>-0.120376</td>\n",
       "      <td>1.430801</td>\n",
       "      <td>...</td>\n",
       "      <td>0.146945</td>\n",
       "      <td>0.411616</td>\n",
       "      <td>-0.621090</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.035827</td>\n",
       "      <td>3.887921</td>\n",
       "      <td>4.115110</td>\n",
       "      <td>3.627599</td>\n",
       "      <td>-1.00016</td>\n",
       "      <td>-1.302343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.04132</td>\n",
       "      <td>1.885292</td>\n",
       "      <td>2.604905</td>\n",
       "      <td>3.509318</td>\n",
       "      <td>1.862272</td>\n",
       "      <td>1.737609</td>\n",
       "      <td>1.789538</td>\n",
       "      <td>1.525931</td>\n",
       "      <td>-0.665416</td>\n",
       "      <td>0.999876</td>\n",
       "      <td>...</td>\n",
       "      <td>1.625342</td>\n",
       "      <td>1.179907</td>\n",
       "      <td>2.171552</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.275321</td>\n",
       "      <td>-0.311012</td>\n",
       "      <td>-0.437481</td>\n",
       "      <td>0.390608</td>\n",
       "      <td>-1.00016</td>\n",
       "      <td>-1.302343</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
       "0          -0.04132         -0.703268         -0.403506        -0.443083   \n",
       "1          -0.04132          1.561722          1.059334         0.442844   \n",
       "2          -0.04132         -0.075161         -0.117707        -0.243856   \n",
       "3          -0.04132         -0.284530          0.698324        -0.339918   \n",
       "4          -0.04132          1.885292          2.604905         3.509318   \n",
       "\n",
       "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
       "0      -0.970833      -0.802971      -1.218398     -0.335187   \n",
       "1      -0.198168       1.737609       1.029597      2.889221   \n",
       "2      -0.455723       0.195114      -0.701161      0.401414   \n",
       "3       1.604717       0.830259       1.029597     -0.462522   \n",
       "4       1.862272       1.737609       1.789538      1.525931   \n",
       "\n",
       "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('pwat', 'amax')  \\\n",
       "0          -1.489517          -1.347127  ...         -0.427987   \n",
       "1          -1.637768          -1.300956  ...          1.120809   \n",
       "2          -1.044765           0.743373  ...          0.369878   \n",
       "3          -0.120376           1.430801  ...          0.146945   \n",
       "4          -0.665416           0.999876  ...          1.625342   \n",
       "\n",
       "   ('pwat', 'mean')  ('pwat', 'var')  ('paramId_0', 'amin')  \\\n",
       "0         -0.811245        -0.267342                    0.0   \n",
       "1          0.538801         2.019032                    0.0   \n",
       "2         -0.403782         0.414241                    0.0   \n",
       "3          0.411616        -0.621090                    0.0   \n",
       "4          1.179907         2.171552                    0.0   \n",
       "\n",
       "   ('paramId_0', 'amax')  ('paramId_0', 'mean')  ('paramId_0', 'var')  \\\n",
       "0              -0.764217              -0.505808             -0.486464   \n",
       "1              -0.764217              -0.505808             -0.486464   \n",
       "2               0.258021              -0.246080             -0.330467   \n",
       "3               2.035827               3.887921              4.115110   \n",
       "4              -0.275321              -0.311012             -0.437481   \n",
       "\n",
       "   ('pres', 'count')  ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
       "0          -0.534247                      -1.00016              -1.594796  \n",
       "1          -0.534247                      -1.00016              -1.594796  \n",
       "2          -0.071820                      -1.00016              -1.302343  \n",
       "3           3.627599                      -1.00016              -1.302343  \n",
       "4           0.390608                      -1.00016              -1.302343  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tenday_no_null_test.head())\n",
    "tenday_part_features_test = pd.DataFrame(scaler.transform(tenday_no_null_test))\n",
    "tenday_part_features_test.columns = tenday_no_null_test.columns\n",
    "tenday_part_features_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.6614\n",
      "Test set accuracy: 0.6647\n",
      "Training Set Precision: 0.3376\n",
      "Training Set Recall: 0.7201\n",
      "Training Set F1 Score: 0.4597\n",
      "Test Set Precision: 0.2523\n",
      "Test Set Recall: 0.6992\n",
      "Test Set F1 Score: 0.3708\n"
     ]
    }
   ],
   "source": [
    "clf_log_10day_part = LogisticRegression(class_weight = \"balanced\")\n",
    "clf_log_10day_part.fit(part_features_4_1_train, target_train_4_1)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_log_10day_part.score(part_features_4_1_train, \n",
    "                                                       target_train_4_1), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_log_10day_part.score(tenday_part_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_log = clf_log_10day_part.predict(part_features_4_1_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(target_train_4_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(target_train_4_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(target_train_4_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_log = clf_log_10day_part.predict(tenday_part_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_log), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_log), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_log), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.8012\n",
      "Test set accuracy: 0.852\n",
      "Training Set Precision: 0.5209\n",
      "Training Set Recall: 0.075\n",
      "Training Set F1 Score: 0.1311\n",
      "Test Set Precision: 0.4022\n",
      "Test Set Recall: 0.0973\n",
      "Test Set F1 Score: 0.1567\n"
     ]
    }
   ],
   "source": [
    "clf_log_10day_part = LogisticRegression()\n",
    "clf_log_10day_part.fit(part_features_4_1_train, target_train_4_1)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_log_10day_part.score(part_features_4_1_train, \n",
    "                                                       target_train_4_1), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_log_10day_part.score(tenday_part_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_log = clf_log_10day_part.predict(part_features_4_1_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(target_train_4_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(target_train_4_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(target_train_4_1, \n",
    "                                                    target_pred_train_log), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_log = clf_log_10day_part.predict(tenday_part_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_log), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_log), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_log), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.647\n",
      "Test set accuracy: 0.6576\n",
      "Training Set Precision: 0.3284\n",
      "Training Set Recall: 0.7325\n",
      "Training Set F1 Score: 0.4535\n",
      "Test Set Precision: 0.2482\n",
      "Test Set Recall: 0.7016\n",
      "Test Set F1 Score: 0.3667\n"
     ]
    }
   ],
   "source": [
    "clf_sgd_10day_part = SGDClassifier(loss = \"log\", class_weight = \"balanced\", random_state = 0)\n",
    "clf_sgd_10day_part.fit(part_features_4_1_train, target_train_4_1)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_sgd_10day_part.score(part_features_4_1_train, \n",
    "                                                       target_train_4_1), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_sgd_10day_part.score(tenday_part_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_sgd = clf_sgd_10day_part.predict(part_features_4_1_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(target_train_4_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(target_train_4_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(target_train_4_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_sgd = clf_sgd_10day_part.predict(tenday_part_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_sgd), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_sgd), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_sgd), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.8008\n",
      "Test set accuracy: 0.8528\n",
      "Training Set Precision: 0.5151\n",
      "Training Set Recall: 0.0662\n",
      "Training Set F1 Score: 0.1174\n",
      "Test Set Precision: 0.4044\n",
      "Test Set Recall: 0.0883\n",
      "Test Set F1 Score: 0.145\n"
     ]
    }
   ],
   "source": [
    "clf_sgd_10day_part = SGDClassifier(loss = \"log\", random_state = 0)\n",
    "clf_sgd_10day_part.fit(part_features_4_1_train, target_train_4_1)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_sgd_10day_part.score(part_features_4_1_train, \n",
    "                                                       target_train_4_1), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_sgd_10day_part.score(tenday_part_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_sgd = clf_sgd_10day_part.predict(part_features_4_1_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(target_train_4_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(target_train_4_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(target_train_4_1, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_sgd = clf_sgd_10day_part.predict(tenday_part_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_sgd), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_sgd), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_sgd), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying Other Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 1.0\n",
      "Test set accuracy: 0.8257\n",
      "Training Set Precision: 1.0\n",
      "Training Set Recall: 1.0\n",
      "Training Set F1 Score: 1.0\n",
      "Test Set Precision: 0.3272\n",
      "Test Set Recall: 0.2205\n",
      "Test Set F1 Score: 0.2635\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf_knn_full = KNeighborsClassifier(n_neighbors = 5, weights = \"distance\")\n",
    "clf_knn_full.fit(tenday_full_features_train, tenday_target_train)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_knn_full.score(tenday_full_features_train, \n",
    "                                                       tenday_target_train), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_knn_full.score(tenday_full_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_knn = clf_knn_full.predict(tenday_full_features_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(tenday_target_train, \n",
    "                                                    target_pred_train_knn), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(tenday_target_train, \n",
    "                                                    target_pred_train_knn), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(tenday_target_train, \n",
    "                                                    target_pred_train_knn), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_knn = clf_knn_full.predict(tenday_full_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_knn), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_knn), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_knn), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "375"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the rule of thumb where k = sqrt(n) \n",
    "# n is number of points in training set \n",
    "k = tenday_full_features_train.shape[0] ** 0.5\n",
    "\n",
    "# Now make sure that k is an integer and odd \n",
    "k = int(k//1)\n",
    "if k % 2 == 0:\n",
    "    k += 1\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 1.0\n",
      "Test set accuracy: 0.861\n",
      "Training Set Precision: 1.0\n",
      "Training Set Recall: 1.0\n",
      "Training Set F1 Score: 1.0\n",
      "Test Set Precision: 0.6035\n",
      "Test Set Recall: 0.0486\n",
      "Test Set F1 Score: 0.0899\n"
     ]
    }
   ],
   "source": [
    "clf_knn_full = KNeighborsClassifier(n_neighbors = k, weights = \"distance\")\n",
    "clf_knn_full.fit(tenday_full_features_train, tenday_target_train)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_knn_full.score(tenday_full_features_train, \n",
    "                                                       tenday_target_train), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_knn_full.score(tenday_full_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_knn = clf_knn_full.predict(tenday_full_features_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(tenday_target_train, \n",
    "                                                    target_pred_train_knn), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(tenday_target_train, \n",
    "                                                    target_pred_train_knn), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(tenday_target_train, \n",
    "                                                    target_pred_train_knn), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_knn = clf_knn_full.predict(tenday_full_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_knn), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_knn), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_knn), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 1.0\n",
      "Test set accuracy: 0.8291\n",
      "Training Set Precision: 1.0\n",
      "Training Set Recall: 1.0\n",
      "Training Set F1 Score: 1.0\n",
      "Test Set Precision: 0.3402\n",
      "Test Set Recall: 0.2226\n",
      "Test Set F1 Score: 0.2691\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf_knn_full = KNeighborsClassifier(n_neighbors = 5, weights = \"distance\", p=1, n_jobs = -1)\n",
    "clf_knn_full.fit(tenday_full_features_train, tenday_target_train)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_knn_full.score(tenday_full_features_train, \n",
    "                                                       tenday_target_train), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_knn_full.score(tenday_full_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_knn = clf_knn_full.predict(tenday_full_features_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(tenday_target_train, \n",
    "                                                    target_pred_train_knn), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(tenday_target_train, \n",
    "                                                    target_pred_train_knn), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(tenday_target_train, \n",
    "                                                    target_pred_train_knn), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_knn = clf_knn_full.predict(tenday_full_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_knn), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_knn), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_knn), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.9056\n",
      "Test set accuracy: 0.8291\n",
      "Training Set Precision: 0.7949\n",
      "Training Set Recall: 0.591\n",
      "Training Set F1 Score: 0.6779\n",
      "Test Set Precision: 0.3402\n",
      "Test Set Recall: 0.2226\n",
      "Test Set F1 Score: 0.2691\n"
     ]
    }
   ],
   "source": [
    "clf_knn_full = KNeighborsClassifier(n_neighbors = 5, p=1, n_jobs = -1)\n",
    "clf_knn_full.fit(tenday_full_features_train, tenday_target_train)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_knn_full.score(tenday_full_features_train, \n",
    "                                                       tenday_target_train), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_knn_full.score(tenday_full_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_knn = clf_knn_full.predict(tenday_full_features_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(tenday_target_train, \n",
    "                                                    target_pred_train_knn), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(tenday_target_train, \n",
    "                                                    target_pred_train_knn), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(tenday_target_train, \n",
    "                                                    target_pred_train_knn), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_knn = clf_knn_full.predict(tenday_full_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_knn), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_knn), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_knn), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 1.0\n",
      "Test set accuracy: 0.8621\n",
      "Training Set Precision: 1.0\n",
      "Training Set Recall: 1.0\n",
      "Training Set F1 Score: 1.0\n",
      "Test Set Precision: 0.6118\n",
      "Test Set Recall: 0.0664\n",
      "Test Set F1 Score: 0.1198\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf_knn_full = KNeighborsClassifier(n_neighbors = k, weights = \"distance\", p=1, n_jobs = -1)\n",
    "clf_knn_full.fit(tenday_full_features_train, tenday_target_train)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_knn_full.score(tenday_full_features_train, \n",
    "                                                       tenday_target_train), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_knn_full.score(tenday_full_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_knn = clf_knn_full.predict(tenday_full_features_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(tenday_target_train, \n",
    "                                                    target_pred_train_knn), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(tenday_target_train, \n",
    "                                                    target_pred_train_knn), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(tenday_target_train, \n",
    "                                                    target_pred_train_knn), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_knn = clf_knn_full.predict(tenday_full_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_knn), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_knn), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_knn), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 1.0\n",
      "Test set accuracy: 0.8593\n",
      "Training Set Precision: 1.0\n",
      "Training Set Recall: 1.0\n",
      "Training Set F1 Score: 1.0\n",
      "Test Set Precision: 0.5261\n",
      "Test Set Recall: 0.0446\n",
      "Test Set F1 Score: 0.0821\n"
     ]
    }
   ],
   "source": [
    "clf_knn_full = KNeighborsClassifier(n_neighbors = k, weights = \"distance\", metric=\"cosine\", n_jobs = -1)\n",
    "clf_knn_full.fit(tenday_full_features_train, tenday_target_train)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_knn_full.score(tenday_full_features_train, \n",
    "                                                       tenday_target_train), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_knn_full.score(tenday_full_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_knn = clf_knn_full.predict(tenday_full_features_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(tenday_target_train, \n",
    "                                                    target_pred_train_knn), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(tenday_target_train, \n",
    "                                                    target_pred_train_knn), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(tenday_target_train, \n",
    "                                                    target_pred_train_knn), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_knn = clf_knn_full.predict(tenday_full_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_knn), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_knn), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_knn), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 1.0\n",
      "Test set accuracy: 0.8181\n",
      "Training Set Precision: 1.0\n",
      "Training Set Recall: 1.0\n",
      "Training Set F1 Score: 1.0\n",
      "Test Set Precision: 0.3138\n",
      "Test Set Recall: 0.242\n",
      "Test Set F1 Score: 0.2733\n"
     ]
    }
   ],
   "source": [
    "clf_knn_full = KNeighborsClassifier(n_neighbors = 5, weights = \"distance\", metric=\"cosine\", n_jobs = -1)\n",
    "clf_knn_full.fit(tenday_full_features_train, tenday_target_train)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_knn_full.score(tenday_full_features_train, \n",
    "                                                       tenday_target_train), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_knn_full.score(tenday_full_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_knn = clf_knn_full.predict(tenday_full_features_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(tenday_target_train, \n",
    "                                                    target_pred_train_knn), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(tenday_target_train, \n",
    "                                                    target_pred_train_knn), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(tenday_target_train, \n",
    "                                                    target_pred_train_knn), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_knn = clf_knn_full.predict(tenday_full_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_knn), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_knn), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_knn), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.8355\n",
      "Test set accuracy: 0.8592\n",
      "Training Set Precision: 0.6574\n",
      "Training Set Recall: 0.0447\n",
      "Training Set F1 Score: 0.0837\n",
      "Test Set Precision: 0.5256\n",
      "Test Set Recall: 0.0411\n",
      "Test Set F1 Score: 0.0763\n"
     ]
    }
   ],
   "source": [
    "clf_knn_full = KNeighborsClassifier(n_neighbors = k, metric=\"cosine\", n_jobs = -1)\n",
    "clf_knn_full.fit(tenday_full_features_train, tenday_target_train)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_knn_full.score(tenday_full_features_train, \n",
    "                                                       tenday_target_train), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_knn_full.score(tenday_full_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_knn = clf_knn_full.predict(tenday_full_features_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(tenday_target_train, \n",
    "                                                    target_pred_train_knn), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(tenday_target_train, \n",
    "                                                    target_pred_train_knn), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(tenday_target_train, \n",
    "                                                    target_pred_train_knn), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_knn = clf_knn_full.predict(tenday_full_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_knn), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_knn), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_knn), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.9022\n",
      "Test set accuracy: 0.8183\n",
      "Training Set Precision: 0.7886\n",
      "Training Set Recall: 0.5713\n",
      "Training Set F1 Score: 0.6626\n",
      "Test Set Precision: 0.3131\n",
      "Test Set Recall: 0.239\n",
      "Test Set F1 Score: 0.2711\n"
     ]
    }
   ],
   "source": [
    "clf_knn_full = KNeighborsClassifier(n_neighbors = 5, metric=\"cosine\", n_jobs = -1)\n",
    "clf_knn_full.fit(tenday_full_features_train, tenday_target_train)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_knn_full.score(tenday_full_features_train, \n",
    "                                                       tenday_target_train), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_knn_full.score(tenday_full_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_knn = clf_knn_full.predict(tenday_full_features_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(tenday_target_train, \n",
    "                                                    target_pred_train_knn), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(tenday_target_train, \n",
    "                                                    target_pred_train_knn), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(tenday_target_train, \n",
    "                                                    target_pred_train_knn), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_knn = clf_knn_full.predict(tenday_full_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_knn), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_knn), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_knn), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.6786\n",
      "Test set accuracy: 0.7367\n",
      "Training Set Precision: 0.3103\n",
      "Training Set Recall: 0.7461\n",
      "Training Set F1 Score: 0.4383\n",
      "Test Set Precision: 0.2894\n",
      "Test Set Recall: 0.593\n",
      "Test Set F1 Score: 0.3889\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf_log = LogisticRegression(class_weight = 'balanced', max_iter = 100000)\n",
    "clf_sgd = SGDClassifier(class_weight = 'balanced')\n",
    "clf_dt = DecisionTreeClassifier()\n",
    "\n",
    "ensemble = VotingClassifier(estimators = [(\"log\", clf_log), (\"sgd\", clf_sgd), ('dt', clf_dt)])\n",
    "ensemble.fit(tenday_full_features_train, tenday_target_train)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(ensemble.score(tenday_full_features_train, \n",
    "                                                       tenday_target_train), 4))\n",
    "print(\"Test set accuracy:\", np.round(ensemble.score(tenday_full_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_ensemble = ensemble.predict(tenday_full_features_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(tenday_target_train, \n",
    "                                                    target_pred_train_ensemble), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(tenday_target_train, \n",
    "                                                    target_pred_train_ensemble), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(tenday_target_train, \n",
    "                                                    target_pred_train_ensemble), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_ensemble = ensemble.predict(tenday_full_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_ensemble), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_ensemble), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_ensemble), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.9359\n",
      "Test set accuracy: 0.7679\n",
      "Training Set Precision: 0.7429\n",
      "Training Set Recall: 0.9457\n",
      "Training Set F1 Score: 0.8321\n",
      "Test Set Precision: 0.2796\n",
      "Test Set Recall: 0.4076\n",
      "Test Set F1 Score: 0.3317\n"
     ]
    }
   ],
   "source": [
    "clf_sgd1 = SGDClassifier(loss = \"log\", class_weight = 'balanced')\n",
    "ensemble1 = VotingClassifier(estimators = [(\"log\", clf_log), (\"sgd\", clf_sgd1), ('dt', clf_dt)], voting = \"soft\")\n",
    "ensemble1.fit(tenday_full_features_train, tenday_target_train)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(ensemble1.score(tenday_full_features_train, \n",
    "                                                       tenday_target_train), 4))\n",
    "print(\"Test set accuracy:\", np.round(ensemble1.score(tenday_full_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_ensemble = ensemble1.predict(tenday_full_features_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(tenday_target_train, \n",
    "                                                    target_pred_train_ensemble), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(tenday_target_train, \n",
    "                                                    target_pred_train_ensemble), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(tenday_target_train, \n",
    "                                                    target_pred_train_ensemble), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_ensemble = ensemble1.predict(tenday_full_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_ensemble), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_ensemble), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_ensemble), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.8319\n",
      "Test set accuracy: 0.8582\n",
      "Training Set Precision: 0.4977\n",
      "Training Set Recall: 0.056\n",
      "Training Set F1 Score: 0.1007\n",
      "Test Set Precision: 0.4365\n",
      "Test Set Recall: 0.011\n",
      "Test Set F1 Score: 0.0215\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada = AdaBoostClassifier(n_estimators = 100, learning_rate = 0.5 )\n",
    "ada.fit(tenday_full_features_train, tenday_target_train)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(ada.score(tenday_full_features_train, \n",
    "                                                       tenday_target_train), 4))\n",
    "print(\"Test set accuracy:\", np.round(ada.score(tenday_full_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_ada = ada.predict(tenday_full_features_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(tenday_target_train, \n",
    "                                                    target_pred_train_ada), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(tenday_target_train, \n",
    "                                                    target_pred_train_ada), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(tenday_target_train, \n",
    "                                                    target_pred_train_ada), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_ada = ada.predict(tenday_full_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_ada), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_ada), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_ada), 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.8386\n",
      "Test set accuracy: 0.8584\n",
      "Training Set Precision: 0.9745\n",
      "Training Set Recall: 0.0403\n",
      "Training Set F1 Score: 0.0775\n",
      "Test Set Precision: 0.4636\n",
      "Test Set Recall: 0.0102\n",
      "Test Set F1 Score: 0.02\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "ensemble2 = StackingClassifier(estimators = [(\"log\", clf_log), (\"sgd\", clf_sgd1), ('dt', clf_dt)])\n",
    "ensemble2.fit(tenday_full_features_train, tenday_target_train)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(ensemble2.score(tenday_full_features_train, \n",
    "                                                       tenday_target_train), 4))\n",
    "print(\"Test set accuracy:\", np.round(ensemble2.score(tenday_full_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_ensemble = ensemble2.predict(tenday_full_features_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(tenday_target_train, \n",
    "                                                    target_pred_train_ensemble), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(tenday_target_train, \n",
    "                                                    target_pred_train_ensemble), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(tenday_target_train, \n",
    "                                                    target_pred_train_ensemble), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_ensemble = ensemble2.predict(tenday_full_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_ensemble), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_ensemble), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_ensemble), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 1.0\n",
      "Test set accuracy: 0.746\n",
      "Training Set Precision: 1.0\n",
      "Training Set Recall: 1.0\n",
      "Training Set F1 Score: 1.0\n",
      "Test Set Precision: 0.2425\n",
      "Test Set Recall: 0.3753\n",
      "Test Set F1 Score: 0.2946\n"
     ]
    }
   ],
   "source": [
    "clf_dt = DecisionTreeClassifier()\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(tenday_full_features_train, tenday_target_train)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(dt.score(tenday_full_features_train, \n",
    "                                                       tenday_target_train), 4))\n",
    "print(\"Test set accuracy:\", np.round(dt.score(tenday_full_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_dt = dt.predict(tenday_full_features_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(tenday_target_train, \n",
    "                                                    target_pred_train_dt), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(tenday_target_train, \n",
    "                                                    target_pred_train_dt), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(tenday_target_train, \n",
    "                                                    target_pred_train_dt), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_dt = dt.predict(tenday_full_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_dt), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_dt), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_dt), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.628\n",
      "Test set accuracy: 0.7168\n",
      "Training Set Precision: 0.2688\n",
      "Training Set Recall: 0.7051\n",
      "Training Set F1 Score: 0.3892\n",
      "Test Set Precision: 0.2652\n",
      "Test Set Recall: 0.5669\n",
      "Test Set F1 Score: 0.3614\n"
     ]
    }
   ],
   "source": [
    "clf_sgd_10day_full = SGDClassifier(loss = \"log\", class_weight = \"balanced\",  random_state = 0, alpha = 0.1)\n",
    "clf_sgd_10day_full.fit(tenday_full_features_train, tenday_target_train)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_sgd_10day_full.score(tenday_full_features_train, \n",
    "                                                       tenday_target_train), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_sgd_10day_full.score(tenday_full_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_sgd = clf_sgd_10day_full.predict(tenday_full_features_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(tenday_target_train, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(tenday_target_train, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(tenday_target_train, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_sgd = clf_sgd_10day_full.predict(tenday_full_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_sgd), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_sgd), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_sgd), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.6427\n",
      "Test set accuracy: 0.7278\n",
      "Training Set Precision: 0.2804\n",
      "Training Set Recall: 0.7192\n",
      "Training Set F1 Score: 0.4035\n",
      "Test Set Precision: 0.2796\n",
      "Test Set Recall: 0.587\n",
      "Test Set F1 Score: 0.3787\n"
     ]
    }
   ],
   "source": [
    "clf_sgd_10day_full = SGDClassifier(loss = \"log\", class_weight = \"balanced\",  random_state = 0, alpha = 0.01)\n",
    "clf_sgd_10day_full.fit(tenday_full_features_train, tenday_target_train)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_sgd_10day_full.score(tenday_full_features_train, \n",
    "                                                       tenday_target_train), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_sgd_10day_full.score(tenday_full_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_sgd = clf_sgd_10day_full.predict(tenday_full_features_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(tenday_target_train, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(tenday_target_train, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(tenday_target_train, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_sgd = clf_sgd_10day_full.predict(tenday_full_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_sgd), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_sgd), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_sgd), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.6302\n",
      "Test set accuracy: 0.7089\n",
      "Training Set Precision: 0.2653\n",
      "Training Set Recall: 0.6787\n",
      "Training Set F1 Score: 0.3815\n",
      "Test Set Precision: 0.2495\n",
      "Test Set Recall: 0.5276\n",
      "Test Set F1 Score: 0.3388\n"
     ]
    }
   ],
   "source": [
    "clf_sgd_10day_full = SGDClassifier(loss = \"log\", class_weight = \"balanced\",  random_state = 0, alpha = 1)\n",
    "clf_sgd_10day_full.fit(tenday_full_features_train, tenday_target_train)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_sgd_10day_full.score(tenday_full_features_train, \n",
    "                                                       tenday_target_train), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_sgd_10day_full.score(tenday_full_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_sgd = clf_sgd_10day_full.predict(tenday_full_features_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(tenday_target_train, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(tenday_target_train, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(tenday_target_train, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_sgd = clf_sgd_10day_full.predict(tenday_full_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_sgd), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_sgd), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_sgd), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = { \"n_neighbors\": np.arange(1, 101,2),\n",
    "               \"p\": [1, 2],\n",
    "                \"weights\": [\"uniform\", \"distance\"]}\n",
    "\n",
    "#Finding the best alpha\n",
    "grid = GridSearchCV(KNeighborsClassifier(), param_grid, n_jobs = -1)\n",
    "grid.fit(tenday_full_features_train, tenday_target_train)\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputing Missing Values Using Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp = SimpleImputer(strategy = \"mean\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 141036 entries, 0 to 176221\n",
      "Data columns (total 31 columns):\n",
      " #   Column                        Non-Null Count   Dtype  \n",
      "---  ------                        --------------   -----  \n",
      " 0   ('cwat', 'amin')              141036 non-null  float64\n",
      " 1   ('cwat', 'amax')              141036 non-null  float64\n",
      " 2   ('cwat', 'mean')              141036 non-null  float64\n",
      " 3   ('cwat', 'var')               141036 non-null  float64\n",
      " 4   ('r', 'amin')                 141036 non-null  float64\n",
      " 5   ('r', 'amax')                 141036 non-null  float64\n",
      " 6   ('r', 'mean')                 141036 non-null  float64\n",
      " 7   ('r', 'var')                  141036 non-null  float64\n",
      " 8   ('tozne', 'amin')             141036 non-null  float64\n",
      " 9   ('tozne', 'amax')             141036 non-null  float64\n",
      " 10  ('tozne', 'mean')             141036 non-null  float64\n",
      " 11  ('tozne', 'var')              141036 non-null  float64\n",
      " 12  ('gh', 'amin')                141036 non-null  float64\n",
      " 13  ('gh', 'amax')                141036 non-null  float64\n",
      " 14  ('gh', 'mean')                141036 non-null  float64\n",
      " 15  ('gh', 'var')                 141036 non-null  float64\n",
      " 16  ('pwat', 'amin')              141036 non-null  float64\n",
      " 17  ('pwat', 'amax')              141036 non-null  float64\n",
      " 18  ('pwat', 'mean')              141036 non-null  float64\n",
      " 19  ('pwat', 'var')               141036 non-null  float64\n",
      " 20  ('paramId_0', 'amin')         141036 non-null  float64\n",
      " 21  ('paramId_0', 'amax')         141036 non-null  float64\n",
      " 22  ('paramId_0', 'mean')         141036 non-null  float64\n",
      " 23  ('paramId_0', 'var')          141036 non-null  float64\n",
      " 24  ('pres', 'amin')              55458 non-null   float64\n",
      " 25  ('pres', 'amax')              55458 non-null   float64\n",
      " 26  ('pres', 'mean')              55458 non-null   float64\n",
      " 27  ('pres', 'var')               34949 non-null   float64\n",
      " 28  ('pres', 'count')             141036 non-null  int64  \n",
      " 29  ('macro_season', '<lambda>')  141036 non-null  int64  \n",
      " 30  ('month', '<lambda>')         141036 non-null  int64  \n",
      "dtypes: float64(28), int64(3)\n",
      "memory usage: 34.4 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 141036 entries, 0 to 141035\n",
      "Data columns (total 31 columns):\n",
      " #   Column                        Non-Null Count   Dtype  \n",
      "---  ------                        --------------   -----  \n",
      " 0   ('cwat', 'amin')              141036 non-null  float64\n",
      " 1   ('cwat', 'amax')              141036 non-null  float64\n",
      " 2   ('cwat', 'mean')              141036 non-null  float64\n",
      " 3   ('cwat', 'var')               141036 non-null  float64\n",
      " 4   ('r', 'amin')                 141036 non-null  float64\n",
      " 5   ('r', 'amax')                 141036 non-null  float64\n",
      " 6   ('r', 'mean')                 141036 non-null  float64\n",
      " 7   ('r', 'var')                  141036 non-null  float64\n",
      " 8   ('tozne', 'amin')             141036 non-null  float64\n",
      " 9   ('tozne', 'amax')             141036 non-null  float64\n",
      " 10  ('tozne', 'mean')             141036 non-null  float64\n",
      " 11  ('tozne', 'var')              141036 non-null  float64\n",
      " 12  ('gh', 'amin')                141036 non-null  float64\n",
      " 13  ('gh', 'amax')                141036 non-null  float64\n",
      " 14  ('gh', 'mean')                141036 non-null  float64\n",
      " 15  ('gh', 'var')                 141036 non-null  float64\n",
      " 16  ('pwat', 'amin')              141036 non-null  float64\n",
      " 17  ('pwat', 'amax')              141036 non-null  float64\n",
      " 18  ('pwat', 'mean')              141036 non-null  float64\n",
      " 19  ('pwat', 'var')               141036 non-null  float64\n",
      " 20  ('paramId_0', 'amin')         141036 non-null  float64\n",
      " 21  ('paramId_0', 'amax')         141036 non-null  float64\n",
      " 22  ('paramId_0', 'mean')         141036 non-null  float64\n",
      " 23  ('paramId_0', 'var')          141036 non-null  float64\n",
      " 24  ('pres', 'amin')              141036 non-null  float64\n",
      " 25  ('pres', 'amax')              141036 non-null  float64\n",
      " 26  ('pres', 'mean')              141036 non-null  float64\n",
      " 27  ('pres', 'var')               141036 non-null  float64\n",
      " 28  ('pres', 'count')             141036 non-null  float64\n",
      " 29  ('macro_season', '<lambda>')  141036 non-null  float64\n",
      " 30  ('month', '<lambda>')         141036 non-null  float64\n",
      "dtypes: float64(31)\n",
      "memory usage: 33.4 MB\n"
     ]
    }
   ],
   "source": [
    "tenday_full_train.info()\n",
    "tenday_imp_train = pd.DataFrame(imp.fit_transform(tenday_full_train))\n",
    "tenday_imp_train.columns = tenday_full_train.columns\n",
    "tenday_imp_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 35259 entries, 292 to 176294\n",
      "Data columns (total 31 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   ('cwat', 'amin')              35259 non-null  float64\n",
      " 1   ('cwat', 'amax')              35259 non-null  float64\n",
      " 2   ('cwat', 'mean')              35259 non-null  float64\n",
      " 3   ('cwat', 'var')               35259 non-null  float64\n",
      " 4   ('r', 'amin')                 35259 non-null  float64\n",
      " 5   ('r', 'amax')                 35259 non-null  float64\n",
      " 6   ('r', 'mean')                 35259 non-null  float64\n",
      " 7   ('r', 'var')                  35259 non-null  float64\n",
      " 8   ('tozne', 'amin')             35259 non-null  float64\n",
      " 9   ('tozne', 'amax')             35259 non-null  float64\n",
      " 10  ('tozne', 'mean')             35259 non-null  float64\n",
      " 11  ('tozne', 'var')              35259 non-null  float64\n",
      " 12  ('gh', 'amin')                35259 non-null  float64\n",
      " 13  ('gh', 'amax')                35259 non-null  float64\n",
      " 14  ('gh', 'mean')                35259 non-null  float64\n",
      " 15  ('gh', 'var')                 35259 non-null  float64\n",
      " 16  ('pwat', 'amin')              35259 non-null  float64\n",
      " 17  ('pwat', 'amax')              35259 non-null  float64\n",
      " 18  ('pwat', 'mean')              35259 non-null  float64\n",
      " 19  ('pwat', 'var')               35259 non-null  float64\n",
      " 20  ('paramId_0', 'amin')         35259 non-null  float64\n",
      " 21  ('paramId_0', 'amax')         35259 non-null  float64\n",
      " 22  ('paramId_0', 'mean')         35259 non-null  float64\n",
      " 23  ('paramId_0', 'var')          35259 non-null  float64\n",
      " 24  ('pres', 'amin')              15069 non-null  float64\n",
      " 25  ('pres', 'amax')              15069 non-null  float64\n",
      " 26  ('pres', 'mean')              15069 non-null  float64\n",
      " 27  ('pres', 'var')               9461 non-null   float64\n",
      " 28  ('pres', 'count')             35259 non-null  int64  \n",
      " 29  ('macro_season', '<lambda>')  35259 non-null  int64  \n",
      " 30  ('month', '<lambda>')         35259 non-null  int64  \n",
      "dtypes: float64(28), int64(3)\n",
      "memory usage: 8.6 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 35259 entries, 0 to 35258\n",
      "Data columns (total 31 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   ('cwat', 'amin')              35259 non-null  float64\n",
      " 1   ('cwat', 'amax')              35259 non-null  float64\n",
      " 2   ('cwat', 'mean')              35259 non-null  float64\n",
      " 3   ('cwat', 'var')               35259 non-null  float64\n",
      " 4   ('r', 'amin')                 35259 non-null  float64\n",
      " 5   ('r', 'amax')                 35259 non-null  float64\n",
      " 6   ('r', 'mean')                 35259 non-null  float64\n",
      " 7   ('r', 'var')                  35259 non-null  float64\n",
      " 8   ('tozne', 'amin')             35259 non-null  float64\n",
      " 9   ('tozne', 'amax')             35259 non-null  float64\n",
      " 10  ('tozne', 'mean')             35259 non-null  float64\n",
      " 11  ('tozne', 'var')              35259 non-null  float64\n",
      " 12  ('gh', 'amin')                35259 non-null  float64\n",
      " 13  ('gh', 'amax')                35259 non-null  float64\n",
      " 14  ('gh', 'mean')                35259 non-null  float64\n",
      " 15  ('gh', 'var')                 35259 non-null  float64\n",
      " 16  ('pwat', 'amin')              35259 non-null  float64\n",
      " 17  ('pwat', 'amax')              35259 non-null  float64\n",
      " 18  ('pwat', 'mean')              35259 non-null  float64\n",
      " 19  ('pwat', 'var')               35259 non-null  float64\n",
      " 20  ('paramId_0', 'amin')         35259 non-null  float64\n",
      " 21  ('paramId_0', 'amax')         35259 non-null  float64\n",
      " 22  ('paramId_0', 'mean')         35259 non-null  float64\n",
      " 23  ('paramId_0', 'var')          35259 non-null  float64\n",
      " 24  ('pres', 'amin')              35259 non-null  float64\n",
      " 25  ('pres', 'amax')              35259 non-null  float64\n",
      " 26  ('pres', 'mean')              35259 non-null  float64\n",
      " 27  ('pres', 'var')               35259 non-null  float64\n",
      " 28  ('pres', 'count')             35259 non-null  float64\n",
      " 29  ('macro_season', '<lambda>')  35259 non-null  float64\n",
      " 30  ('month', '<lambda>')         35259 non-null  float64\n",
      "dtypes: float64(31)\n",
      "memory usage: 8.3 MB\n"
     ]
    }
   ],
   "source": [
    "tenday_full_test.info()\n",
    "tenday_imp_test = pd.DataFrame(imp.fit_transform(tenday_full_test))\n",
    "tenday_imp_test.columns = tenday_full_test.columns\n",
    "tenday_imp_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling Using Standard Scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10-Day Average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Full Feature Training and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
      "0               0.0              0.42           0.10325         0.009705   \n",
      "1               0.0              0.22           0.07200         0.005263   \n",
      "2               0.0              0.43           0.05825         0.007605   \n",
      "3               0.0              0.32           0.03575         0.005051   \n",
      "4               0.0              0.69           0.11475         0.025159   \n",
      "\n",
      "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
      "0            8.0           48.0         21.625    124.189103   \n",
      "1           10.0           35.0         23.425     35.789103   \n",
      "2           13.0           55.0         27.250     77.935897   \n",
      "3            7.0           29.0         17.275     34.871154   \n",
      "4           18.0           50.0         27.250     39.987179   \n",
      "\n",
      "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('paramId_0', 'amax')  \\\n",
      "0         263.500000         304.299988  ...                   59.0   \n",
      "1         270.799988         360.100006  ...                   53.0   \n",
      "2         244.000000         334.700012  ...                   46.0   \n",
      "3         264.399994         348.000000  ...                    0.0   \n",
      "4         278.700012         407.299988  ...                   64.0   \n",
      "\n",
      "   ('paramId_0', 'mean')  ('paramId_0', 'var')  ('pres', 'amin')  \\\n",
      "0                  1.475             87.025000      34430.000000   \n",
      "1                  3.200            144.420513      61430.000000   \n",
      "2                  2.400             86.605128      65030.000000   \n",
      "3                  0.000              0.000000      44487.204732   \n",
      "4                 10.550            408.920513      41470.000000   \n",
      "\n",
      "   ('pres', 'amax')  ('pres', 'mean')  ('pres', 'var')  ('pres', 'count')  \\\n",
      "0        34430.0000      34430.000000     8.288320e+07                1.0   \n",
      "1        62960.0000      62443.333333     7.702333e+05                3.0   \n",
      "2        75960.0000      71433.333333     3.250763e+07                3.0   \n",
      "3        54632.9159      49690.400823     8.288320e+07                0.0   \n",
      "4        76180.0000      57815.000000     2.113109e+08               10.0   \n",
      "\n",
      "   ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
      "0                           0.0                    1.0  \n",
      "1                           0.0                    1.0  \n",
      "2                           0.0                    1.0  \n",
      "3                           0.0                    2.0  \n",
      "4                           0.0                    2.0  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>('cwat', 'amin')</th>\n",
       "      <th>('cwat', 'amax')</th>\n",
       "      <th>('cwat', 'mean')</th>\n",
       "      <th>('cwat', 'var')</th>\n",
       "      <th>('r', 'amin')</th>\n",
       "      <th>('r', 'amax')</th>\n",
       "      <th>('r', 'mean')</th>\n",
       "      <th>('r', 'var')</th>\n",
       "      <th>('tozne', 'amin')</th>\n",
       "      <th>('tozne', 'amax')</th>\n",
       "      <th>...</th>\n",
       "      <th>('paramId_0', 'amax')</th>\n",
       "      <th>('paramId_0', 'mean')</th>\n",
       "      <th>('paramId_0', 'var')</th>\n",
       "      <th>('pres', 'amin')</th>\n",
       "      <th>('pres', 'amax')</th>\n",
       "      <th>('pres', 'mean')</th>\n",
       "      <th>('pres', 'var')</th>\n",
       "      <th>('pres', 'count')</th>\n",
       "      <th>('macro_season', '&lt;lambda&gt;')</th>\n",
       "      <th>('month', '&lt;lambda&gt;')</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.041787</td>\n",
       "      <td>-0.178081</td>\n",
       "      <td>0.698012</td>\n",
       "      <td>-0.293179</td>\n",
       "      <td>-0.209981</td>\n",
       "      <td>1.268567</td>\n",
       "      <td>0.438169</td>\n",
       "      <td>1.704667</td>\n",
       "      <td>-0.752168</td>\n",
       "      <td>-0.975314</td>\n",
       "      <td>...</td>\n",
       "      <td>1.843341</td>\n",
       "      <td>0.125417</td>\n",
       "      <td>0.530528</td>\n",
       "      <td>-1.265956e+00</td>\n",
       "      <td>-2.239855</td>\n",
       "      <td>-1.967448e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.078436</td>\n",
       "      <td>-0.993174</td>\n",
       "      <td>-1.577616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.041787</td>\n",
       "      <td>-0.558465</td>\n",
       "      <td>0.229878</td>\n",
       "      <td>-0.387740</td>\n",
       "      <td>0.302413</td>\n",
       "      <td>0.092740</td>\n",
       "      <td>0.723553</td>\n",
       "      <td>-0.328350</td>\n",
       "      <td>-0.435469</td>\n",
       "      <td>0.452479</td>\n",
       "      <td>...</td>\n",
       "      <td>1.577662</td>\n",
       "      <td>0.868005</td>\n",
       "      <td>1.203807</td>\n",
       "      <td>2.132683e+00</td>\n",
       "      <td>0.923206</td>\n",
       "      <td>1.644173e+00</td>\n",
       "      <td>-1.612299</td>\n",
       "      <td>0.840588</td>\n",
       "      <td>-0.993174</td>\n",
       "      <td>-1.577616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.041787</td>\n",
       "      <td>-0.159062</td>\n",
       "      <td>0.023900</td>\n",
       "      <td>-0.337884</td>\n",
       "      <td>1.071004</td>\n",
       "      <td>1.901705</td>\n",
       "      <td>1.329995</td>\n",
       "      <td>0.640939</td>\n",
       "      <td>-1.598148</td>\n",
       "      <td>-0.197448</td>\n",
       "      <td>...</td>\n",
       "      <td>1.267704</td>\n",
       "      <td>0.523616</td>\n",
       "      <td>0.525603</td>\n",
       "      <td>2.585834e+00</td>\n",
       "      <td>2.364489</td>\n",
       "      <td>2.803209e+00</td>\n",
       "      <td>-0.989131</td>\n",
       "      <td>0.840588</td>\n",
       "      <td>-0.993174</td>\n",
       "      <td>-1.577616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.041787</td>\n",
       "      <td>-0.368273</td>\n",
       "      <td>-0.313157</td>\n",
       "      <td>-0.392250</td>\n",
       "      <td>-0.466177</td>\n",
       "      <td>-0.449949</td>\n",
       "      <td>-0.251511</td>\n",
       "      <td>-0.349461</td>\n",
       "      <td>-0.713123</td>\n",
       "      <td>0.142868</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.769165</td>\n",
       "      <td>-0.509550</td>\n",
       "      <td>-0.490321</td>\n",
       "      <td>9.158648e-16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.380534e-16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.537948</td>\n",
       "      <td>-0.993174</td>\n",
       "      <td>-1.287525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.041787</td>\n",
       "      <td>0.335437</td>\n",
       "      <td>0.870285</td>\n",
       "      <td>0.035812</td>\n",
       "      <td>2.351988</td>\n",
       "      <td>1.449463</td>\n",
       "      <td>1.329995</td>\n",
       "      <td>-0.231803</td>\n",
       "      <td>-0.092737</td>\n",
       "      <td>1.660218</td>\n",
       "      <td>...</td>\n",
       "      <td>2.064740</td>\n",
       "      <td>4.032076</td>\n",
       "      <td>4.306531</td>\n",
       "      <td>-3.797921e-01</td>\n",
       "      <td>2.388880</td>\n",
       "      <td>1.047465e+00</td>\n",
       "      <td>2.521696</td>\n",
       "      <td>4.057170</td>\n",
       "      <td>-0.993174</td>\n",
       "      <td>-1.287525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
       "0         -0.041787         -0.178081          0.698012        -0.293179   \n",
       "1         -0.041787         -0.558465          0.229878        -0.387740   \n",
       "2         -0.041787         -0.159062          0.023900        -0.337884   \n",
       "3         -0.041787         -0.368273         -0.313157        -0.392250   \n",
       "4         -0.041787          0.335437          0.870285         0.035812   \n",
       "\n",
       "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
       "0      -0.209981       1.268567       0.438169      1.704667   \n",
       "1       0.302413       0.092740       0.723553     -0.328350   \n",
       "2       1.071004       1.901705       1.329995      0.640939   \n",
       "3      -0.466177      -0.449949      -0.251511     -0.349461   \n",
       "4       2.351988       1.449463       1.329995     -0.231803   \n",
       "\n",
       "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('paramId_0', 'amax')  \\\n",
       "0          -0.752168          -0.975314  ...               1.843341   \n",
       "1          -0.435469           0.452479  ...               1.577662   \n",
       "2          -1.598148          -0.197448  ...               1.267704   \n",
       "3          -0.713123           0.142868  ...              -0.769165   \n",
       "4          -0.092737           1.660218  ...               2.064740   \n",
       "\n",
       "   ('paramId_0', 'mean')  ('paramId_0', 'var')  ('pres', 'amin')  \\\n",
       "0               0.125417              0.530528     -1.265956e+00   \n",
       "1               0.868005              1.203807      2.132683e+00   \n",
       "2               0.523616              0.525603      2.585834e+00   \n",
       "3              -0.509550             -0.490321      9.158648e-16   \n",
       "4               4.032076              4.306531     -3.797921e-01   \n",
       "\n",
       "   ('pres', 'amax')  ('pres', 'mean')  ('pres', 'var')  ('pres', 'count')  \\\n",
       "0         -2.239855     -1.967448e+00         0.000000          -0.078436   \n",
       "1          0.923206      1.644173e+00        -1.612299           0.840588   \n",
       "2          2.364489      2.803209e+00        -0.989131           0.840588   \n",
       "3          0.000000      9.380534e-16         0.000000          -0.537948   \n",
       "4          2.388880      1.047465e+00         2.521696           4.057170   \n",
       "\n",
       "   ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
       "0                     -0.993174              -1.577616  \n",
       "1                     -0.993174              -1.577616  \n",
       "2                     -0.993174              -1.577616  \n",
       "3                     -0.993174              -1.287525  \n",
       "4                     -0.993174              -1.287525  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tenday_imp_train.head())\n",
    "scaler.fit(tenday_imp_train)\n",
    "tenday_full_features_train = pd.DataFrame(scaler.transform(tenday_imp_train))\n",
    "tenday_full_features_train.columns = tenday_imp_train.columns\n",
    "tenday_full_features_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>('cwat', 'amin')</th>\n",
       "      <th>('cwat', 'amax')</th>\n",
       "      <th>('cwat', 'mean')</th>\n",
       "      <th>('cwat', 'var')</th>\n",
       "      <th>('r', 'amin')</th>\n",
       "      <th>('r', 'amax')</th>\n",
       "      <th>('r', 'mean')</th>\n",
       "      <th>('r', 'var')</th>\n",
       "      <th>('tozne', 'amin')</th>\n",
       "      <th>('tozne', 'amax')</th>\n",
       "      <th>...</th>\n",
       "      <th>('paramId_0', 'amax')</th>\n",
       "      <th>('paramId_0', 'mean')</th>\n",
       "      <th>('paramId_0', 'var')</th>\n",
       "      <th>('pres', 'amin')</th>\n",
       "      <th>('pres', 'amax')</th>\n",
       "      <th>('pres', 'mean')</th>\n",
       "      <th>('pres', 'var')</th>\n",
       "      <th>('pres', 'count')</th>\n",
       "      <th>('macro_season', '&lt;lambda&gt;')</th>\n",
       "      <th>('month', '&lt;lambda&gt;')</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.041787</td>\n",
       "      <td>-0.710618</td>\n",
       "      <td>-0.414273</td>\n",
       "      <td>-0.447935</td>\n",
       "      <td>-0.978571</td>\n",
       "      <td>-0.811742</td>\n",
       "      <td>-1.226575</td>\n",
       "      <td>-0.340970</td>\n",
       "      <td>-1.476674</td>\n",
       "      <td>-1.351453</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.769165</td>\n",
       "      <td>-0.509550</td>\n",
       "      <td>-0.490321</td>\n",
       "      <td>-0.021296</td>\n",
       "      <td>-0.030840</td>\n",
       "      <td>-0.039578</td>\n",
       "      <td>-0.051727</td>\n",
       "      <td>-0.537948</td>\n",
       "      <td>-0.993174</td>\n",
       "      <td>-1.577616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.041787</td>\n",
       "      <td>1.552666</td>\n",
       "      <td>1.042558</td>\n",
       "      <td>0.436756</td>\n",
       "      <td>-0.209981</td>\n",
       "      <td>1.720808</td>\n",
       "      <td>1.012901</td>\n",
       "      <td>2.867610</td>\n",
       "      <td>-1.624179</td>\n",
       "      <td>-1.305395</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.769165</td>\n",
       "      <td>-0.509550</td>\n",
       "      <td>-0.490321</td>\n",
       "      <td>-0.021296</td>\n",
       "      <td>-0.030840</td>\n",
       "      <td>-0.039578</td>\n",
       "      <td>-0.051727</td>\n",
       "      <td>-0.537948</td>\n",
       "      <td>-0.993174</td>\n",
       "      <td>-1.577616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.041787</td>\n",
       "      <td>-0.082985</td>\n",
       "      <td>-0.129648</td>\n",
       "      <td>-0.248986</td>\n",
       "      <td>-0.466177</td>\n",
       "      <td>0.183188</td>\n",
       "      <td>-0.711297</td>\n",
       "      <td>0.392016</td>\n",
       "      <td>-1.034162</td>\n",
       "      <td>0.733944</td>\n",
       "      <td>...</td>\n",
       "      <td>0.249269</td>\n",
       "      <td>-0.251259</td>\n",
       "      <td>-0.335237</td>\n",
       "      <td>4.087529</td>\n",
       "      <td>2.475357</td>\n",
       "      <td>3.515735</td>\n",
       "      <td>-0.051727</td>\n",
       "      <td>-0.078436</td>\n",
       "      <td>-0.993174</td>\n",
       "      <td>-1.287525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.041787</td>\n",
       "      <td>-0.292196</td>\n",
       "      <td>0.683031</td>\n",
       "      <td>-0.344914</td>\n",
       "      <td>1.583398</td>\n",
       "      <td>0.816326</td>\n",
       "      <td>1.012901</td>\n",
       "      <td>-0.467680</td>\n",
       "      <td>-0.114429</td>\n",
       "      <td>1.419694</td>\n",
       "      <td>...</td>\n",
       "      <td>2.020460</td>\n",
       "      <td>3.859881</td>\n",
       "      <td>4.084313</td>\n",
       "      <td>-0.374757</td>\n",
       "      <td>2.571812</td>\n",
       "      <td>1.329739</td>\n",
       "      <td>3.066291</td>\n",
       "      <td>3.597659</td>\n",
       "      <td>-0.993174</td>\n",
       "      <td>-1.287525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.041787</td>\n",
       "      <td>1.875992</td>\n",
       "      <td>2.581780</td>\n",
       "      <td>3.498951</td>\n",
       "      <td>1.839594</td>\n",
       "      <td>1.720808</td>\n",
       "      <td>1.769963</td>\n",
       "      <td>1.511012</td>\n",
       "      <td>-0.656724</td>\n",
       "      <td>0.989821</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.282088</td>\n",
       "      <td>-0.315831</td>\n",
       "      <td>-0.441624</td>\n",
       "      <td>0.051961</td>\n",
       "      <td>1.650500</td>\n",
       "      <td>0.969465</td>\n",
       "      <td>4.323441</td>\n",
       "      <td>0.381076</td>\n",
       "      <td>-0.993174</td>\n",
       "      <td>-1.287525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
       "0         -0.041787         -0.710618         -0.414273        -0.447935   \n",
       "1         -0.041787          1.552666          1.042558         0.436756   \n",
       "2         -0.041787         -0.082985         -0.129648        -0.248986   \n",
       "3         -0.041787         -0.292196          0.683031        -0.344914   \n",
       "4         -0.041787          1.875992          2.581780         3.498951   \n",
       "\n",
       "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
       "0      -0.978571      -0.811742      -1.226575     -0.340970   \n",
       "1      -0.209981       1.720808       1.012901      2.867610   \n",
       "2      -0.466177       0.183188      -0.711297      0.392016   \n",
       "3       1.583398       0.816326       1.012901     -0.467680   \n",
       "4       1.839594       1.720808       1.769963      1.511012   \n",
       "\n",
       "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('paramId_0', 'amax')  \\\n",
       "0          -1.476674          -1.351453  ...              -0.769165   \n",
       "1          -1.624179          -1.305395  ...              -0.769165   \n",
       "2          -1.034162           0.733944  ...               0.249269   \n",
       "3          -0.114429           1.419694  ...               2.020460   \n",
       "4          -0.656724           0.989821  ...              -0.282088   \n",
       "\n",
       "   ('paramId_0', 'mean')  ('paramId_0', 'var')  ('pres', 'amin')  \\\n",
       "0              -0.509550             -0.490321         -0.021296   \n",
       "1              -0.509550             -0.490321         -0.021296   \n",
       "2              -0.251259             -0.335237          4.087529   \n",
       "3               3.859881              4.084313         -0.374757   \n",
       "4              -0.315831             -0.441624          0.051961   \n",
       "\n",
       "   ('pres', 'amax')  ('pres', 'mean')  ('pres', 'var')  ('pres', 'count')  \\\n",
       "0         -0.030840         -0.039578        -0.051727          -0.537948   \n",
       "1         -0.030840         -0.039578        -0.051727          -0.537948   \n",
       "2          2.475357          3.515735        -0.051727          -0.078436   \n",
       "3          2.571812          1.329739         3.066291           3.597659   \n",
       "4          1.650500          0.969465         4.323441           0.381076   \n",
       "\n",
       "   ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
       "0                     -0.993174              -1.577616  \n",
       "1                     -0.993174              -1.577616  \n",
       "2                     -0.993174              -1.287525  \n",
       "3                     -0.993174              -1.287525  \n",
       "4                     -0.993174              -1.287525  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tenday_full_features_test = pd.DataFrame(scaler.transform(tenday_imp_test))\n",
    "tenday_full_features_test.columns = tenday_imp_test.columns\n",
    "tenday_full_features_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Features With No Nulls Training and Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
      "0               0.0              0.42           0.10325         0.009705   \n",
      "1               0.0              0.22           0.07200         0.005263   \n",
      "2               0.0              0.43           0.05825         0.007605   \n",
      "3               0.0              0.32           0.03575         0.005051   \n",
      "4               0.0              0.69           0.11475         0.025159   \n",
      "\n",
      "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
      "0            8.0           48.0         21.625    124.189103   \n",
      "1           10.0           35.0         23.425     35.789103   \n",
      "2           13.0           55.0         27.250     77.935897   \n",
      "3            7.0           29.0         17.275     34.871154   \n",
      "4           18.0           50.0         27.250     39.987179   \n",
      "\n",
      "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('pwat', 'amax')  \\\n",
      "0         263.500000         304.299988  ...         28.500000   \n",
      "1         270.799988         360.100006  ...         18.299999   \n",
      "2         244.000000         334.700012  ...         22.600000   \n",
      "3         264.399994         348.000000  ...         20.100000   \n",
      "4         278.700012         407.299988  ...         28.000000   \n",
      "\n",
      "   ('pwat', 'mean')  ('pwat', 'var')  ('paramId_0', 'amin')  \\\n",
      "0           13.6700        37.680100                    0.0   \n",
      "1           13.1850         7.657718                    0.0   \n",
      "2           14.3550        16.905103                    0.0   \n",
      "3           11.1450        14.892795                    0.0   \n",
      "4           15.1125        15.711378                    0.0   \n",
      "\n",
      "   ('paramId_0', 'amax')  ('paramId_0', 'mean')  ('paramId_0', 'var')  \\\n",
      "0                   59.0                  1.475             87.025000   \n",
      "1                   53.0                  3.200            144.420513   \n",
      "2                   46.0                  2.400             86.605128   \n",
      "3                    0.0                  0.000              0.000000   \n",
      "4                   64.0                 10.550            408.920513   \n",
      "\n",
      "   ('pres', 'count')  ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
      "0                  1                             0                      1  \n",
      "1                  3                             0                      1  \n",
      "2                  3                             0                      1  \n",
      "3                  0                             0                      2  \n",
      "4                 10                             0                      2  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>('cwat', 'amin')</th>\n",
       "      <th>('cwat', 'amax')</th>\n",
       "      <th>('cwat', 'mean')</th>\n",
       "      <th>('cwat', 'var')</th>\n",
       "      <th>('r', 'amin')</th>\n",
       "      <th>('r', 'amax')</th>\n",
       "      <th>('r', 'mean')</th>\n",
       "      <th>('r', 'var')</th>\n",
       "      <th>('tozne', 'amin')</th>\n",
       "      <th>('tozne', 'amax')</th>\n",
       "      <th>...</th>\n",
       "      <th>('pwat', 'amax')</th>\n",
       "      <th>('pwat', 'mean')</th>\n",
       "      <th>('pwat', 'var')</th>\n",
       "      <th>('paramId_0', 'amin')</th>\n",
       "      <th>('paramId_0', 'amax')</th>\n",
       "      <th>('paramId_0', 'mean')</th>\n",
       "      <th>('paramId_0', 'var')</th>\n",
       "      <th>('pres', 'count')</th>\n",
       "      <th>('macro_season', '&lt;lambda&gt;')</th>\n",
       "      <th>('month', '&lt;lambda&gt;')</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.041787</td>\n",
       "      <td>-0.178081</td>\n",
       "      <td>0.698012</td>\n",
       "      <td>-0.293179</td>\n",
       "      <td>-0.209981</td>\n",
       "      <td>1.268567</td>\n",
       "      <td>0.438169</td>\n",
       "      <td>1.704667</td>\n",
       "      <td>-0.752168</td>\n",
       "      <td>-0.975314</td>\n",
       "      <td>...</td>\n",
       "      <td>0.811691</td>\n",
       "      <td>0.245159</td>\n",
       "      <td>1.054886</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.843341</td>\n",
       "      <td>0.125417</td>\n",
       "      <td>0.530528</td>\n",
       "      <td>-0.078436</td>\n",
       "      <td>-0.993174</td>\n",
       "      <td>-1.577616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.041787</td>\n",
       "      <td>-0.558465</td>\n",
       "      <td>0.229878</td>\n",
       "      <td>-0.387740</td>\n",
       "      <td>0.302413</td>\n",
       "      <td>0.092740</td>\n",
       "      <td>0.723553</td>\n",
       "      <td>-0.328350</td>\n",
       "      <td>-0.435469</td>\n",
       "      <td>0.452479</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.379780</td>\n",
       "      <td>0.154107</td>\n",
       "      <td>-0.619355</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.577662</td>\n",
       "      <td>0.868005</td>\n",
       "      <td>1.203807</td>\n",
       "      <td>0.840588</td>\n",
       "      <td>-0.993174</td>\n",
       "      <td>-1.577616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.041787</td>\n",
       "      <td>-0.159062</td>\n",
       "      <td>0.023900</td>\n",
       "      <td>-0.337884</td>\n",
       "      <td>1.071004</td>\n",
       "      <td>1.901705</td>\n",
       "      <td>1.329995</td>\n",
       "      <td>0.640939</td>\n",
       "      <td>-1.598148</td>\n",
       "      <td>-0.197448</td>\n",
       "      <td>...</td>\n",
       "      <td>0.122507</td>\n",
       "      <td>0.373759</td>\n",
       "      <td>-0.103661</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.267704</td>\n",
       "      <td>0.523616</td>\n",
       "      <td>0.525603</td>\n",
       "      <td>0.840588</td>\n",
       "      <td>-0.993174</td>\n",
       "      <td>-1.577616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.041787</td>\n",
       "      <td>-0.368273</td>\n",
       "      <td>-0.313157</td>\n",
       "      <td>-0.392250</td>\n",
       "      <td>-0.466177</td>\n",
       "      <td>-0.449949</td>\n",
       "      <td>-0.251511</td>\n",
       "      <td>-0.349461</td>\n",
       "      <td>-0.713123</td>\n",
       "      <td>0.142868</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.169520</td>\n",
       "      <td>-0.228875</td>\n",
       "      <td>-0.215881</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.769165</td>\n",
       "      <td>-0.509550</td>\n",
       "      <td>-0.490321</td>\n",
       "      <td>-0.537948</td>\n",
       "      <td>-0.993174</td>\n",
       "      <td>-1.287525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.041787</td>\n",
       "      <td>0.335437</td>\n",
       "      <td>0.870285</td>\n",
       "      <td>0.035812</td>\n",
       "      <td>2.351988</td>\n",
       "      <td>1.449463</td>\n",
       "      <td>1.329995</td>\n",
       "      <td>-0.231803</td>\n",
       "      <td>-0.092737</td>\n",
       "      <td>1.660218</td>\n",
       "      <td>...</td>\n",
       "      <td>0.753286</td>\n",
       "      <td>0.515969</td>\n",
       "      <td>-0.170231</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.064740</td>\n",
       "      <td>4.032076</td>\n",
       "      <td>4.306531</td>\n",
       "      <td>4.057170</td>\n",
       "      <td>-0.993174</td>\n",
       "      <td>-1.287525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
       "0         -0.041787         -0.178081          0.698012        -0.293179   \n",
       "1         -0.041787         -0.558465          0.229878        -0.387740   \n",
       "2         -0.041787         -0.159062          0.023900        -0.337884   \n",
       "3         -0.041787         -0.368273         -0.313157        -0.392250   \n",
       "4         -0.041787          0.335437          0.870285         0.035812   \n",
       "\n",
       "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
       "0      -0.209981       1.268567       0.438169      1.704667   \n",
       "1       0.302413       0.092740       0.723553     -0.328350   \n",
       "2       1.071004       1.901705       1.329995      0.640939   \n",
       "3      -0.466177      -0.449949      -0.251511     -0.349461   \n",
       "4       2.351988       1.449463       1.329995     -0.231803   \n",
       "\n",
       "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('pwat', 'amax')  \\\n",
       "0          -0.752168          -0.975314  ...          0.811691   \n",
       "1          -0.435469           0.452479  ...         -0.379780   \n",
       "2          -1.598148          -0.197448  ...          0.122507   \n",
       "3          -0.713123           0.142868  ...         -0.169520   \n",
       "4          -0.092737           1.660218  ...          0.753286   \n",
       "\n",
       "   ('pwat', 'mean')  ('pwat', 'var')  ('paramId_0', 'amin')  \\\n",
       "0          0.245159         1.054886                    0.0   \n",
       "1          0.154107        -0.619355                    0.0   \n",
       "2          0.373759        -0.103661                    0.0   \n",
       "3         -0.228875        -0.215881                    0.0   \n",
       "4          0.515969        -0.170231                    0.0   \n",
       "\n",
       "   ('paramId_0', 'amax')  ('paramId_0', 'mean')  ('paramId_0', 'var')  \\\n",
       "0               1.843341               0.125417              0.530528   \n",
       "1               1.577662               0.868005              1.203807   \n",
       "2               1.267704               0.523616              0.525603   \n",
       "3              -0.769165              -0.509550             -0.490321   \n",
       "4               2.064740               4.032076              4.306531   \n",
       "\n",
       "   ('pres', 'count')  ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
       "0          -0.078436                     -0.993174              -1.577616  \n",
       "1           0.840588                     -0.993174              -1.577616  \n",
       "2           0.840588                     -0.993174              -1.577616  \n",
       "3          -0.537948                     -0.993174              -1.287525  \n",
       "4           4.057170                     -0.993174              -1.287525  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tenday_no_null_train.head())\n",
    "scaler.fit(tenday_no_null_train)\n",
    "tenday_part_features_train = pd.DataFrame(scaler.transform(tenday_no_null_train))\n",
    "tenday_part_features_train.columns = tenday_no_null_train.columns\n",
    "tenday_part_features_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>('cwat', 'amin')</th>\n",
       "      <th>('cwat', 'amax')</th>\n",
       "      <th>('cwat', 'mean')</th>\n",
       "      <th>('cwat', 'var')</th>\n",
       "      <th>('r', 'amin')</th>\n",
       "      <th>('r', 'amax')</th>\n",
       "      <th>('r', 'mean')</th>\n",
       "      <th>('r', 'var')</th>\n",
       "      <th>('tozne', 'amin')</th>\n",
       "      <th>('tozne', 'amax')</th>\n",
       "      <th>...</th>\n",
       "      <th>('pwat', 'amax')</th>\n",
       "      <th>('pwat', 'mean')</th>\n",
       "      <th>('pwat', 'var')</th>\n",
       "      <th>('paramId_0', 'amin')</th>\n",
       "      <th>('paramId_0', 'amax')</th>\n",
       "      <th>('paramId_0', 'mean')</th>\n",
       "      <th>('paramId_0', 'var')</th>\n",
       "      <th>('pres', 'count')</th>\n",
       "      <th>('macro_season', '&lt;lambda&gt;')</th>\n",
       "      <th>('month', '&lt;lambda&gt;')</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.041787</td>\n",
       "      <td>-0.710618</td>\n",
       "      <td>-0.414273</td>\n",
       "      <td>-0.447935</td>\n",
       "      <td>-0.978571</td>\n",
       "      <td>-0.811742</td>\n",
       "      <td>-1.226575</td>\n",
       "      <td>-0.340970</td>\n",
       "      <td>-1.476674</td>\n",
       "      <td>-1.351453</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.426504</td>\n",
       "      <td>-0.808042</td>\n",
       "      <td>-0.266732</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.769165</td>\n",
       "      <td>-0.509550</td>\n",
       "      <td>-0.490321</td>\n",
       "      <td>-0.537948</td>\n",
       "      <td>-0.993174</td>\n",
       "      <td>-1.577616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.041787</td>\n",
       "      <td>1.552666</td>\n",
       "      <td>1.042558</td>\n",
       "      <td>0.436756</td>\n",
       "      <td>-0.209981</td>\n",
       "      <td>1.720808</td>\n",
       "      <td>1.012901</td>\n",
       "      <td>2.867610</td>\n",
       "      <td>-1.624179</td>\n",
       "      <td>-1.305395</td>\n",
       "      <td>...</td>\n",
       "      <td>1.115400</td>\n",
       "      <td>0.537089</td>\n",
       "      <td>2.006364</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.769165</td>\n",
       "      <td>-0.509550</td>\n",
       "      <td>-0.490321</td>\n",
       "      <td>-0.537948</td>\n",
       "      <td>-0.993174</td>\n",
       "      <td>-1.577616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.041787</td>\n",
       "      <td>-0.082985</td>\n",
       "      <td>-0.129648</td>\n",
       "      <td>-0.248986</td>\n",
       "      <td>-0.466177</td>\n",
       "      <td>0.183188</td>\n",
       "      <td>-0.711297</td>\n",
       "      <td>0.392016</td>\n",
       "      <td>-1.034162</td>\n",
       "      <td>0.733944</td>\n",
       "      <td>...</td>\n",
       "      <td>0.367810</td>\n",
       "      <td>-0.402062</td>\n",
       "      <td>0.410892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.249269</td>\n",
       "      <td>-0.251259</td>\n",
       "      <td>-0.335237</td>\n",
       "      <td>-0.078436</td>\n",
       "      <td>-0.993174</td>\n",
       "      <td>-1.287525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.041787</td>\n",
       "      <td>-0.292196</td>\n",
       "      <td>0.683031</td>\n",
       "      <td>-0.344914</td>\n",
       "      <td>1.583398</td>\n",
       "      <td>0.816326</td>\n",
       "      <td>1.012901</td>\n",
       "      <td>-0.467680</td>\n",
       "      <td>-0.114429</td>\n",
       "      <td>1.419694</td>\n",
       "      <td>...</td>\n",
       "      <td>0.145869</td>\n",
       "      <td>0.410367</td>\n",
       "      <td>-0.618427</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.020460</td>\n",
       "      <td>3.859881</td>\n",
       "      <td>4.084313</td>\n",
       "      <td>3.597659</td>\n",
       "      <td>-0.993174</td>\n",
       "      <td>-1.287525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.041787</td>\n",
       "      <td>1.875992</td>\n",
       "      <td>2.581780</td>\n",
       "      <td>3.498951</td>\n",
       "      <td>1.839594</td>\n",
       "      <td>1.720808</td>\n",
       "      <td>1.769963</td>\n",
       "      <td>1.511012</td>\n",
       "      <td>-0.656724</td>\n",
       "      <td>0.989821</td>\n",
       "      <td>...</td>\n",
       "      <td>1.617687</td>\n",
       "      <td>1.175862</td>\n",
       "      <td>2.157998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.282088</td>\n",
       "      <td>-0.315831</td>\n",
       "      <td>-0.441624</td>\n",
       "      <td>0.381076</td>\n",
       "      <td>-0.993174</td>\n",
       "      <td>-1.287525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ('cwat', 'amin')  ('cwat', 'amax')  ('cwat', 'mean')  ('cwat', 'var')  \\\n",
       "0         -0.041787         -0.710618         -0.414273        -0.447935   \n",
       "1         -0.041787          1.552666          1.042558         0.436756   \n",
       "2         -0.041787         -0.082985         -0.129648        -0.248986   \n",
       "3         -0.041787         -0.292196          0.683031        -0.344914   \n",
       "4         -0.041787          1.875992          2.581780         3.498951   \n",
       "\n",
       "   ('r', 'amin')  ('r', 'amax')  ('r', 'mean')  ('r', 'var')  \\\n",
       "0      -0.978571      -0.811742      -1.226575     -0.340970   \n",
       "1      -0.209981       1.720808       1.012901      2.867610   \n",
       "2      -0.466177       0.183188      -0.711297      0.392016   \n",
       "3       1.583398       0.816326       1.012901     -0.467680   \n",
       "4       1.839594       1.720808       1.769963      1.511012   \n",
       "\n",
       "   ('tozne', 'amin')  ('tozne', 'amax')  ...  ('pwat', 'amax')  \\\n",
       "0          -1.476674          -1.351453  ...         -0.426504   \n",
       "1          -1.624179          -1.305395  ...          1.115400   \n",
       "2          -1.034162           0.733944  ...          0.367810   \n",
       "3          -0.114429           1.419694  ...          0.145869   \n",
       "4          -0.656724           0.989821  ...          1.617687   \n",
       "\n",
       "   ('pwat', 'mean')  ('pwat', 'var')  ('paramId_0', 'amin')  \\\n",
       "0         -0.808042        -0.266732                    0.0   \n",
       "1          0.537089         2.006364                    0.0   \n",
       "2         -0.402062         0.410892                    0.0   \n",
       "3          0.410367        -0.618427                    0.0   \n",
       "4          1.175862         2.157998                    0.0   \n",
       "\n",
       "   ('paramId_0', 'amax')  ('paramId_0', 'mean')  ('paramId_0', 'var')  \\\n",
       "0              -0.769165              -0.509550             -0.490321   \n",
       "1              -0.769165              -0.509550             -0.490321   \n",
       "2               0.249269              -0.251259             -0.335237   \n",
       "3               2.020460               3.859881              4.084313   \n",
       "4              -0.282088              -0.315831             -0.441624   \n",
       "\n",
       "   ('pres', 'count')  ('macro_season', '<lambda>')  ('month', '<lambda>')  \n",
       "0          -0.537948                     -0.993174              -1.577616  \n",
       "1          -0.537948                     -0.993174              -1.577616  \n",
       "2          -0.078436                     -0.993174              -1.287525  \n",
       "3           3.597659                     -0.993174              -1.287525  \n",
       "4           0.381076                     -0.993174              -1.287525  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tenday_part_features_test = pd.DataFrame(scaler.transform(tenday_no_null_test))\n",
    "tenday_part_features_test.columns = tenday_no_null_test.columns\n",
    "tenday_part_features_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10-Day Average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.6579\n",
      "Test set accuracy: 0.6618\n",
      "Training Set Precision: 0.2912\n",
      "Training Set Recall: 0.7223\n",
      "Training Set F1 Score: 0.4151\n",
      "Test Set Precision: 0.252\n",
      "Test Set Recall: 0.7078\n",
      "Test Set F1 Score: 0.3717\n"
     ]
    }
   ],
   "source": [
    "clf_log_10day_full = LogisticRegression(class_weight = \"balanced\", max_iter = 10000)\n",
    "clf_log_10day_full.fit(tenday_full_features_train, tenday_target_train)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_log_10day_full.score(tenday_full_features_train, \n",
    "                                                       tenday_target_train), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_log_10day_full.score(tenday_full_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_log = clf_log_10day_full.predict(tenday_full_features_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(tenday_target_train, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(tenday_target_train, \n",
    "                                                    target_pred_train_log), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(tenday_target_train, \n",
    "                                                    target_pred_train_log), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_log = clf_log_10day_full.predict(tenday_full_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_log), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_log), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_log), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.6355\n",
      "Test set accuracy: 0.6323\n",
      "Training Set Precision: 0.2803\n",
      "Training Set Recall: 0.7459\n",
      "Training Set F1 Score: 0.4075\n",
      "Test Set Precision: 0.2387\n",
      "Test Set Recall: 0.7317\n",
      "Test Set F1 Score: 0.36\n"
     ]
    }
   ],
   "source": [
    "clf_sgd_10day_full = SGDClassifier(loss = \"log\", class_weight = \"balanced\",  random_state = 0)\n",
    "clf_sgd_10day_full.fit(tenday_full_features_train, tenday_target_train)\n",
    "\n",
    "# Accuracies of Training and Test Set \n",
    "print(\"Training set accuracy:\", np.round(clf_sgd_10day_full.score(tenday_full_features_train, \n",
    "                                                       tenday_target_train), 4))\n",
    "print(\"Test set accuracy:\", np.round(clf_sgd_10day_full.score(tenday_full_features_test,\n",
    "                                                        tenday_target_test),\n",
    "                                     4))\n",
    "\n",
    "# Precision and Recall of the Training Set\n",
    "target_pred_train_sgd = clf_sgd_10day_full.predict(tenday_full_features_train)\n",
    "print(\"Training Set Precision:\", np.round(precision_score(tenday_target_train, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set Recall:\", np.round(recall_score(tenday_target_train, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "print(\"Training Set F1 Score:\", np.round(f1_score(tenday_target_train, \n",
    "                                                    target_pred_train_sgd), 4))\n",
    "\n",
    "# Precision and Recall of the Test Set\n",
    "target_pred_test_sgd = clf_sgd_10day_full.predict(tenday_full_features_test)\n",
    "print(\"Test Set Precision:\", np.round(precision_score(tenday_target_test, \n",
    "                                                      target_pred_test_sgd), 4))\n",
    "print(\"Test Set Recall:\", np.round(recall_score(tenday_target_test, \n",
    "                                                target_pred_test_sgd), 4))\n",
    "print(\"Test Set F1 Score:\", np.round(f1_score(tenday_target_test, \n",
    "                                                    target_pred_test_sgd), 4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
